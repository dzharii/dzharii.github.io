<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Dmytro Zharii RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Dmytro Zharii Atom Feed">
<link rel="stylesheet" href="/clippy.js/clippy.css">
<script src="/clippy.js/jquery-3.6.0.min.js"></script>
<script src="/clippy.js/clippy.js"></script><title data-react-helmet="true">Apache Spark Reference | Dmytro Zharii</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://blog.zharii.com/docs/gpt4-generated-content/2023-05-03-apache-spark-ref"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Apache Spark Reference | Dmytro Zharii"><meta data-react-helmet="true" name="description" content="2023-05-03"><meta data-react-helmet="true" property="og:description" content="2023-05-03"><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://blog.zharii.com/docs/gpt4-generated-content/2023-05-03-apache-spark-ref"><link data-react-helmet="true" rel="alternate" href="https://blog.zharii.com/docs/gpt4-generated-content/2023-05-03-apache-spark-ref" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://blog.zharii.com/docs/gpt4-generated-content/2023-05-03-apache-spark-ref" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.7d6ab89a.css">
<link rel="preload" href="/assets/js/runtime~main.fe7f993d.js" as="script">
<link rel="preload" href="/assets/js/main.1d2740ab.js" as="script">
</head>
<body data-theme="light">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="My Site Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/favicon.ico" alt="My Site Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Home</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Personal Wiki</a><a class="navbar__item navbar__link" href="/blog">Blog</a><a href="https://blog.zharii.com/blog/rss.xml" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">[RSS]</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/dzharii/dzharii.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">ðŸŒœ</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">ðŸŒž</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div><div class="dsla-search-wrapper"><div class="dsla-search-field" data-tags="default,docs-default-current"></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">intro</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/azure-and-aad-articles">azure-and-aad-articles</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/bicycling-inbox">bicycling-inbox</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/bookmarklets">ðŸ“œ BookmarkLETS</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/bookmarks">ðŸ“Œ Bookmarks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/design-assets">design-assets</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-cpp">dev-cpp</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-csharp">dev-csharp</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-design-and-process">dev-design-and-process</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-ide-visualstudio">dev-ide-visualstudio</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-linux">dev-linux</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-oculus-quest2-inbox">dev-oculus-quest2-inbox</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-powershell">dev-powershell</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-projects-backend">dev-projects-backend</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-retro-8bit">dev-retro-8bit</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-scala-spark-synapse-bigdata">dev-scala-spark-synapse-bigdata</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-stable-diffusion-cpu">dev-stable-diffusion-cpu</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-webapi">dev-webapi</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/ffmpeg-inbox">ffmpeg-inbox</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/ffmpeg-recipes">ffmpeg-recipes</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/ffmpeg-snippets">ffmpeg-snippets</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/fun-meme-cry-like-a-man">fun-meme-cry-like-a-man</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/linux-odroid-cli-90s-project">linux-odroid-cli-90s-project</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/machine-learning">machine-learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/markdown-tools">markdown-tools</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/network">network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/prep-resources">prep-resources</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/psy-2022-07-28-BoundariesMeeting">psy-2022-07-28-BoundariesMeeting</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/psy-Nonviolent Communication">psy-Nonviolent Communication</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/psy-Unschedule">psy-Unschedule</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/public-apis">public-apis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/retro-www-buttons">retro-www-buttons</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/seattle">seattle</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/security-articles">security-articles</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/self-hosted-software">self-hosted-software</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/testing-distributed-systems">testing-distributed-systems</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/tools-mindmaps">tools-mindmaps</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/tools-WireGuard">tools-WireGuard</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/workplace-articles">workplace-articles</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/workplace-levels-and-layoffs">workplace-levels-and-layoffs</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/a-watch-reading-list/my-watch-reading-list-now">a-watch-reading-list</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/chatGPT/2022-12-14-5-Harmless-Ways-Software-Developers-Can-Have-Fun-at-Work.md">chatGPT</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/fun/2023-02-16-Seattle-Link-Pictograms">fun</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_VCh3" aria-current="page" href="/docs/gpt4-generated-content/2023-05-03-apache-spark-ref">gpt4-generated-content</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/gpt4-generated-content/2023-05-03-apache-spark-ref">Apache Spark Reference</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/resources/2023-02-12-Slides--Apache-Spark-Core-Deep-Dive-Proper-Optimization">resources</a></div></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Apache Spark Reference</h1><p>2023-05-03</p><p>[<!-- -->[<em>TOC</em>]<!-- -->]</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="inspired-by">Inspired by:<a class="hash-link" href="#inspired-by" title="Direct link to heading">â€‹</a></h2><ul><li><a href="https://github.com/f/awesome-chatgpt-prompts#act-as-a-tech-writer" target="_blank" rel="noopener noreferrer">Act as a Tech Writer</a></li><li><a href="https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener noreferrer">https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/Dataset.html</a></li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="prompt">Prompt:<a class="hash-link" href="#prompt" title="Direct link to heading">â€‹</a></h2><blockquote><p>Act as a tech writer and Apache Spark expert. You will act as a creative and engaging technical writer and create guides on how to do use a specific function in Scala Spark.
I will provide you with the function name, and you will do the best to provide a short description, usage example, and output. The usage example should be good to explain what the function do and clear to the reader (simple to understand). Also the description should be concise. Clear, simple and concise.
Scala codding requirements:</p><ol><li>Prefer writhing variable datatypes;</li><li>Use more simple Scala syntax for the scala beginners. Keep it clear for someone who knows Java, C#, JavaScript, but try to keep good scala programming style.;</li><li>Instead of expressions in the string literals, prefer using language constructs;</li><li>Try to avoid using tuples, like &quot;._1&quot;, &quot;._2&quot;. Don&#x27;t use &quot;employee =&gt; employee._2 &gt;= 30&quot;; rather introduce a case class;</li><li>Nicely format code with indentation; Format Array initialization, object initialization and the output to illustrate the hierarchical structure of the data.</li><li>Maintain line length 120 characters. This is Okay to have longer lines, but rather prefer splitting statement in multiple lines with nice indentation.
Please explain: Spark 3 Dataset:</li></ol><ul><li>unpersist()S</li><li>unpersist(blocking: Boolean): Dataset</li></ul></blockquote><h2 class="anchor anchorWithStickyNavbar_mojV" id="actions">Actions<a class="hash-link" href="#actions" title="Direct link to heading">â€‹</a></h2><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-collect">Function: collect()<a class="hash-link" href="#function-collect" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The collect() function in Spark 3 Dataset is used to retrieve all the elements of a dataset and return them as an array or a list in the driver program. It returns an array of objects that can be used to perform operations on the data or iterate over the elements.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to retrieve the information of all employees whose age is greater than or equal to 30. We can use the collect() function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val filteredEmployees = employeeDataset.filter(_.age &gt;= 30).collect()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. We apply the filter() function to select only the employees whose age is greater than or equal to 30. Finally, we use the collect() function to retrieve all the filtered employees.</p><p>Output:</p><p>The output of the collect() function is an array of objects. In our example, the filteredEmployees variable will contain an array of Employee instances representing the information of the filtered employees:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">Array(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that using the collect() function to retrieve all the elements of a large dataset can be computationally expensive and memory-intensive, and may lead to performance issues or even out-of-memory errors. Therefore, it is recommended to use this function with caution and only for small datasets or for testing purposes.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-collectaslist">Function: collectAsList()<a class="hash-link" href="#function-collectaslist" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The collectAsList() function in Spark 3 Dataset is similar to the collect() function but it returns the result as a Java List instead of an array. It collects all the elements of a dataset to the driver program and returns them as a Java List that can be used for further processing.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to retrieve the information of all employees whose age is greater than or equal to 30. We can use the collectAsList() function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val filteredEmployees = employeeDataset.filter(_.age &gt;= 30).collectAsList()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. We apply the filter() function to select only the employees whose age is greater than or equal to 30. Finally, we use the collectAsList() function to retrieve all the filtered employees as a Java List.</p><p>Output:</p><p>The output of the collectAsList() function is a Java List of objects. In our example, the filteredEmployees variable will contain a Java List of Employee instances representing the information of the filtered employees:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">List(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the collectAsList() function has the same performance and memory implications as the collect() function, and should be used with caution for large datasets.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-count">Function: count()<a class="hash-link" href="#function-count" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The count() function in Spark 3 Dataset is used to count the number of elements in a dataset. It returns the total number of rows or records present in the dataset.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to count the number of employees in the dataset. We can use the count() function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeCount = employeeDataset.count()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the count() function to count the total number of employees in the dataset.</p><p>Output:</p><p>The output of the count() function is a Long value representing the total number of elements in the dataset. In our example, the employeeCount variable will contain the total number of employees in the dataset, which is 4:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">4</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the count() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-describe">Function: describe()<a class="hash-link" href="#function-describe" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The describe() function in Spark 3 Dataset is used to summarize the statistics of a dataset. It computes the summary statistics, including count, mean, standard deviation, minimum, and maximum values, for each numeric column in the dataset.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to compute the summary statistics for the age and salary columns in the dataset. We can use the describe() function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val summaryStats = employeeDataset.describe(&quot;age&quot;, &quot;salary&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the describe() function to compute the summary statistics for the age and salary columns in the dataset.</p><p>Output:</p><p>The output of the describe() function is a new dataset containing the summary statistics for each column specified in the function. In our example, the summaryStats variable will contain a new dataset with the summary statistics for the age and salary columns:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">+-------+-----------------+-----------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|summary|              age|           salary|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------+-----------------+-----------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  count|                4|                4|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|   mean|            28.00|           3075.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| stddev|2.581988897471611|222.1084339724426|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    min|               25|           2800.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    max|               32|           3500.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------+-----------------+-----------------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the describe() function is a transformation operation that does not trigger the computation of the dataset, and can be used for large datasets without performance issues.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-first">Function: first()<a class="hash-link" href="#function-first" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The first() function in Spark 3 Dataset is used to retrieve the first element of the dataset. It returns the first element of the dataset as a new instance of the case class or a tuple, depending on the structure of the dataset.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to retrieve the details of the first employee in the dataset. We can use the first() function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val firstEmployee = employeeDataset.first()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the first() function to retrieve the details of the first employee in the dataset.</p><p>Output:</p><p>The output of the first() function is the first element of the dataset as a new instance of the case class or a tuple, depending on the structure of the dataset. In our example, the firstEmployee variable will contain a new instance of the Employee case class representing the details of the first employee in the dataset:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">Employee(&quot;John&quot;, 28, 3000.0)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the first() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-foreach">Function: foreach()<a class="hash-link" href="#function-foreach" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The foreach() function in Spark 3 Dataset is used to apply a given function to each element of the dataset. It iterates over the elements of the dataset and applies a user-defined function to each element.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to print the details of each employee in the dataset. We can use the foreach() function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDataset.foreach(employee =&gt; println(s&quot;Name: ${employee.name}, Age: ${employee.age}, Salary: ${employee.salary}&quot;))</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the foreach() function to print the details of each employee in the dataset.</p><p>Output:</p><p>The output of the foreach() function is the result of applying the given function to each element of the dataset. In our example, the output will be the details of each employee in the dataset printed to the console:</p><div class="codeBlockContainer_I0IT language-yaml theme-code-block"><div class="codeBlockContent_wNvx yaml"><pre tabindex="0" class="prism-code language-yaml codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> John</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Age</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">28</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Salary</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">3000.0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Mary</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Age</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">25</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Salary</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2800.0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Peter</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Age</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">32</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Salary</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">3500.0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Jane</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Age</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">27</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Salary</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">3200.0</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the foreach() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets. Also, the order of the elements in the output is not guaranteed, as Spark processes elements in parallel.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-foreachpartition">Function: foreachPartition()<a class="hash-link" href="#function-foreachpartition" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The foreachPartition() function in Spark 3 Dataset is used to apply a given function to each partition of the dataset. It is similar to the foreach() function, but instead of applying the function to each element, it applies the function to each partition of the dataset. It can be useful when we need to perform some expensive initialization or setup once per partition, rather than once per element.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to compute the total salary of each partition of the dataset. We can use the foreachPartition() function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDataset.foreachPartition(iterator =&gt; {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  var partitionTotalSalary = 0.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  iterator.foreach(employee =&gt; partitionTotalSalary += employee.salary)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  println(s&quot;Partition Total Salary: $partitionTotalSalary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">})</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the foreachPartition() function to compute the total salary of each partition of the dataset.</p><p>Output:</p><p>The output of the foreachPartition() function is the result of applying the given function to each partition of the dataset. In our example, the output will be the total salary of each partition of the dataset printed to the console:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">Partition Total Salary: 5800.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Partition Total Salary: 6700.0</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the foreachPartition() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets. Also, the order of the partitions in the output is not guaranteed, as Spark processes partitions in parallel.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-head">Function: head()<a class="hash-link" href="#function-head" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The head() function in Spark 3 Dataset is used to return the first n rows of a dataset as a new Dataset. It is similar to the take() function, but instead of returning an Array, it returns a new Dataset.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to get the first 2 rows of the dataset. We can use the head() function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val firstTwoEmployees = employeeDataset.head(2)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the head() function to get the first 2 rows of the dataset as a new Dataset.</p><p>Output:</p><p>The output of the head() function is a new Dataset containing the first n rows of the original dataset. In our example, the output will be a new Dataset containing the details of the first 2 employees:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|name|age|salary|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|John| 28|3000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Mary| 25|2800.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the head() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets. Also, the order of the rows in the output is not guaranteed, as Spark processes data in parallel.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-reduce">Function: reduce()<a class="hash-link" href="#function-reduce" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The reduce() function in Spark 3 Dataset is used to aggregate the elements of a dataset using a specified binary operator. The binary operator must be associative and commutative, and it is applied to pairs of elements until a single result is obtained.</p><p>Usage Example:</p><p>Suppose we have a dataset containing a sequence of integers, and we want to compute the sum of all these integers using the reduce() function. We can use the reduce() function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">val numbers = Seq(1, 2, 3, 4, 5)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val numbersDataset = numbers.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val sum = numbersDataset.reduce((x, y) =&gt; x + y)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of integers. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the reduce() function to compute the sum of all the integers in the dataset.</p><p>Output:</p><p>The output of the reduce() function is a single value obtained by applying the binary operator to pairs of elements until a single result is obtained. In our example, the output will be the sum of all the integers in the dataset:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">15</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the reduce() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets. Also, the order of the elements in the dataset is not guaranteed, as Spark processes data in parallel.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-show">Function: show()<a class="hash-link" href="#function-show" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The show() function in Spark 3 Dataset is used to display the contents of a dataset in a tabular format. It is useful for debugging and exploring the contents of a dataset.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to display the contents of the dataset in a tabular format. We can use the show() function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDataset.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the show() function to display the contents of the dataset in a tabular format.</p><p>Output:</p><p>The output of the show() function is a tabular representation of the dataset, with each row corresponding to a record in the dataset. In our example, the output will be:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| name|age|salary|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| John| 28|3000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Mary| 25|2800.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Peter| 32|3500.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Jane| 27|3200.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the show() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets. It is intended for interactive use, such as exploring data in a notebook or shell, rather than for production use.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-summary">Function: summary()<a class="hash-link" href="#function-summary" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The summary() function in Spark 3 Dataset is used to compute summary statistics for the columns of a dataset. It provides a quick way to obtain basic statistics for a large number of columns in a dataset.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to compute summary statistics for the age and salary columns. We can use the summary() function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDataset.select(&quot;age&quot;, &quot;salary&quot;).summary().show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the select() function to select the age and salary columns, and then apply the summary() function to compute summary statistics for these columns.</p><p>Output:</p><p>The output of the summary() function is a tabular representation of the summary statistics for the selected columns, including the count, mean, standard deviation, minimum, and maximum values. In our example, the output will be:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">+-------+-----------------+------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|summary|              age|            salary|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------+-----------------+------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  count|                4|                 4|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|   mean|             28.0|            3125.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| stddev|2.160246899469287| 298.8118022040274|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    min|               25|            2800.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    25%|               27|            3000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    50%|               28|            3200.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    75%|               32|            3325.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    max|               32|            3500.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------+-----------------+------------------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the summary() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets. It is intended for quick analysis of data, rather than for production use.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-tail">Function: tail()<a class="hash-link" href="#function-tail" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The tail() function in Spark 3 Dataset returns the last n rows of the dataset. It is useful when we want to inspect the end of a dataset or when we need to extract only the last few rows of a dataset for further processing.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to extract the last 2 rows of this dataset. We can use the tail() function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDataset.tail(2).foreach(println)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the tail() function to extract the last 2 rows of the dataset, and then apply the foreach() function to print these rows.</p><p>Output:</p><p>The output of the tail() function is a Dataset containing the last n rows of the original dataset. In our example, the output will be:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">Employee(Peter,32,3500.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Employee(Jane,27,3200.0)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the tail() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets. It is intended for quick inspection of data, rather than for production use.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-take">Function: take()<a class="hash-link" href="#function-take" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The take() function in Spark 3 Dataset returns an array of the first n rows of the dataset. It is useful when we need to extract only a few rows of a dataset for further processing, and we don&#x27;t want to load the entire dataset into memory.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to extract the first 3 rows of this dataset. We can use the take() function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val firstThreeRows = employeeDataset.take(3)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">firstThreeRows.foreach(println)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the take() function to extract the first 3 rows of the dataset, and then apply the foreach() function to print these rows.</p><p>Output:</p><p>The output of the take() function is an array containing the first n rows of the original dataset. In our example, the output will be:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">Employee(John,28,3000.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Employee(Mary,25,2800.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Employee(Peter,32,3500.0)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the take() function is a transformation operation that does not trigger the computation of the entire dataset, and is useful for extracting only a few rows of data for further processing.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-takeaslist">Function: takeAsList()<a class="hash-link" href="#function-takeaslist" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The takeAsList() function in Spark 3 Dataset is similar to the take() function, but it returns a Java list instead of a Scala array. It is useful when we want to use the extracted rows of a dataset in Java-based APIs that accept Java collections.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to extract the first 3 rows of this dataset as a Java list. We can use the takeAsList() function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val firstThreeRows = employeeDataset.takeAsList(3)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">firstThreeRows.forEach(println)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the takeAsList() function to extract the first 3 rows of the dataset, and then apply the forEach() function to print these rows.</p><p>Output:</p><p>The output of the takeAsList() function is a Java list containing the first n rows of the original dataset. In our example, the output will be:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">Employee(John,28,3000.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Employee(Mary,25,2800.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Employee(Peter,32,3500.0)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the takeAsList() function is a transformation operation that does not trigger the computation of the entire dataset, and is useful for extracting only a few rows of data for further processing in Java-based APIs.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-tolocaliterator">Function: toLocalIterator()<a class="hash-link" href="#function-tolocaliterator" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The toLocalIterator() function in Spark 3 Dataset allows iterating over the elements of a dataset on the driver node, one partition at a time. This function can be useful when we want to perform some custom processing on the dataset elements using code that is only available on the driver node.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to print the name and age of each employee by iterating over the dataset on the driver node using the toLocalIterator() function. We can do this as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val iterator = employeeDataset.toLocalIterator()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">while (iterator.hasNext) {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  val employee = iterator.next()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  println(s&quot;Name: ${employee.name}, Age: ${employee.age}&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the toLocalIterator() function to obtain an iterator over the elements of the dataset on the driver node, and then iterate over this iterator using a while loop to print the name and age of each employee.</p><p>Output:</p><p>The output of the toLocalIterator() function is the result of the custom processing performed on the dataset elements on the driver node. In our example, the output will be:</p><div class="codeBlockContainer_I0IT language-yaml theme-code-block"><div class="codeBlockContent_wNvx yaml"><pre tabindex="0" class="prism-code language-yaml codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> John</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Age</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">28</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Mary</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Age</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">25</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Peter</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Age</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">32</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Jane</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Age</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">27</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the toLocalIterator() function can be used to perform custom processing on the dataset elements using any code that is available on the driver node.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="basic-dataset-functions">Basic Dataset functions<a class="hash-link" href="#basic-dataset-functions" title="Direct link to heading">â€‹</a></h2><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-asschema-structtype-dataframe">Function: as(schema: StructType): DataFrame<a class="hash-link" href="#function-asschema-structtype-dataframe" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The <code>as</code> function in Spark 3 Dataset API is used to convert a Dataset of generic objects to a strongly typed Dataset by specifying a schema. The function takes a StructType object as an argument that represents the schema of the resulting strongly typed Dataset. The StructType object can be created using the <code>StructType</code> class and by adding <code>StructField</code> objects that represent the columns of the resulting strongly typed Dataset.</p><p>Usage Example:</p><p>Suppose we have a CSV file containing the details of employees, including their names, ages, and salaries. We want to read this CSV file into a DataFrame and then convert it into a strongly typed Dataset by specifying a schema. We can do this as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, DoubleType}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeSchema = new StructType()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .add(StructField(&quot;name&quot;, StringType, true))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .add(StructField(&quot;age&quot;, IntegerType, true))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .add(StructField(&quot;salary&quot;, DoubleType, true))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDF = spark.read.format(&quot;csv&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;header&quot;, &quot;true&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;inferSchema&quot;, &quot;true&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .load(&quot;employee.csv&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDS = employeeDF.as(employeeSchema)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDS.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a <code>StructType</code> object called <code>employeeSchema</code> that represents the schema of the resulting strongly typed Dataset. This schema consists of three columns: name (StringType), age (IntegerType), and salary (DoubleType).</p><p>Then, we read the CSV file into a DataFrame using the <code>read</code> method of the <code>SparkSession</code> object. We set the <code>header</code> option to <code>true</code> to indicate that the first row of the CSV file contains the column names, and set the <code>inferSchema</code> option to <code>true</code> to infer the data types of the columns from the contents of the CSV file.</p><p>Next, we use the <code>as</code> function to convert the DataFrame into a strongly typed Dataset by specifying the <code>employeeSchema</code>. Finally, we call the <code>show</code> method on the resulting Dataset to display its contents.</p><p>Output:</p><p>The output of the <code>show</code> method will be a table that displays the contents of the strongly typed Dataset, which will have the specified schema. For example:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| name|age|salary|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|John | 28|3000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Mary | 25|2800.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Peter| 32|3500.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Jane | 27|3200.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that if the data in the DataFrame does not match the specified schema, the <code>as</code> function will throw an exception. It is important to ensure that the schema is correctly specified to avoid errors at runtime.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-cache">Function: cache()<a class="hash-link" href="#function-cache" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The <code>cache</code> function in Spark 3 Dataset API is used to persist the contents of a Dataset in memory, so that subsequent operations on the Dataset can be performed more quickly. This function stores the Dataset in the cache memory of the executor nodes, which makes it faster to access and reduces the number of times the Dataset needs to be computed.</p><p>Usage Example:</p><p>Suppose we have a Dataset of employee data, and we want to perform multiple operations on this Dataset. In order to avoid recomputing the Dataset every time an operation is performed, we can cache the Dataset in memory using the <code>cache</code> function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDS = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;).as[Employee]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDS.cache()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val filteredDS = employeeDS.filter(employee =&gt; employee.age &gt;= 30)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val count = filteredDS.count()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(s&quot;Number of employees above 30: $count&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a Dataset of employee data using the <code>toDF</code> method and then converting it into a strongly typed Dataset using the <code>as</code> method. Then we call the <code>cache</code> method on this Dataset to cache it in memory.</p><p>Next, we perform an operation on the cached Dataset by filtering out the employees whose age is less than 30 using the <code>filter</code> function. Finally, we call the <code>count</code> function to count the number of employees whose age is greater than or equal to 30.</p><p>Output:</p><p>The output of this program will be the number of employees whose age is greater than or equal to 30. Since the Dataset has been cached in memory, the <code>filter</code> and <code>count</code> operations will be performed more quickly than if the Dataset had not been cached.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-checkpointeager-boolean">Function: checkpoint(eager: Boolean)<a class="hash-link" href="#function-checkpointeager-boolean" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The <code>checkpoint</code> function in Spark 3 Dataset API is used to save the contents of a Dataset to a checkpoint file in HDFS. This function can be used to avoid recomputing a Dataset from scratch if a failure occurs during its computation. </p><p>Usage Example:</p><p>Suppose we have a Dataset of employee data, and we want to checkpoint this Dataset to avoid recomputing it from scratch if a failure occurs. We can use the <code>checkpoint</code> function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDS = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;).as[Employee]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDS.checkpoint()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val filteredDS = employeeDS.filter(employee =&gt; employee.age &gt;= 30)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val count = filteredDS.count()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(s&quot;Number of employees above 30: $count&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a Dataset of employee data using the <code>toDF</code> method and then converting it into a strongly typed Dataset using the <code>as</code> method. Then we call the <code>checkpoint</code> method on this Dataset to save its contents to a checkpoint file.</p><p>Next, we perform an operation on the Dataset by filtering out the employees whose age is less than 30 using the <code>filter</code> function. Finally, we call the <code>count</code> function to count the number of employees whose age is greater than or equal to 30.</p><p>Output:</p><p>The output of this program will be the number of employees whose age is greater than or equal to 30. If a failure occurs during the computation of this Dataset, it can be recovered from the checkpoint file rather than being recomputed from scratch.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-columns-arraystring">Function: columns: Array<!-- -->[String]<a class="hash-link" href="#function-columns-arraystring" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The <code>columns</code> function in Spark 3 DataFrame API returns an array of column names present in the DataFrame. It is a simple and quick way to get the list of column names present in the DataFrame.</p><p>Usage Example:</p><p>Suppose we have a DataFrame containing employee data, and we want to get the list of column names present in this DataFrame. We can use the <code>columns</code> function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDF = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val columnNames = employeeDF.columns</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(&quot;Columns in the DataFrame: &quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">columnNames.foreach(println)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a DataFrame of employee data using the <code>toDF</code> method. Then we call the <code>columns</code> method on this DataFrame to get the list of column names present in it.</p><p>Finally, we print the list of column names using a <code>foreach</code> loop.</p><p>Output:</p><p>The output of this program will be a list of column names present in the DataFrame:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">Columns in the DataFrame:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">name</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">age</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">salary</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-createglobaltempviewviewname-string-unit">Function: createGlobalTempView(viewName: String): Unit<a class="hash-link" href="#function-createglobaltempviewviewname-string-unit" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The <code>createGlobalTempView</code> function in Spark 3 DataFrame API creates a global temporary view with the specified name. A global temporary view is a view that is accessible across all SparkSessions and even across different applications. It is tied to a system preserved database <code>global_temp</code>.</p><p>Usage Example:</p><p>Suppose we have a DataFrame containing employee data, and we want to create a global temporary view named &quot;employee_view&quot; using this DataFrame. We can use the <code>createGlobalTempView</code> function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDF = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDF.createGlobalTempView(&quot;employee_view&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a DataFrame of employee data using the <code>toDF</code> method. Then we call the <code>createGlobalTempView</code> method on this DataFrame with the view name &quot;employee_view&quot; to create a global temporary view.</p><p>Output:</p><p>The output of this program will not be visible on the console. The global temporary view is now created and can be accessed from any SparkSession in the same application or across different applications.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-createorreplaceglobaltempviewviewname-string-unit">Function: createOrReplaceGlobalTempView(viewName: String): Unit<a class="hash-link" href="#function-createorreplaceglobaltempviewviewname-string-unit" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The <code>createOrReplaceGlobalTempView</code> function in Spark 3 DataFrame API creates a global temporary view with the specified name, or replaces an existing global temporary view with the same name. A global temporary view is a view that is accessible across all SparkSessions and even across different applications. It is tied to a system preserved database <code>global_temp</code>.</p><p>Usage Example:</p><p>Suppose we have a DataFrame containing employee data, and we want to create a global temporary view named &quot;employee_view&quot; using this DataFrame. We can use the <code>createOrReplaceGlobalTempView</code> function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDF = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDF.createOrReplaceGlobalTempView(&quot;employee_view&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a DataFrame of employee data using the <code>toDF</code> method. Then we call the <code>createOrReplaceGlobalTempView</code> method on this DataFrame with the view name &quot;employee_view&quot; to create or replace a global temporary view.</p><p>Output:</p><p>The output of this program will not be visible on the console. The global temporary view is now created or replaced and can be accessed from any SparkSession in the same application or across different applications.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-createtempviewviewname-string-unit">Function: createTempView(viewName: String): Unit<a class="hash-link" href="#function-createtempviewviewname-string-unit" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The <code>createTempView</code> function in Spark 3 DataFrame API creates a local temporary view with the specified name. A local temporary view is only accessible within the same SparkSession that created it.</p><p>Usage Example:</p><p>Suppose we have a DataFrame containing employee data, and we want to create a local temporary view named &quot;employee_view&quot; using this DataFrame. We can use the <code>createTempView</code> function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDF = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDF.createTempView(&quot;employee_view&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a DataFrame of employee data using the <code>toDF</code> method. Then we call the <code>createTempView</code> method on this DataFrame with the view name &quot;employee_view&quot; to create a local temporary view.</p><p>Output:</p><p>The output of this program will not be visible on the console. The local temporary view is now created and can be accessed within the same SparkSession using the specified view name.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-dtypes-arraystring-string">Function: dtypes: Array<!-- -->[(String, String)]<a class="hash-link" href="#function-dtypes-arraystring-string" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The <code>dtypes</code> function in Spark 3 DataFrame API returns an array of tuples containing the name of each column in the DataFrame and its corresponding data type. The data type is represented as a string.</p><p>Usage Example:</p><p>Suppose we have a DataFrame containing employee data, and we want to get the data types of each column in this DataFrame. We can use the <code>dtypes</code> function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDF = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataTypes = employeeDF.dtypes</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dataTypes.foreach(println)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a DataFrame of employee data using the <code>toDF</code> method. Then we call the <code>dtypes</code> function on this DataFrame to get the data types of each column. Finally, we use a foreach loop to print out the name and data type of each column.</p><p>Output:</p><p>The output of this program will be:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">(name,StringType)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(age,IntegerType)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(salary,DoubleType)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>This shows that the &quot;name&quot; column has a String data type, the &quot;age&quot; column has an Integer data type, and the &quot;salary&quot; column has a Double data type.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-explain-unit">Function: explain(): Unit<a class="hash-link" href="#function-explain-unit" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The <code>explain</code> function in Spark 3 DataFrame API is used to display the physical plan of a DataFrame or Dataset. It provides an easy way to understand how Spark will execute a given query. The <code>explain</code> function returns the execution plan as a string, which can be printed to the console or written to a log file.</p><p>Usage Example:</p><p>Suppose we have a DataFrame containing employee data, and we want to see how Spark will execute a query on this DataFrame. We can use the <code>explain</code> function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDF = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDF.filter($&quot;age&quot; &gt; 25).explain()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a DataFrame of employee data using the <code>toDF</code> method. Then we apply a filter to this DataFrame to select only those employees whose age is greater than 25. Finally, we call the <code>explain</code> function on this filtered DataFrame to display the physical plan of this query.</p><p>Output:</p><p>The output of this program will be:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">== Physical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">*(1) Filter (isnotnull(age#1) &amp;&amp; (age#1 &gt; 25))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- *(1) Scan ExistingRDD[name#0,age#1,salary#2]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>This output shows the physical plan that Spark will use to execute this query. It indicates that Spark will first scan an existing RDD containing the data, then apply the filter to select only those rows where the age is greater than 25.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-explainextended-boolean-unit">Function: explain(extended: Boolean): Unit<a class="hash-link" href="#function-explainextended-boolean-unit" title="Direct link to heading">â€‹</a></h4><p>Description:</p><p>The <code>explain</code> function in Spark 3 DataFrame API is used to display the physical plan of a DataFrame or Dataset, with optional extended information. It provides an easy way to understand how Spark will execute a given query. The <code>explain</code> function returns the execution plan as a string, which can be printed to the console or written to a log file.</p><p>Usage Example:</p><p>Suppose we have a DataFrame containing employee data, and we want to see how Spark will execute a query on this DataFrame. We can use the <code>explain</code> function as shown below:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDF = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDF.filter($&quot;age&quot; &gt; 25).explain(true)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first create a DataFrame of employee data using the <code>toDF</code> method. Then we apply a filter to this DataFrame to select only those employees whose age is greater than 25. Finally, we call the <code>explain</code> function on this filtered DataFrame with the <code>extended</code> parameter set to <code>true</code> to display the physical plan of this query with extended information.</p><p>Output:</p><p>The output of this program will be:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">== Parsed Logical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&#x27;Filter (&#x27;age &gt; 25)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- AnalysisBarrier[name#0, age#1, salary#2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">== Analyzed Logical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">name: string, age: int, salary: double</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Filter (age#1 &gt; 25)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- Project [name#0, age#1, salary#2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   +- AnalysisBarrier[name#0, age#1, salary#2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">== Optimized Logical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Filter (isnotnull(age#1) &amp;&amp; (age#1 &gt; 25))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- Project [name#0, age#1, salary#2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   +- Scan ExistingRDD[name#0,age#1,salary#2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">== Physical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">*(1) Filter (isnotnull(age#1) &amp;&amp; (age#1 &gt; 25))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- *(1) Scan ExistingRDD[name#0,age#1,salary#2]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>This output shows the physical plan that Spark will use to execute this query, with additional information about the logical plan and the optimized plan. The <code>extended</code> parameter set to <code>true</code> adds additional details to the plan, making it easier to understand how Spark will execute the query.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="the-explain-method-in-spark-provides-information-about-the-physical-and-logical-plans-of-a-query-execution-it-helps-in-understanding-how-spark-executes-a-query-and-can-be-useful-in-optimizing-the-query">The <code>explain()</code> method in Spark provides information about the physical and logical plans of a query execution. It helps in understanding how Spark executes a query and can be useful in optimizing the query.<a class="hash-link" href="#the-explain-method-in-spark-provides-information-about-the-physical-and-logical-plans-of-a-query-execution-it-helps-in-understanding-how-spark-executes-a-query-and-can-be-useful-in-optimizing-the-query" title="Direct link to heading">â€‹</a></h4><p>The <code>explain(mode: String)</code> method in Spark Dataset API is used to display the physical plan of the query execution in a specific mode. The supported modes are <code>formatted</code>, <code>extended</code>, and <code>codegen</code>.</p><ul><li>The <code>formatted</code> mode displays the physical plan in a human-readable format.</li><li>The <code>extended</code> mode provides additional details about the physical plan, such as the number of output rows, the size of the output in bytes, and the estimated cost of the execution.</li><li>The <code>codegen</code> mode displays the generated code for the query execution.</li></ul><p>Here&#x27;s an example usage of the <code>explain()</code> method:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.functions._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = Seq((1,&quot;a&quot;), (2, &quot;b&quot;), (3, &quot;c&quot;)).toDF(&quot;id&quot;, &quot;name&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.filter(col(&quot;id&quot;) &gt; 1).explain(&quot;extended&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">== Parsed Logical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&#x27;Filter (&#x27;id &gt; 1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- LogicalRDD [id#0, name#1], false</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">== Analyzed Logical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">id: int, name: string</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Filter (id#0 &gt; 1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- LogicalRDD [id#0, name#1], false</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">== Optimized Logical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Filter (id#0 &gt; 1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- LogicalRDD [id#0, name#1], false</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">== Physical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">*(1) Filter (isnotnull(id#0) AND (id#0 &gt; 1))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- *(1) Scan JDBCRelation((SELECT `id`, `name` FROM `test`) tmpTable) [numPartitions=1] []</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>As you can see in the output, the physical plan of the query execution is displayed in the <code>extended</code> mode.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="hint">hint()<a class="hash-link" href="#hint" title="Direct link to heading">â€‹</a></h4><p>The <code>hint</code> method in Apache Spark allows you to provide a hint to the query optimizer. Hints provide additional information about the dataset to the optimizer that can help it make better decisions about how to execute the query. You can use the <code>hint</code> method to specify optimizer rules or optimization hints in JSON format.</p><p>The <code>hint</code> method takes two parameters: the name of the hint and its parameters. The hint name is a string that specifies the type of hint you want to use. The parameters are optional and are used to provide additional information to the hint.</p><p>Here&#x27;s an example of how to use the <code>hint</code> method:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">val df = spark.read.parquet(&quot;path/to/parquet/file&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Specify the hint &quot;broadcast&quot; with parameter &quot;join&quot; to optimize a join</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.hint(&quot;broadcast&quot;, &quot;join&quot;).show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In the example above, we read in a Parquet file and apply the <code>hint</code> method to the resulting <code>DataFrame</code>. We provide the hint name &quot;broadcast&quot; and the parameter &quot;join&quot; to optimize a join operation. Finally, we call the <code>show</code> method to display the result.</p><p>Note that hints should be used with caution, and you should only use them if you have a good understanding of the query optimizer and how it works. In most cases, the optimizer will make the right decisions about how to execute a query without hints.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="inputfiles-arraystring">inputFiles: Array<!-- -->[String]<a class="hash-link" href="#inputfiles-arraystring" title="Direct link to heading">â€‹</a></h4><p>The <code>inputFiles</code> method is available on Spark 2.0+ and returns an array of strings that represent the input files that were read to create the current <code>DataFrame</code> or <code>Dataset</code>.</p><p>Here&#x27;s an example:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">val df = spark.read.json(&quot;path/to/json/files&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val inputFiles = df.inputFiles</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In the example above, we read in a JSON file and create a <code>DataFrame</code> <code>df</code>. Then, we use the <code>inputFiles</code> method to retrieve the list of input files that were used to create the <code>DataFrame</code>. </p><p>Note that if the <code>DataFrame</code> was created from multiple input files, the <code>inputFiles</code> method returns an array with the path of all files. Also, if the input file is a directory, then all the files within that directory will be returned as part of the array.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="spark-3-dataset-isempty-boolean">Spark 3 Dataset: isEmpty: Boolean<a class="hash-link" href="#spark-3-dataset-isempty-boolean" title="Direct link to heading">â€‹</a></h4><p>The <code>isEmpty</code> method in Spark 3 Dataset API returns a Boolean that indicates if the Dataset is empty or not. If the Dataset is empty, it returns true; otherwise, it returns false.</p><p>Usage</p><p>The <code>isEmpty</code> method is called on a Dataset object and returns a Boolean value indicating if the Dataset is empty or not. Here is the basic syntax of calling the <code>isEmpty</code> method:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">val ds: Dataset[Person] = Seq.empty[Person].toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val isEmpty: Boolean = ds.isEmpty</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Example</p><p>Here&#x27;s an example of using the <code>isEmpty</code> method to check if a Dataset is empty or not:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                .appName(&quot;isEmptyExample&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import spark.implicits._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val emptyDs = Seq.empty[Person].toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val nonEmptyDs = Seq(Person(&quot;Alice&quot;, 25), Person(&quot;Bob&quot;, 30)).toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(emptyDs.isEmpty)      // true</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(nonEmptyDs.isEmpty)   // false</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Output</p><p>The output of the example code will be:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">true</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">false</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the <code>isEmpty</code> method returns <code>true</code> for an empty Dataset and <code>false</code> for a non-empty Dataset.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-name-islocal">Function Name: isLocal()<a class="hash-link" href="#function-name-islocal" title="Direct link to heading">â€‹</a></h4><p>Description:
isLocal() is a boolean function in Spark Dataset API that checks if the current Dataset is local or distributed across a cluster.</p><p>Usage:
Assuming a Dataset &quot;ds&quot; has been created, we can use the isLocal() function as follows:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">val isDsLocal: Boolean = ds.isLocal</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Output:
The isLocal() function returns true if the Dataset is local, meaning that all data is stored on a single machine, and false if the data is distributed across a cluster.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-javardd">Function: javaRDD()<a class="hash-link" href="#function-javardd" title="Direct link to heading">â€‹</a></h4><p>The <code>javaRDD</code> function is a method of the Spark 3 Dataset API in Scala. It returns a JavaRDD object that represents the current Dataset as an RDD of Java objects. The elements in the RDD are of type T, which is the same type as the elements in the Dataset.</p><p>Usage Example:</p><p>Suppose you have a Dataset of employee data and you want to convert it to a JavaRDD. Here&#x27;s how you can use the <code>javaRDD</code> function to do that:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.api.java.JavaRDD</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeData = Seq(Employee(&quot;John Doe&quot;, 25), Employee(&quot;Jane Smith&quot;, 30), Employee(&quot;Bob Johnson&quot;, 40))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDS = employeeData.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeRDD: JavaRDD[Employee] = employeeDS.javaRDD()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Output:</p><p>In this example, the <code>javaRDD</code> function is called on the <code>employeeDS</code> Dataset, which converts it to a JavaRDD of Employee objects. The <code>employeeRDD</code> JavaRDD can now be used with Java APIs that require RDDs, instead of using the Dataset APIs.</p><p>Note: The JavaRDD object returned by <code>javaRDD</code> is immutable, meaning it cannot be modified.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-name-localcheckpoint">Function Name: localCheckpoint<a class="hash-link" href="#function-name-localcheckpoint" title="Direct link to heading">â€‹</a></h4><p>Description: The localCheckpoint() function is used to persist a Dataset on the local file system. Unlike checkpoint() function which persists the Dataset on a fault-tolerant storage system, localCheckpoint() function stores the Dataset on the local file system which is not fault-tolerant. The purpose of localCheckpoint() function is to improve the performance of iterative algorithms.</p><p>Usage: The localCheckpoint() function can be used on a Dataset to persist it on the local file system. Once persisted, the Dataset can be retrieved from the local file system and used for further processing.</p><p>Example:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                .appName(&quot;localCheckpoint Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(1, 2, 3, 4, 5)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val ds = spark.createDataset(data)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ds.localCheckpoint()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val result = ds.map(x =&gt; x * 2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">result.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| _1|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  2|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  4|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  6|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  8|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| 10|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In the above example, we create a Dataset &#x27;ds&#x27; from a sequence of integers. We then apply the localCheckpoint() function on it to persist it on the local file system. Later, we apply the map() function on the Dataset to multiply each element by 2 and store the result in &#x27;result&#x27; variable. Finally, we print the output of the &#x27;result&#x27; variable using the show() function.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="spark-3-dataset-persistnewlevel-storagelevel">Spark 3 Dataset: persist(newLevel: StorageLevel)<a class="hash-link" href="#spark-3-dataset-persistnewlevel-storagelevel" title="Direct link to heading">â€‹</a></h4><p>The <code>persist(newLevel: StorageLevel)</code> function in Spark 3 Dataset API is used to persist the dataset with the specified storage level in memory or disk or both. The storage level determines where the data is stored and in what format, and it can be one of the following: </p><ul><li>MEMORY_ONLY</li><li>MEMORY_AND_DISK</li><li>MEMORY_ONLY_SER</li><li>MEMORY_AND_DISK_SER</li><li>DISK_ONLY</li><li>MEMORY_ONLY_2</li><li>MEMORY_AND_DISK_2</li><li>MEMORY_ONLY_SER_2</li><li>MEMORY_AND_DISK_SER_2</li><li>OFF_HEAP</li></ul><p>By default, the storage level is set to MEMORY_AND_DISK, which means that the data is stored both in memory and on disk.</p><p>Usage</p><p>Here&#x27;s an example of how to use <code>persist(newLevel: StorageLevel)</code> in Scala:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.storage.StorageLevel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset = Seq((1, &quot;John&quot;), (2, &quot;Sarah&quot;), (3, &quot;Alex&quot;)).toDF(&quot;id&quot;, &quot;name&quot;).as[(Int, String)]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Persisting the dataset with MEMORY_ONLY storage level</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dataset.persist(StorageLevel.MEMORY_ONLY)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Doing some transformations on the persisted dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val filteredDataset = dataset.filter(_._2.startsWith(&quot;A&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Unpersisting the dataset from memory</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dataset.unpersist()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In the above example, we created a simple dataset of (id, name) pairs, and persisted it with the MEMORY_ONLY storage level. We then performed some transformations on the persisted dataset, and finally unpersisted it from memory.</p><p>Output</p><p>There is no output generated by <code>persist(newLevel: StorageLevel)</code> function. However, it can improve the performance of subsequent operations on the dataset by avoiding the need to recompute the dataset from scratch each time it is accessed.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="printschemalevel-int-unit">printSchema(level: Int): Unit<a class="hash-link" href="#printschemalevel-int-unit" title="Direct link to heading">â€‹</a></h4><p>The <code>printSchema</code> method is used to print the schema of a Spark <code>Dataset</code>. The schema is a representation of the data structure of the <code>Dataset</code>, which includes the names and data types of its columns.</p><p>The method takes an optional <code>level</code> parameter that specifies the indentation level for nested fields. By default, the level is set to 0, which means that all fields are printed at the same level.</p><p>Usage Example:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Row}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;printSchemaExample&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// define a schema for a DataFrame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val schema = StructType(Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  StructField(&quot;name&quot;, StringType, nullable = false),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  StructField(&quot;age&quot;, IntegerType, nullable = true)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// create a sample DataFrame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Row(&quot;Alice&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Row(&quot;Bob&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Row(&quot;Charlie&quot;, null)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// print the schema</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.printSchema()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">root</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> |-- name: string (nullable = false)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> |-- age: integer (nullable = true)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_mojV" id="schema-structtype">schema: StructType<a class="hash-link" href="#schema-structtype" title="Direct link to heading">â€‹</a></h4><p><code>schema</code> is a method available in Spark 3&#x27;s Dataset API. It returns the schema of the Dataset as a StructType object. The schema describes the structure of the data in the Dataset, including column names, data types, and nullable constraints.</p><p>Usage Example:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.functions.col</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq((&quot;John&quot;, 25), (&quot;Bob&quot;, 30), (&quot;Jane&quot;, 35))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = spark.createDataFrame(data).toDF(&quot;name&quot;, &quot;age&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val mySchema: StructType = df.schema</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(&quot;Schema of my DataFrame:&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mySchema.printTreeString()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">Schema of my DataFrame:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">root</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> |-- name: string (nullable = true)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> |-- age: integer (nullable = false)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we create a DataFrame called <code>df</code> from a sequence of tuples. We then use the <code>schema</code> method to obtain the schema of the DataFrame as a <code>StructType</code> object called <code>mySchema</code>. Finally, we use the <code>printTreeString</code> method to print the schema in a tree-like format. The output shows that the DataFrame has two columns, &quot;name&quot; and &quot;age&quot;, and the data types of these columns are String and Integer, respectively.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="storagelevel-storagelevel">storageLevel: StorageLevel<a class="hash-link" href="#storagelevel-storagelevel" title="Direct link to heading">â€‹</a></h4><p><code>storageLevel</code> is a method in Spark 3 Dataset API that returns the storage level of the Dataset.</p><p>The storage level is the level of caching used to persist the Dataset in memory or disk, and it is specified using the <code>persist()</code> method. It determines the trade-off between memory usage and performance, as a higher storage level will use more memory but result in faster execution times.</p><p>The <code>storageLevel</code> method returns an object of the <code>StorageLevel</code> class, which represents the storage level of the Dataset. The <code>StorageLevel</code> class has several static methods that can be used to create a storage level object with different caching options, such as <code>MEMORY_ONLY</code>, <code>MEMORY_AND_DISK</code>, <code>DISK_ONLY</code>, etc.</p><p>Example usage:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.storage.StorageLevel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// create a DataFrame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = Seq((1, &quot;John&quot;), (2, &quot;Mary&quot;), (3, &quot;Bob&quot;)).toDF(&quot;id&quot;, &quot;name&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// cache the DataFrame with MEMORY_ONLY storage level</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.persist(StorageLevel.MEMORY_ONLY)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// get the storage level of the DataFrame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val level = df.storageLevel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// print the storage level</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(level)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">Memory Deserialized 1x Replicated</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we create a DataFrame <code>df</code> and cache it in memory with <code>MEMORY_ONLY</code> storage level using the <code>persist()</code> method. We then call the <code>storageLevel</code> method on the DataFrame to get its storage level, and print it to the console. The output shows that the DataFrame is stored in memory, deserialized, and replicated once.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-todfcolnames-string-dataframe">Function: toDF(colNames: String*): DataFrame<a class="hash-link" href="#function-todfcolnames-string-dataframe" title="Direct link to heading">â€‹</a></h4><p>The <code>toDF</code> function is used to convert a <code>Dataset</code> into a <code>DataFrame</code>, where the column names are specified by the input argument <code>colNames</code>. This function returns a <code>DataFrame</code>.</p><p>Usage Example:</p><p>Suppose we have a case class <code>Person</code> and a <code>Dataset</code> of <code>Person</code> objects. We can convert this <code>Dataset</code> into a <code>DataFrame</code> as follows:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val people = Seq(Person(&quot;Alice&quot;, 25), Person(&quot;Bob&quot;, 30)).toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = people.toDF(&quot;person_name&quot;, &quot;person_age&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">+------------+----------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|person_name |person_age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+------------+----------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice       |        25|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Bob         |        30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+------------+----------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Function: toDF(): DataFrame</p><p>The <code>toDF</code> function is used to convert a <code>Dataset</code> into a <code>DataFrame</code>, where the column names are automatically generated based on the field names of the case class used to create the <code>Dataset</code>. This function returns a <code>DataFrame</code>.</p><p>Usage Example:</p><p>Suppose we have a case class <code>Person</code> and a <code>Dataset</code> of <code>Person</code> objects. We can convert this <code>Dataset</code> into a <code>DataFrame</code> as follows:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val people = Seq(Person(&quot;Alice&quot;, 25), Person(&quot;Bob&quot;, 30)).toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = people.toDF()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice| 25|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Bob| 30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note: The <code>toDF</code> function generates column names based on the field names of the case class used to create the <code>Dataset</code>. It removes the prefix <code>_$outer</code> from the column names, which is added by Scala&#x27;s inner classes. If there are no field names in the case class, then the generated column names will be &quot;col0&quot;, &quot;col1&quot;, etc.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="tojavardd-javarddt">toJavaRDD: JavaRDD<!-- -->[T]<a class="hash-link" href="#tojavardd-javarddt" title="Direct link to heading">â€‹</a></h4><p><code>toJavaRDD</code> is a function in Apache Spark&#x27;s Dataset API that converts a <code>Dataset[T]</code> object into a JavaRDD<!-- -->[T]<!-- --> object. The JavaRDD<!-- -->[T]<!-- --> object can then be used in Java code to perform operations on the data.</p><p>Here&#x27;s an example usage of <code>toJavaRDD</code>:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.api.java.JavaRDD</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{Dataset, SparkSession}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;toJavaRDDExample&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// create a Dataset of Person objects</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val peopleData: Seq[Person] = Seq(Person(&quot;Alice&quot;, 25), Person(&quot;Bob&quot;, 30), Person(&quot;Charlie&quot;, 35))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val peopleDS: Dataset[Person] = spark.createDataset(peopleData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// convert the Dataset to a JavaRDD</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val peopleRDD: JavaRDD[Person] = peopleDS.toJavaRDD()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// print out the elements in the JavaRDD</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">peopleRDD.foreach(println)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we create a <code>Dataset</code> of <code>Person</code> objects using the <code>createDataset</code> function of a <code>SparkSession</code>. We then call <code>toJavaRDD</code> on this <code>Dataset</code> to convert it into a <code>JavaRDD</code>. Finally, we print out the elements in the <code>JavaRDD</code> using the <code>foreach</code> function.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="function-name-unpersist">Function name: unpersist()<a class="hash-link" href="#function-name-unpersist" title="Direct link to heading">â€‹</a></h4><p>Description: The <code>unpersist()</code> function in Spark 3 Dataset API is used to remove the persisted memory of the RDD or Dataset from the Spark&#x27;s cache. This method is useful when the data in the cache is no longer needed and we want to release the memory occupied by the cache for other use.</p><p>Usage example:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;unpersistExample&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[2]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = spark.read.csv(&quot;example.csv&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Cache the DataFrame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.cache()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Unpersist the DataFrame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.unpersist()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Check if the DataFrame is still in the cache</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(df.is_cached)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">false</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first read a CSV file and then cache the resulting DataFrame. We then unpersist the DataFrame and check whether it is still in the cache by using the <code>is_cached</code> method. The output shows that the DataFrame is no longer cached. </p><hr><p>Function name: unpersist(blocking: Boolean)</p><p>Description: The <code>unpersist(blocking: Boolean)</code> function in Spark 3 Dataset API is used to remove the persisted memory of the RDD or Dataset from the Spark&#x27;s cache. The blocking parameter indicates whether the method should block until the cache is removed.</p><p>Usage example:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;unpersistExample&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[2]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = spark.read.csv(&quot;example.csv&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Cache the DataFrame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.cache()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Unpersist the DataFrame blocking until it is removed</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.unpersist(blocking = true)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Check if the DataFrame is still in the cache</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(df.is_cached)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">false</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first read a CSV file and then cache the resulting DataFrame. We then unpersist the DataFrame with blocking set to true and check whether it is still in the cache by using the <code>is_cached</code> method. The output shows that the DataFrame is no longer cached.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="write-dataframewritert">write: DataFrameWriter<!-- -->[T]<a class="hash-link" href="#write-dataframewritert" title="Direct link to heading">â€‹</a></h4><p>The <code>write</code> method is used to write the contents of a Spark <code>Dataset</code> to an external storage system in various formats such as CSV, JSON, Parquet, ORC, and more. It returns a <code>DataFrameWriter</code> object, which provides methods for configuring the write operation, such as setting the output path, format, partitioning, compression, and more.</p><p>Here&#x27;s an example of using the <code>write</code> method to write a <code>Dataset</code> of <code>Person</code> objects to a Parquet file:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{Dataset, SparkSession}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Write Dataset to Parquet&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val people = Seq(Person(&quot;Alice&quot;, 25), Person(&quot;Bob&quot;, 30), Person(&quot;Charlie&quot;, 35))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val peopleDS: Dataset[Person] = spark.createDataset(people)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">peopleDS.write</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .format(&quot;parquet&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .mode(&quot;overwrite&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .save(&quot;/path/to/output/parquet&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, we first define a case class <code>Person</code> and create a <code>Dataset</code> of <code>Person</code> objects. Then we call the <code>write</code> method on the <code>peopleDS</code> <code>Dataset</code>, which returns a <code>DataFrameWriter</code>. We configure the writer to use the Parquet format, overwrite any existing output, and save the output to the specified path. Finally, we call the <code>save</code> method to execute the write operation.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="writestream-datastreamwritert">writeStream: DataStreamWriter<!-- -->[T]<a class="hash-link" href="#writestream-datastreamwritert" title="Direct link to heading">â€‹</a></h4><p>The <code>writeStream</code> function is used in Spark to write the contents of a streaming <code>Dataset</code> to an output sink. It returns a <code>DataStreamWriter</code> object, which can be used to configure the output sink and write the data.</p><p>Here&#x27;s a brief description of the function parameters and their usage:</p><ul><li><code>format(source: String)</code>: Specifies the format of the output sink. The <code>source</code> parameter should be a valid identifier for the sink format, such as <code>&quot;parquet&quot;</code>, <code>&quot;csv&quot;</code>, <code>&quot;kafka&quot;</code>, or <code>&quot;memory&quot;</code>.</li><li><code>outputMode(outputMode: String)</code>: Specifies the output mode of the sink. The <code>outputMode</code> parameter should be a valid identifier for the output mode, such as <code>&quot;append&quot;</code>, <code>&quot;complete&quot;</code>, or <code>&quot;update&quot;</code>.</li><li><code>option(key: String, value: String)</code>: Allows additional configuration options to be passed to the sink.</li><li><code>partitioning(columns: Column*)</code>: Specifies the partitioning scheme for the output data.</li><li><code>queryName(queryName: String)</code>: Specifies a name for the streaming query, which can be useful for monitoring and debugging.</li><li><code>trigger(trigger: Trigger)</code>: Specifies the trigger for the streaming query, which determines how often the sink is updated.</li></ul><p>Usage example:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.streaming.Trigger</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// create a streaming DataFrame from a Kafka topic</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = spark</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .readStream</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .format(&quot;kafka&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;subscribe&quot;, &quot;my_topic&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .load()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// write the streaming DataFrame to a console sink</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val query = df</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .writeStream</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .format(&quot;console&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .outputMode(&quot;append&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;truncate&quot;, false)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .start()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// trigger the sink to update every 10 seconds</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">query.trigger(Trigger.ProcessingTime(&quot;10 seconds&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// wait for the query to finish</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">query.awaitTermination()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>This code reads a streaming DataFrame from a Kafka topic and writes it to the console sink in append mode. The <code>trigger</code> function is used to update the sink every 10 seconds. Finally, the <code>awaitTermination</code> function is called to start the query and wait for it to finish.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="writetotable-string-dataframewriterv2t">writeTo(table: String): DataFrameWriterV2<!-- -->[T]<a class="hash-link" href="#writetotable-string-dataframewriterv2t" title="Direct link to heading">â€‹</a></h4><p>The <code>writeTo</code> function in Spark 3 Dataset API allows you to write the content of a Dataset to a specific table in a catalog. </p><p>Here is a brief description of the function:</p><ul><li><code>writeTo(table: String): DataFrameWriterV2[T]</code>: Writes the content of the dataset to a specific table in a catalog.</li></ul><p>The <code>writeTo</code> function returns a <code>DataFrameWriterV2[T]</code> object that can be used to configure various output options for the data being written.</p><p>Usage example:</p><div class="codeBlockContainer_I0IT language-scala theme-code-block"><div class="codeBlockContent_wNvx scala"><pre tabindex="0" class="prism-code language-scala codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.SaveMode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = Seq((&quot;Alice&quot;, 25), (&quot;Bob&quot;, 30), (&quot;Charlie&quot;, 35)).toDF(&quot;name&quot;, &quot;age&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Write the content of the dataframe to a specific table in a catalog</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.writeTo(&quot;my_database.my_table&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .mode(SaveMode.Append)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .save()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>In this example, the content of the DataFrame <code>df</code> is written to the table <code>my_database.my_table</code> using the <code>writeTo</code> function. The <code>mode</code> function is used to specify the save mode as <code>Append</code>. Finally, the <code>save</code> function is called to write the data to the specified table.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/dzharii/dzharii.github.io/docs/gpt4-generated-content/2023-05-03-apache-spark-ref.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/fun/2023-02-16-Seattle-Link-Pictograms"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">2023-02-16-Seattle-Link-Pictograms</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/resources/2023-02-12-Slides--Apache-Spark-Core-Deep-Dive-Proper-Optimization"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">2023-02-12 Apache Spark Coreâ€”Deep Diveâ€”Proper Optimization Daniel Tomes Databricks</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#inspired-by" class="table-of-contents__link toc-highlight">Inspired by:</a></li><li><a href="#prompt" class="table-of-contents__link toc-highlight">Prompt:</a></li><li><a href="#actions" class="table-of-contents__link toc-highlight">Actions</a></li><li><a href="#basic-dataset-functions" class="table-of-contents__link toc-highlight">Basic Dataset functions</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Wiki</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Please, remind me to put some good description here!
        Write an awesome description for your new site here. You can edit this
        line in _config.yml. It will appear in your document head meta (for
        Google search results) and in your feed.xml site description.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.fe7f993d.js"></script>
<script src="/assets/js/main.1d2740ab.js"></script>
</body>
</html>