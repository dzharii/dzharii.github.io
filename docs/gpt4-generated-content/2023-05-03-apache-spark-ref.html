<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Dmytro Zharii RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Dmytro Zharii Atom Feed">
<link rel="stylesheet" href="/clippy.js/clippy.css">
<script src="/clippy.js/jquery-3.6.0.min.js"></script>
<script src="/clippy.js/clippy.js"></script><title data-react-helmet="true">Apache Spark Reference | Dmytro Zharii</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://blog.zharii.com/docs/gpt4-generated-content/2023-05-03-apache-spark-ref"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Apache Spark Reference | Dmytro Zharii"><meta data-react-helmet="true" name="description" content="2023-05-03"><meta data-react-helmet="true" property="og:description" content="2023-05-03"><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://blog.zharii.com/docs/gpt4-generated-content/2023-05-03-apache-spark-ref"><link data-react-helmet="true" rel="alternate" href="https://blog.zharii.com/docs/gpt4-generated-content/2023-05-03-apache-spark-ref" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://blog.zharii.com/docs/gpt4-generated-content/2023-05-03-apache-spark-ref" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.73058418.css">
<link rel="preload" href="/assets/js/runtime~main.e3d900ad.js" as="script">
<link rel="preload" href="/assets/js/main.77d6e0e3.js" as="script">
</head>
<body data-theme="light">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_t6R9">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon.ico" alt="My Site Logo" class="themedImage_ixIS themedImage--light_oE8D"><img src="/img/favicon.ico" alt="My Site Logo" class="themedImage_ixIS themedImage--dark_Pcj1"></div><b class="navbar__title">Home</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Personal Wiki</a><a class="navbar__item navbar__link" href="/blog">Blog</a><a href="https://blog.zharii.com/blog/rss.xml" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">[RSS]</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/dzharii/dzharii.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_ENIg"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_BOl1 toggle_D9iD toggleDisabled_z3Bs"><div class="toggleTrack_RGsh" role="button" tabindex="-1"><div class="toggleTrackCheck_G703"><span class="toggleIcon_YjN9">🌜</span></div><div class="toggleTrackX_bXpe"><span class="toggleIcon_YjN9">🌞</span></div><div class="toggleTrackThumb_Hv7O"></div></div><input type="checkbox" class="toggleScreenReader_8nUd" aria-label="Switch between dark and light mode"></div><div class="dsla-search-wrapper"><div class="dsla-search-field" data-tags="default,docs-default-current"></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_EQ68"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_gkLb" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_unvu"><div class="sidebar_HVo4"><nav class="menu thin-scrollbar menu_Qsok"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">intro</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/azure-and-aad-articles">azure-and-aad-articles</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/bicycling-inbox">bicycling-inbox</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/big-tech-folks">big-tech-folks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/bookmark-my-GPT-prompts">🤖 Bookmarks: My GPT/AI Prompts</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/bookmarklets">📜 BookmarkLETS</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/bookmarks">📌 Bookmarks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/cheatsheets">Cheat-sheets</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/design-assets">design-assets</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-bicep">dev-bicep</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-c99-github-banned-h">dev-c99-github-banned-h</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-c99-libraries">dev-c99-libraries</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-c99-video-content-creators">dev-c99-video-content-creators</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-c99">dev-c99</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-cpp-snippets">dev-cpp-snippets</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-cpp">dev-cpp</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-csharp">dev-csharp</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-design-and-process">dev-design-and-process</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-docfx">dev-docfx</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-emacs">dev-emacs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-ide-visualstudio">dev-ide-visualstudio</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-javascript-snippets-dom">dev-javascript-snippets-dom</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-linux">dev-linux</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-lua">dev-lua</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-oculus-quest2-inbox">dev-oculus-quest2-inbox</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-powershell">dev-powershell</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-projects-backend">dev-projects-backend</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-recreational-programming-in-c">dev-recreational-programming-in-c</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-retro-8bit">dev-retro-8bit</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-scala-spark-synapse-bigdata">dev-scala-spark-synapse-bigdata</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-stable-diffusion-cpu">dev-stable-diffusion-cpu</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-testing-endpoints">dev-testing-endpoints</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-webapi">dev-webapi</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/dev-writing">✏️ dev-writing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/ffmpeg-inbox">ffmpeg-inbox</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/ffmpeg-recipes">ffmpeg-recipes</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/ffmpeg-snippets">ffmpeg-snippets</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/fun-meme-cry-like-a-man">fun-meme-cry-like-a-man</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/fun-rubik-cube">Rubik&#x27;s Cube</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/fun-web-comics">fun-web-comics</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/fun-yodel">fun-yodel</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/image-processing">Image Processing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/knowledge-management">Knowledge Management</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/linux-odroid-cli-90s-project">linux-odroid-cli-90s-project</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/markdown-tools">markdown-tools</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/math">math and stats</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/network">network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/podcasts">Podcasts</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/prep-resources">prep-resources</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/psy-2022-07-28-BoundariesMeeting">psy-2022-07-28-BoundariesMeeting</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/psy-ADHD">Mental Health / ADHD</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/psy-burnout">Psy-Burnout (mental wellbeing)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/psy-gaslighting">psy-gaslighting</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/psy-Nonviolent Communication">psy-Nonviolent Communication</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/psy-Unschedule">psy-Unschedule</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/public-apis">public-apis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/radiation">☢ Radiation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/retro-win3.11">retro-win3.11</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/retro-www-buttons">retro-www-buttons</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/seattle">seattle</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/security-articles">security-articles</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/self-hosted-software">self-hosted-software</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/software2025">Software 2025</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/testing-distributed-systems">testing-distributed-systems</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/testing">testing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/tools-mindmaps">tools-mindmaps</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/tools-WireGuard">tools-WireGuard</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/workplace-articles">workplace-articles</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/workplace-interview-prep">workplace-interview-prep</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/workplace-levels-and-layoffs">workplace-levels-and-layoffs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/workplace-tech-companies">Seattle Tech Companies and Their Career Pages</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_aX2u" href="/docs/a-watch-reading-list/my-watch-reading-list-now">a-watch-reading-list</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_aX2u" href="/docs/aww-articles/2021-10-23-Moving-faster">aww-articles</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_aX2u" href="/docs/chatGPT/2022-12-14-5-Harmless-Ways-Software-Developers-Can-Have-Fun-at-Work.md">chatGPT</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_aX2u" href="/docs/dev-good-GPT-prompts/2023-06-19-CPP-17-describe-new-language-features">dev-good-GPT-prompts</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_aX2u" href="/docs/fun/2023-02-16-Seattle-Link-Pictograms">fun</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_aX2u" aria-current="page" href="/docs/gpt4-generated-content/2023-05-03-apache-spark-ref">gpt4-generated-content</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/gpt4-generated-content/2023-05-03-apache-spark-ref">Apache Spark Reference</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_aX2u" href="/docs/psy-saved-articles/a-programmers-guide-to-stress">psy-saved-articles</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_aX2u" href="/docs/resources/2023-02-12-Slides--Apache-Spark-Core-Deep-Dive-Proper-Optimization">resources</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_aX2u" href="/docs/walking-seatle-places/0000-00-00-GPT-Prompts">walking-seatle-places</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_aX2u" href="/docs/llm-experiments/2025-06-27/2025-06-27-success-software-2025-list-from-all-link-dump-post">llm-experiments</a></div></li></ul></nav></div></aside><main class="docMainContainer_U6YS"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_GrMM"><div class="docItemContainer_IRuv"><article><div class="tocCollapsible_GIoR theme-doc-toc-mobile tocMobile_jv3u"><button type="button" class="clean-btn tocCollapsibleButton_PfAB">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Apache Spark Reference</h1><p>2023-05-03</p><p>[<!-- -->[<em>TOC</em>]<!-- -->]</p><h2 class="anchor anchorWithStickyNavbar_DXrL" id="inspired-by">Inspired by:<a class="hash-link" href="#inspired-by" title="Direct link to heading">​</a></h2><ul><li><a href="https://github.com/f/awesome-chatgpt-prompts#act-as-a-tech-writer" target="_blank" rel="noopener noreferrer">Act as a Tech Writer</a></li><li><a href="https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener noreferrer">https://api-docs.databricks.com/scala/spark/latest/org/apache/spark/sql/Dataset.html</a></li></ul><h2 class="anchor anchorWithStickyNavbar_DXrL" id="prompt">Prompt:<a class="hash-link" href="#prompt" title="Direct link to heading">​</a></h2><p>Act as a tech writer and Apache Spark expert. You will act as a creative and engaging technical writer and create guides on how to do use a specific function in Scala Spark.
I will provide you with the function name, and you will do the best to provide a short description, usage example, and output. The usage example should be good to explain what the function do and clear to the reader (simple to understand). Also the description should be concise. Clear, simple and concise.
Scala codding requirements:</p><ol><li>Prefer writhing variable datatypes;</li><li>Use more simple Scala syntax for the scala beginners. Keep it clear for someone who knows Java, C#, JavaScript, but try to keep good scala programming style.;</li><li>Instead of expressions in the string literals, prefer using language constructs;</li><li>Try to avoid using tuples, like &quot;._1&quot;, &quot;._2&quot;. Don&#x27;t use &quot;employee =&gt; employee._2 &gt;= 30&quot;; rather introduce a case class;</li><li>Nicely format code with indentation; Format Array initialization, object initialization and the output to illustrate the hierarchical structure of the data.</li><li>Start each reply with the level-4 markdown header. Inside the header, include function name, parameters and return type inside markdown back-ticks: <code>except(other: Dataset[T]): Dataset[T]</code></li><li>Maintain line length 120 characters. This is Okay to have longer lines, but rather prefer splitting statement in multiple lines with nice indentation.
Please explain: Spark 3 Dataset:</li></ol><ul><li>except(other: Dataset<!-- -->[T]<!-- -->): Dataset<!-- -->[T]</li></ul><h2 class="anchor anchorWithStickyNavbar_DXrL" id="actions">Actions<a class="hash-link" href="#actions" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_DXrL" id="collect">collect()<a class="hash-link" href="#collect" title="Direct link to heading">​</a></h4><p>Description:</p><p>The collect() function in Spark 3 Dataset is used to retrieve all the elements of a dataset and return them as an array or a list in the driver program. It returns an array of objects that can be used to perform operations on the data or iterate over the elements.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to retrieve the information of all employees whose age is greater than or equal to 30. We can use the collect() function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val filteredEmployees = employeeDataset.filter(_.age &gt;= 30).collect()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. We apply the filter() function to select only the employees whose age is greater than or equal to 30. Finally, we use the collect() function to retrieve all the filtered employees.</p><p>Output:</p><p>The output of the collect() function is an array of objects. In our example, the filteredEmployees variable will contain an array of Employee instances representing the information of the filtered employees:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Array(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that using the collect() function to retrieve all the elements of a large dataset can be computationally expensive and memory-intensive, and may lead to performance issues or even out-of-memory errors. Therefore, it is recommended to use this function with caution and only for small datasets or for testing purposes.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="collectaslist">collectAsList()<a class="hash-link" href="#collectaslist" title="Direct link to heading">​</a></h4><p>Description:</p><p>The collectAsList() function in Spark 3 Dataset is similar to the collect() function but it returns the result as a Java List instead of an array. It collects all the elements of a dataset to the driver program and returns them as a Java List that can be used for further processing.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to retrieve the information of all employees whose age is greater than or equal to 30. We can use the collectAsList() function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val filteredEmployees = employeeDataset.filter(_.age &gt;= 30).collectAsList()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. We apply the filter() function to select only the employees whose age is greater than or equal to 30. Finally, we use the collectAsList() function to retrieve all the filtered employees as a Java List.</p><p>Output:</p><p>The output of the collectAsList() function is a Java List of objects. In our example, the filteredEmployees variable will contain a Java List of Employee instances representing the information of the filtered employees:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">List(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that the collectAsList() function has the same performance and memory implications as the collect() function, and should be used with caution for large datasets.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="count">count()<a class="hash-link" href="#count" title="Direct link to heading">​</a></h4><p>Description:</p><p>The count() function in Spark 3 Dataset is used to count the number of elements in a dataset. It returns the total number of rows or records present in the dataset.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to count the number of employees in the dataset. We can use the count() function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeCount = employeeDataset.count()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the count() function to count the total number of employees in the dataset.</p><p>Output:</p><p>The output of the count() function is a Long value representing the total number of elements in the dataset. In our example, the employeeCount variable will contain the total number of employees in the dataset, which is 4:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">4</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that the count() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="describe">describe()<a class="hash-link" href="#describe" title="Direct link to heading">​</a></h4><p>Description:</p><p>The describe() function in Spark 3 Dataset is used to summarize the statistics of a dataset. It computes the summary statistics, including count, mean, standard deviation, minimum, and maximum values, for each numeric column in the dataset.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to compute the summary statistics for the age and salary columns in the dataset. We can use the describe() function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val summaryStats = employeeDataset.describe(&quot;age&quot;, &quot;salary&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the describe() function to compute the summary statistics for the age and salary columns in the dataset.</p><p>Output:</p><p>The output of the describe() function is a new dataset containing the summary statistics for each column specified in the function. In our example, the summaryStats variable will contain a new dataset with the summary statistics for the age and salary columns:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+-------+-----------------+-----------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|summary|              age|           salary|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------+-----------------+-----------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  count|                4|                4|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|   mean|            28.00|           3075.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| stddev|2.581988897471611|222.1084339724426|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    min|               25|           2800.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    max|               32|           3500.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------+-----------------+-----------------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that the describe() function is a transformation operation that does not trigger the computation of the dataset, and can be used for large datasets without performance issues.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="first">first()<a class="hash-link" href="#first" title="Direct link to heading">​</a></h4><p>Description:</p><p>The first() function in Spark 3 Dataset is used to retrieve the first element of the dataset. It returns the first element of the dataset as a new instance of the case class or a tuple, depending on the structure of the dataset.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to retrieve the details of the first employee in the dataset. We can use the first() function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val firstEmployee = employeeDataset.first()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the first() function to retrieve the details of the first employee in the dataset.</p><p>Output:</p><p>The output of the first() function is the first element of the dataset as a new instance of the case class or a tuple, depending on the structure of the dataset. In our example, the firstEmployee variable will contain a new instance of the Employee case class representing the details of the first employee in the dataset:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Employee(&quot;John&quot;, 28, 3000.0)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that the first() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="foreach">foreach()<a class="hash-link" href="#foreach" title="Direct link to heading">​</a></h4><p>Description:</p><p>The foreach() function in Spark 3 Dataset is used to apply a given function to each element of the dataset. It iterates over the elements of the dataset and applies a user-defined function to each element.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to print the details of each employee in the dataset. We can use the foreach() function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDataset.foreach(employee =&gt; println(s&quot;Name: ${employee.name}, Age: ${employee.age}, Salary: ${employee.salary}&quot;))</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the foreach() function to print the details of each employee in the dataset.</p><p>Output:</p><p>The output of the foreach() function is the result of applying the given function to each element of the dataset. In our example, the output will be the details of each employee in the dataset printed to the console:</p><div class="codeBlockContainer_aLwb language-yaml theme-code-block"><div class="codeBlockContent_INfL yaml"><pre tabindex="0" class="prism-code language-yaml codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> John</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Age</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">28</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Salary</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">3000.0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Mary</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Age</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">25</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Salary</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2800.0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Peter</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Age</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">32</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Salary</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">3500.0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Jane</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Age</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">27</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Salary</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">3200.0</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that the foreach() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets. Also, the order of the elements in the output is not guaranteed, as Spark processes elements in parallel.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="foreachpartition">foreachPartition()<a class="hash-link" href="#foreachpartition" title="Direct link to heading">​</a></h4><p>Description:</p><p>The foreachPartition() function in Spark 3 Dataset is used to apply a given function to each partition of the dataset. It is similar to the foreach() function, but instead of applying the function to each element, it applies the function to each partition of the dataset. It can be useful when we need to perform some expensive initialization or setup once per partition, rather than once per element.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to compute the total salary of each partition of the dataset. We can use the foreachPartition() function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDataset.foreachPartition(iterator =&gt; {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  var partitionTotalSalary = 0.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  iterator.foreach(employee =&gt; partitionTotalSalary += employee.salary)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  println(s&quot;Partition Total Salary: $partitionTotalSalary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">})</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the foreachPartition() function to compute the total salary of each partition of the dataset.</p><p>Output:</p><p>The output of the foreachPartition() function is the result of applying the given function to each partition of the dataset. In our example, the output will be the total salary of each partition of the dataset printed to the console:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Partition Total Salary: 5800.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Partition Total Salary: 6700.0</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that the foreachPartition() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets. Also, the order of the partitions in the output is not guaranteed, as Spark processes partitions in parallel.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="head">head()<a class="hash-link" href="#head" title="Direct link to heading">​</a></h4><p>Description:</p><p>The head() function in Spark 3 Dataset is used to return the first n rows of a dataset as a new Dataset. It is similar to the take() function, but instead of returning an Array, it returns a new Dataset.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to get the first 2 rows of the dataset. We can use the head() function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val firstTwoEmployees = employeeDataset.head(2)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the head() function to get the first 2 rows of the dataset as a new Dataset.</p><p>Output:</p><p>The output of the head() function is a new Dataset containing the first n rows of the original dataset. In our example, the output will be a new Dataset containing the details of the first 2 employees:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|name|age|salary|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|John| 28|3000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Mary| 25|2800.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that the head() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets. Also, the order of the rows in the output is not guaranteed, as Spark processes data in parallel.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="reduce">reduce()<a class="hash-link" href="#reduce" title="Direct link to heading">​</a></h4><p>Description:</p><p>The reduce() function in Spark 3 Dataset is used to aggregate the elements of a dataset using a specified binary operator. The binary operator must be associative and commutative, and it is applied to pairs of elements until a single result is obtained.</p><p>Usage Example:</p><p>Suppose we have a dataset containing a sequence of integers, and we want to compute the sum of all these integers using the reduce() function. We can use the reduce() function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">val numbers = Seq(1, 2, 3, 4, 5)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val numbersDataset = numbers.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val sum = numbersDataset.reduce((x, y) =&gt; x + y)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of integers. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the reduce() function to compute the sum of all the integers in the dataset.</p><p>Output:</p><p>The output of the reduce() function is a single value obtained by applying the binary operator to pairs of elements until a single result is obtained. In our example, the output will be the sum of all the integers in the dataset:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">15</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that the reduce() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets. Also, the order of the elements in the dataset is not guaranteed, as Spark processes data in parallel.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="show">show()<a class="hash-link" href="#show" title="Direct link to heading">​</a></h4><p>Description:</p><p>The show() function in Spark 3 Dataset is used to display the contents of a dataset in a tabular format. It is useful for debugging and exploring the contents of a dataset.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to display the contents of the dataset in a tabular format. We can use the show() function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDataset.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the show() function to display the contents of the dataset in a tabular format.</p><p>Output:</p><p>The output of the show() function is a tabular representation of the dataset, with each row corresponding to a record in the dataset. In our example, the output will be:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| name|age|salary|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| John| 28|3000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Mary| 25|2800.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Peter| 32|3500.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Jane| 27|3200.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that the show() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets. It is intended for interactive use, such as exploring data in a notebook or shell, rather than for production use.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="summary">summary()<a class="hash-link" href="#summary" title="Direct link to heading">​</a></h4><p>Description:</p><p>The summary() function in Spark 3 Dataset is used to compute summary statistics for the columns of a dataset. It provides a quick way to obtain basic statistics for a large number of columns in a dataset.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to compute summary statistics for the age and salary columns. We can use the summary() function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDataset.select(&quot;age&quot;, &quot;salary&quot;).summary().show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the select() function to select the age and salary columns, and then apply the summary() function to compute summary statistics for these columns.</p><p>Output:</p><p>The output of the summary() function is a tabular representation of the summary statistics for the selected columns, including the count, mean, standard deviation, minimum, and maximum values. In our example, the output will be:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+-------+-----------------+------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|summary|              age|            salary|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------+-----------------+------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  count|                4|                 4|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|   mean|             28.0|            3125.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| stddev|2.160246899469287| 298.8118022040274|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    min|               25|            2800.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    25%|               27|            3000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    50%|               28|            3200.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    75%|               32|            3325.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    max|               32|            3500.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------+-----------------+------------------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that the summary() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets. It is intended for quick analysis of data, rather than for production use.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="tail">tail()<a class="hash-link" href="#tail" title="Direct link to heading">​</a></h4><p>Description:</p><p>The tail() function in Spark 3 Dataset returns the last n rows of the dataset. It is useful when we want to inspect the end of a dataset or when we need to extract only the last few rows of a dataset for further processing.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to extract the last 2 rows of this dataset. We can use the tail() function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDataset.tail(2).foreach(println)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the tail() function to extract the last 2 rows of the dataset, and then apply the foreach() function to print these rows.</p><p>Output:</p><p>The output of the tail() function is a Dataset containing the last n rows of the original dataset. In our example, the output will be:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Employee(Peter,32,3500.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Employee(Jane,27,3200.0)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that the tail() function is an action operation that triggers the computation of the dataset, and should be used with caution for large datasets. It is intended for quick inspection of data, rather than for production use.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="take">take()<a class="hash-link" href="#take" title="Direct link to heading">​</a></h4><p>Description:</p><p>The take() function in Spark 3 Dataset returns an array of the first n rows of the dataset. It is useful when we need to extract only a few rows of a dataset for further processing, and we don&#x27;t want to load the entire dataset into memory.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to extract the first 3 rows of this dataset. We can use the take() function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val firstThreeRows = employeeDataset.take(3)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">firstThreeRows.foreach(println)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the take() function to extract the first 3 rows of the dataset, and then apply the foreach() function to print these rows.</p><p>Output:</p><p>The output of the take() function is an array containing the first n rows of the original dataset. In our example, the output will be:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Employee(John,28,3000.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Employee(Mary,25,2800.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Employee(Peter,32,3500.0)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that the take() function is a transformation operation that does not trigger the computation of the entire dataset, and is useful for extracting only a few rows of data for further processing.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="takeaslist">takeAsList()<a class="hash-link" href="#takeaslist" title="Direct link to heading">​</a></h4><p>Description:</p><p>The takeAsList() function in Spark 3 Dataset is similar to the take() function, but it returns a Java list instead of a Scala array. It is useful when we want to use the extracted rows of a dataset in Java-based APIs that accept Java collections.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to extract the first 3 rows of this dataset as a Java list. We can use the takeAsList() function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val firstThreeRows = employeeDataset.takeAsList(3)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">firstThreeRows.forEach(println)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the takeAsList() function to extract the first 3 rows of the dataset, and then apply the forEach() function to print these rows.</p><p>Output:</p><p>The output of the takeAsList() function is a Java list containing the first n rows of the original dataset. In our example, the output will be:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Employee(John,28,3000.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Employee(Mary,25,2800.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Employee(Peter,32,3500.0)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that the takeAsList() function is a transformation operation that does not trigger the computation of the entire dataset, and is useful for extracting only a few rows of data for further processing in Java-based APIs.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="tolocaliterator">toLocalIterator()<a class="hash-link" href="#tolocaliterator" title="Direct link to heading">​</a></h4><p>Description:</p><p>The toLocalIterator() function in Spark 3 Dataset allows iterating over the elements of a dataset on the driver node, one partition at a time. This function can be useful when we want to perform some custom processing on the dataset elements using code that is only available on the driver node.</p><p>Usage Example:</p><p>Suppose we have a dataset containing the details of employees, including their names, ages, and salaries. We want to print the name and age of each employee by iterating over the dataset on the driver node using the toLocalIterator() function. We can do this as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDetails: Seq[Employee] = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset = employeeDetails.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val iterator = employeeDataset.toLocalIterator()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">while (iterator.hasNext) {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  val employee = iterator.next()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  println(s&quot;Name: ${employee.name}, Age: ${employee.age}&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a sequence of case class instances containing the details of employees. Then, we convert this sequence to a Dataset using the toDS() function. Finally, we use the toLocalIterator() function to obtain an iterator over the elements of the dataset on the driver node, and then iterate over this iterator using a while loop to print the name and age of each employee.</p><p>Output:</p><p>The output of the toLocalIterator() function is the result of the custom processing performed on the dataset elements on the driver node. In our example, the output will be:</p><div class="codeBlockContainer_aLwb language-yaml theme-code-block"><div class="codeBlockContent_INfL yaml"><pre tabindex="0" class="prism-code language-yaml codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> John</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Age</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">28</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Mary</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Age</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">25</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Peter</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Age</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">32</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Jane</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token key atrule" style="color:#00a4db">Age</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">27</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that the toLocalIterator() function can be used to perform custom processing on the dataset elements using any code that is available on the driver node.</p><h2 class="anchor anchorWithStickyNavbar_DXrL" id="basic-dataset-functions">Basic Dataset functions<a class="hash-link" href="#basic-dataset-functions" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_DXrL" id="asschema-structtype-dataframe">as(schema: StructType): DataFrame<a class="hash-link" href="#asschema-structtype-dataframe" title="Direct link to heading">​</a></h4><p>Description:</p><p>The <code>as</code> function in Spark 3 Dataset API is used to convert a Dataset of generic objects to a strongly typed Dataset by specifying a schema. The function takes a StructType object as an argument that represents the schema of the resulting strongly typed Dataset. The StructType object can be created using the <code>StructType</code> class and by adding <code>StructField</code> objects that represent the columns of the resulting strongly typed Dataset.</p><p>Usage Example:</p><p>Suppose we have a CSV file containing the details of employees, including their names, ages, and salaries. We want to read this CSV file into a DataFrame and then convert it into a strongly typed Dataset by specifying a schema. We can do this as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, DoubleType}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeSchema = new StructType()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .add(StructField(&quot;name&quot;, StringType, true))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .add(StructField(&quot;age&quot;, IntegerType, true))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .add(StructField(&quot;salary&quot;, DoubleType, true))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDF = spark.read.format(&quot;csv&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;header&quot;, &quot;true&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;inferSchema&quot;, &quot;true&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .load(&quot;employee.csv&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDS = employeeDF.as(employeeSchema)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDS.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a <code>StructType</code> object called <code>employeeSchema</code> that represents the schema of the resulting strongly typed Dataset. This schema consists of three columns: name (StringType), age (IntegerType), and salary (DoubleType).</p><p>Then, we read the CSV file into a DataFrame using the <code>read</code> method of the <code>SparkSession</code> object. We set the <code>header</code> option to <code>true</code> to indicate that the first row of the CSV file contains the column names, and set the <code>inferSchema</code> option to <code>true</code> to infer the data types of the columns from the contents of the CSV file.</p><p>Next, we use the <code>as</code> function to convert the DataFrame into a strongly typed Dataset by specifying the <code>employeeSchema</code>. Finally, we call the <code>show</code> method on the resulting Dataset to display its contents.</p><p>Output:</p><p>The output of the <code>show</code> method will be a table that displays the contents of the strongly typed Dataset, which will have the specified schema. For example:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| name|age|salary|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|John | 28|3000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Mary | 25|2800.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Peter| 32|3500.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Jane | 27|3200.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that if the data in the DataFrame does not match the specified schema, the <code>as</code> function will throw an exception. It is important to ensure that the schema is correctly specified to avoid errors at runtime.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="cache">cache()<a class="hash-link" href="#cache" title="Direct link to heading">​</a></h4><p>Description:</p><p>The <code>cache</code> function in Spark 3 Dataset API is used to persist the contents of a Dataset in memory, so that subsequent operations on the Dataset can be performed more quickly. This function stores the Dataset in the cache memory of the executor nodes, which makes it faster to access and reduces the number of times the Dataset needs to be computed.</p><p>Usage Example:</p><p>Suppose we have a Dataset of employee data, and we want to perform multiple operations on this Dataset. In order to avoid recomputing the Dataset every time an operation is performed, we can cache the Dataset in memory using the <code>cache</code> function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDS = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;).as[Employee]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDS.cache()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val filteredDS = employeeDS.filter(employee =&gt; employee.age &gt;= 30)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val count = filteredDS.count()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(s&quot;Number of employees above 30: $count&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a Dataset of employee data using the <code>toDF</code> method and then converting it into a strongly typed Dataset using the <code>as</code> method. Then we call the <code>cache</code> method on this Dataset to cache it in memory.</p><p>Next, we perform an operation on the cached Dataset by filtering out the employees whose age is less than 30 using the <code>filter</code> function. Finally, we call the <code>count</code> function to count the number of employees whose age is greater than or equal to 30.</p><p>Output:</p><p>The output of this program will be the number of employees whose age is greater than or equal to 30. Since the Dataset has been cached in memory, the <code>filter</code> and <code>count</code> operations will be performed more quickly than if the Dataset had not been cached.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="checkpointeager-boolean">checkpoint(eager: Boolean)<a class="hash-link" href="#checkpointeager-boolean" title="Direct link to heading">​</a></h4><p>Description:</p><p>The <code>checkpoint</code> function in Spark 3 Dataset API is used to save the contents of a Dataset to a checkpoint file in HDFS. This function can be used to avoid recomputing a Dataset from scratch if a failure occurs during its computation. </p><p>Usage Example:</p><p>Suppose we have a Dataset of employee data, and we want to checkpoint this Dataset to avoid recomputing it from scratch if a failure occurs. We can use the <code>checkpoint</code> function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDS = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;).as[Employee]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDS.checkpoint()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val filteredDS = employeeDS.filter(employee =&gt; employee.age &gt;= 30)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val count = filteredDS.count()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(s&quot;Number of employees above 30: $count&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a Dataset of employee data using the <code>toDF</code> method and then converting it into a strongly typed Dataset using the <code>as</code> method. Then we call the <code>checkpoint</code> method on this Dataset to save its contents to a checkpoint file.</p><p>Next, we perform an operation on the Dataset by filtering out the employees whose age is less than 30 using the <code>filter</code> function. Finally, we call the <code>count</code> function to count the number of employees whose age is greater than or equal to 30.</p><p>Output:</p><p>The output of this program will be the number of employees whose age is greater than or equal to 30. If a failure occurs during the computation of this Dataset, it can be recovered from the checkpoint file rather than being recomputed from scratch.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="columns-arraystring">columns: Array<!-- -->[String]<a class="hash-link" href="#columns-arraystring" title="Direct link to heading">​</a></h4><p>Description:</p><p>The <code>columns</code> function in Spark 3 DataFrame API returns an array of column names present in the DataFrame. It is a simple and quick way to get the list of column names present in the DataFrame.</p><p>Usage Example:</p><p>Suppose we have a DataFrame containing employee data, and we want to get the list of column names present in this DataFrame. We can use the <code>columns</code> function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDF = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val columnNames = employeeDF.columns</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(&quot;Columns in the DataFrame: &quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">columnNames.foreach(println)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a DataFrame of employee data using the <code>toDF</code> method. Then we call the <code>columns</code> method on this DataFrame to get the list of column names present in it.</p><p>Finally, we print the list of column names using a <code>foreach</code> loop.</p><p>Output:</p><p>The output of this program will be a list of column names present in the DataFrame:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Columns in the DataFrame:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">name</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">age</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">salary</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_DXrL" id="createglobaltempviewviewname-string-unit">createGlobalTempView(viewName: String): Unit<a class="hash-link" href="#createglobaltempviewviewname-string-unit" title="Direct link to heading">​</a></h4><p>Description:</p><p>The <code>createGlobalTempView</code> function in Spark 3 DataFrame API creates a global temporary view with the specified name. A global temporary view is a view that is accessible across all SparkSessions and even across different applications. It is tied to a system preserved database <code>global_temp</code>.</p><p>Usage Example:</p><p>Suppose we have a DataFrame containing employee data, and we want to create a global temporary view named &quot;employee_view&quot; using this DataFrame. We can use the <code>createGlobalTempView</code> function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDF = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDF.createGlobalTempView(&quot;employee_view&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a DataFrame of employee data using the <code>toDF</code> method. Then we call the <code>createGlobalTempView</code> method on this DataFrame with the view name &quot;employee_view&quot; to create a global temporary view.</p><p>Output:</p><p>The output of this program will not be visible on the console. The global temporary view is now created and can be accessed from any SparkSession in the same application or across different applications.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="createorreplaceglobaltempviewviewname-string-unit">createOrReplaceGlobalTempView(viewName: String): Unit<a class="hash-link" href="#createorreplaceglobaltempviewviewname-string-unit" title="Direct link to heading">​</a></h4><p>Description:</p><p>The <code>createOrReplaceGlobalTempView</code> function in Spark 3 DataFrame API creates a global temporary view with the specified name, or replaces an existing global temporary view with the same name. A global temporary view is a view that is accessible across all SparkSessions and even across different applications. It is tied to a system preserved database <code>global_temp</code>.</p><p>Usage Example:</p><p>Suppose we have a DataFrame containing employee data, and we want to create a global temporary view named &quot;employee_view&quot; using this DataFrame. We can use the <code>createOrReplaceGlobalTempView</code> function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDF = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDF.createOrReplaceGlobalTempView(&quot;employee_view&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a DataFrame of employee data using the <code>toDF</code> method. Then we call the <code>createOrReplaceGlobalTempView</code> method on this DataFrame with the view name &quot;employee_view&quot; to create or replace a global temporary view.</p><p>Output:</p><p>The output of this program will not be visible on the console. The global temporary view is now created or replaced and can be accessed from any SparkSession in the same application or across different applications.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="createtempviewviewname-string-unit">createTempView(viewName: String): Unit<a class="hash-link" href="#createtempviewviewname-string-unit" title="Direct link to heading">​</a></h4><p>Description:</p><p>The <code>createTempView</code> function in Spark 3 DataFrame API creates a local temporary view with the specified name. A local temporary view is only accessible within the same SparkSession that created it.</p><p>Usage Example:</p><p>Suppose we have a DataFrame containing employee data, and we want to create a local temporary view named &quot;employee_view&quot; using this DataFrame. We can use the <code>createTempView</code> function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDF = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDF.createTempView(&quot;employee_view&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a DataFrame of employee data using the <code>toDF</code> method. Then we call the <code>createTempView</code> method on this DataFrame with the view name &quot;employee_view&quot; to create a local temporary view.</p><p>Output:</p><p>The output of this program will not be visible on the console. The local temporary view is now created and can be accessed within the same SparkSession using the specified view name.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="dtypes-arraystring-string">dtypes: Array<!-- -->[(String, String)]<a class="hash-link" href="#dtypes-arraystring-string" title="Direct link to heading">​</a></h4><p>Description:</p><p>The <code>dtypes</code> function in Spark 3 DataFrame API returns an array of tuples containing the name of each column in the DataFrame and its corresponding data type. The data type is represented as a string.</p><p>Usage Example:</p><p>Suppose we have a DataFrame containing employee data, and we want to get the data types of each column in this DataFrame. We can use the <code>dtypes</code> function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDF = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataTypes = employeeDF.dtypes</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dataTypes.foreach(println)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a DataFrame of employee data using the <code>toDF</code> method. Then we call the <code>dtypes</code> function on this DataFrame to get the data types of each column. Finally, we use a foreach loop to print out the name and data type of each column.</p><p>Output:</p><p>The output of this program will be:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">(name,StringType)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(age,IntegerType)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(salary,DoubleType)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>This shows that the &quot;name&quot; column has a String data type, the &quot;age&quot; column has an Integer data type, and the &quot;salary&quot; column has a Double data type.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="explain-unit">explain(): Unit<a class="hash-link" href="#explain-unit" title="Direct link to heading">​</a></h4><p>Description:</p><p>The <code>explain</code> function in Spark 3 DataFrame API is used to display the physical plan of a DataFrame or Dataset. It provides an easy way to understand how Spark will execute a given query. The <code>explain</code> function returns the execution plan as a string, which can be printed to the console or written to a log file.</p><p>Usage Example:</p><p>Suppose we have a DataFrame containing employee data, and we want to see how Spark will execute a query on this DataFrame. We can use the <code>explain</code> function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDF = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDF.filter($&quot;age&quot; &gt; 25).explain()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a DataFrame of employee data using the <code>toDF</code> method. Then we apply a filter to this DataFrame to select only those employees whose age is greater than 25. Finally, we call the <code>explain</code> function on this filtered DataFrame to display the physical plan of this query.</p><p>Output:</p><p>The output of this program will be:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">== Physical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">*(1) Filter (isnotnull(age#1) &amp;&amp; (age#1 &gt; 25))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- *(1) Scan ExistingRDD[name#0,age#1,salary#2]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>This output shows the physical plan that Spark will use to execute this query. It indicates that Spark will first scan an existing RDD containing the data, then apply the filter to select only those rows where the age is greater than 25.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="explainextended-boolean-unit">explain(extended: Boolean): Unit<a class="hash-link" href="#explainextended-boolean-unit" title="Direct link to heading">​</a></h4><p>Description:</p><p>The <code>explain</code> function in Spark 3 DataFrame API is used to display the physical plan of a DataFrame or Dataset, with optional extended information. It provides an easy way to understand how Spark will execute a given query. The <code>explain</code> function returns the execution plan as a string, which can be printed to the console or written to a log file.</p><p>Usage Example:</p><p>Suppose we have a DataFrame containing employee data, and we want to see how Spark will execute a query on this DataFrame. We can use the <code>explain</code> function as shown below:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDF = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;John&quot;, 28, 3000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Mary&quot;, 25, 2800.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Peter&quot;, 32, 3500.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (&quot;Jane&quot;, 27, 3200.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">).toDF(&quot;name&quot;, &quot;age&quot;, &quot;salary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeeDF.filter($&quot;age&quot; &gt; 25).explain(true)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first create a DataFrame of employee data using the <code>toDF</code> method. Then we apply a filter to this DataFrame to select only those employees whose age is greater than 25. Finally, we call the <code>explain</code> function on this filtered DataFrame with the <code>extended</code> parameter set to <code>true</code> to display the physical plan of this query with extended information.</p><p>Output:</p><p>The output of this program will be:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">== Parsed Logical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&#x27;Filter (&#x27;age &gt; 25)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- AnalysisBarrier[name#0, age#1, salary#2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">== Analyzed Logical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">name: string, age: int, salary: double</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Filter (age#1 &gt; 25)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- Project [name#0, age#1, salary#2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   +- AnalysisBarrier[name#0, age#1, salary#2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">== Optimized Logical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Filter (isnotnull(age#1) &amp;&amp; (age#1 &gt; 25))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- Project [name#0, age#1, salary#2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   +- Scan ExistingRDD[name#0,age#1,salary#2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">== Physical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">*(1) Filter (isnotnull(age#1) &amp;&amp; (age#1 &gt; 25))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- *(1) Scan ExistingRDD[name#0,age#1,salary#2]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>This output shows the physical plan that Spark will use to execute this query, with additional information about the logical plan and the optimized plan. The <code>extended</code> parameter set to <code>true</code> adds additional details to the plan, making it easier to understand how Spark will execute the query.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="the-explain-method-in-spark-provides-information-about-the-physical-and-logical-plans-of-a-query-execution-it-helps-in-understanding-how-spark-executes-a-query-and-can-be-useful-in-optimizing-the-query">The <code>explain()</code> method in Spark provides information about the physical and logical plans of a query execution. It helps in understanding how Spark executes a query and can be useful in optimizing the query.<a class="hash-link" href="#the-explain-method-in-spark-provides-information-about-the-physical-and-logical-plans-of-a-query-execution-it-helps-in-understanding-how-spark-executes-a-query-and-can-be-useful-in-optimizing-the-query" title="Direct link to heading">​</a></h4><p>The <code>explain(mode: String)</code> method in Spark Dataset API is used to display the physical plan of the query execution in a specific mode. The supported modes are <code>formatted</code>, <code>extended</code>, and <code>codegen</code>.</p><ul><li>The <code>formatted</code> mode displays the physical plan in a human-readable format.</li><li>The <code>extended</code> mode provides additional details about the physical plan, such as the number of output rows, the size of the output in bytes, and the estimated cost of the execution.</li><li>The <code>codegen</code> mode displays the generated code for the query execution.</li></ul><p>Here&#x27;s an example usage of the <code>explain()</code> method:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.functions._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = Seq((1,&quot;a&quot;), (2, &quot;b&quot;), (3, &quot;c&quot;)).toDF(&quot;id&quot;, &quot;name&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.filter(col(&quot;id&quot;) &gt; 1).explain(&quot;extended&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">== Parsed Logical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&#x27;Filter (&#x27;id &gt; 1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- LogicalRDD [id#0, name#1], false</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">== Analyzed Logical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">id: int, name: string</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Filter (id#0 &gt; 1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- LogicalRDD [id#0, name#1], false</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">== Optimized Logical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Filter (id#0 &gt; 1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- LogicalRDD [id#0, name#1], false</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">== Physical Plan ==</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">*(1) Filter (isnotnull(id#0) AND (id#0 &gt; 1))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+- *(1) Scan JDBCRelation((SELECT `id`, `name` FROM `test`) tmpTable) [numPartitions=1] []</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>As you can see in the output, the physical plan of the query execution is displayed in the <code>extended</code> mode.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="hint">hint()<a class="hash-link" href="#hint" title="Direct link to heading">​</a></h4><p>The <code>hint</code> method in Apache Spark allows you to provide a hint to the query optimizer. Hints provide additional information about the dataset to the optimizer that can help it make better decisions about how to execute the query. You can use the <code>hint</code> method to specify optimizer rules or optimization hints in JSON format.</p><p>The <code>hint</code> method takes two parameters: the name of the hint and its parameters. The hint name is a string that specifies the type of hint you want to use. The parameters are optional and are used to provide additional information to the hint.</p><p>Here&#x27;s an example of how to use the <code>hint</code> method:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">val df = spark.read.parquet(&quot;path/to/parquet/file&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Specify the hint &quot;broadcast&quot; with parameter &quot;join&quot; to optimize a join</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.hint(&quot;broadcast&quot;, &quot;join&quot;).show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In the example above, we read in a Parquet file and apply the <code>hint</code> method to the resulting <code>DataFrame</code>. We provide the hint name &quot;broadcast&quot; and the parameter &quot;join&quot; to optimize a join operation. Finally, we call the <code>show</code> method to display the result.</p><p>Note that hints should be used with caution, and you should only use them if you have a good understanding of the query optimizer and how it works. In most cases, the optimizer will make the right decisions about how to execute a query without hints.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="inputfiles-arraystring">inputFiles: Array<!-- -->[String]<a class="hash-link" href="#inputfiles-arraystring" title="Direct link to heading">​</a></h4><p>The <code>inputFiles</code> method is available on Spark 2.0+ and returns an array of strings that represent the input files that were read to create the current <code>DataFrame</code> or <code>Dataset</code>.</p><p>Here&#x27;s an example:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">val df = spark.read.json(&quot;path/to/json/files&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val inputFiles = df.inputFiles</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In the example above, we read in a JSON file and create a <code>DataFrame</code> <code>df</code>. Then, we use the <code>inputFiles</code> method to retrieve the list of input files that were used to create the <code>DataFrame</code>. </p><p>Note that if the <code>DataFrame</code> was created from multiple input files, the <code>inputFiles</code> method returns an array with the path of all files. Also, if the input file is a directory, then all the files within that directory will be returned as part of the array.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="spark-3-dataset-isempty-boolean">Spark 3 Dataset: isEmpty: Boolean<a class="hash-link" href="#spark-3-dataset-isempty-boolean" title="Direct link to heading">​</a></h4><p>The <code>isEmpty</code> method in Spark 3 Dataset API returns a Boolean that indicates if the Dataset is empty or not. If the Dataset is empty, it returns true; otherwise, it returns false.</p><p>Usage</p><p>The <code>isEmpty</code> method is called on a Dataset object and returns a Boolean value indicating if the Dataset is empty or not. Here is the basic syntax of calling the <code>isEmpty</code> method:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">val ds: Dataset[Person] = Seq.empty[Person].toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val isEmpty: Boolean = ds.isEmpty</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Example</p><p>Here&#x27;s an example of using the <code>isEmpty</code> method to check if a Dataset is empty or not:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                .appName(&quot;isEmptyExample&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import spark.implicits._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val emptyDs = Seq.empty[Person].toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val nonEmptyDs = Seq(Person(&quot;Alice&quot;, 25), Person(&quot;Bob&quot;, 30)).toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(emptyDs.isEmpty)      // true</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(nonEmptyDs.isEmpty)   // false</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output</p><p>The output of the example code will be:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">true</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">false</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note that the <code>isEmpty</code> method returns <code>true</code> for an empty Dataset and <code>false</code> for a non-empty Dataset.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="function-name-islocal">Function Name: isLocal()<a class="hash-link" href="#function-name-islocal" title="Direct link to heading">​</a></h4><p>Description:
isLocal() is a boolean function in Spark Dataset API that checks if the current Dataset is local or distributed across a cluster.</p><p>Usage:
Assuming a Dataset &quot;ds&quot; has been created, we can use the isLocal() function as follows:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">val isDsLocal: Boolean = ds.isLocal</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:
The isLocal() function returns true if the Dataset is local, meaning that all data is stored on a single machine, and false if the data is distributed across a cluster.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="javardd">javaRDD()<a class="hash-link" href="#javardd" title="Direct link to heading">​</a></h4><p>The <code>javaRDD</code> function is a method of the Spark 3 Dataset API in Scala. It returns a JavaRDD object that represents the current Dataset as an RDD of Java objects. The elements in the RDD are of type T, which is the same type as the elements in the Dataset.</p><p>Usage Example:</p><p>Suppose you have a Dataset of employee data and you want to convert it to a JavaRDD. Here&#x27;s how you can use the <code>javaRDD</code> function to do that:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.api.java.JavaRDD</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeData = Seq(Employee(&quot;John Doe&quot;, 25), Employee(&quot;Jane Smith&quot;, 30), Employee(&quot;Bob Johnson&quot;, 40))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDS = employeeData.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeRDD: JavaRDD[Employee] = employeeDS.javaRDD()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:</p><p>In this example, the <code>javaRDD</code> function is called on the <code>employeeDS</code> Dataset, which converts it to a JavaRDD of Employee objects. The <code>employeeRDD</code> JavaRDD can now be used with Java APIs that require RDDs, instead of using the Dataset APIs.</p><p>Note: The JavaRDD object returned by <code>javaRDD</code> is immutable, meaning it cannot be modified.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="function-name-localcheckpoint">Function Name: localCheckpoint<a class="hash-link" href="#function-name-localcheckpoint" title="Direct link to heading">​</a></h4><p>Description: The localCheckpoint() function is used to persist a Dataset on the local file system. Unlike checkpoint() function which persists the Dataset on a fault-tolerant storage system, localCheckpoint() function stores the Dataset on the local file system which is not fault-tolerant. The purpose of localCheckpoint() function is to improve the performance of iterative algorithms.</p><p>Usage: The localCheckpoint() function can be used on a Dataset to persist it on the local file system. Once persisted, the Dataset can be retrieved from the local file system and used for further processing.</p><p>Example:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                .appName(&quot;localCheckpoint Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(1, 2, 3, 4, 5)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val ds = spark.createDataset(data)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ds.localCheckpoint()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val result = ds.map(x =&gt; x * 2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">result.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| _1|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  2|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  4|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  6|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  8|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| 10|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In the above example, we create a Dataset &#x27;ds&#x27; from a sequence of integers. We then apply the localCheckpoint() function on it to persist it on the local file system. Later, we apply the map() function on the Dataset to multiply each element by 2 and store the result in &#x27;result&#x27; variable. Finally, we print the output of the &#x27;result&#x27; variable using the show() function.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="spark-3-dataset-persistnewlevel-storagelevel">Spark 3 Dataset: persist(newLevel: StorageLevel)<a class="hash-link" href="#spark-3-dataset-persistnewlevel-storagelevel" title="Direct link to heading">​</a></h4><p>The <code>persist(newLevel: StorageLevel)</code> function in Spark 3 Dataset API is used to persist the dataset with the specified storage level in memory or disk or both. The storage level determines where the data is stored and in what format, and it can be one of the following: </p><ul><li>MEMORY_ONLY</li><li>MEMORY_AND_DISK</li><li>MEMORY_ONLY_SER</li><li>MEMORY_AND_DISK_SER</li><li>DISK_ONLY</li><li>MEMORY_ONLY_2</li><li>MEMORY_AND_DISK_2</li><li>MEMORY_ONLY_SER_2</li><li>MEMORY_AND_DISK_SER_2</li><li>OFF_HEAP</li></ul><p>By default, the storage level is set to MEMORY_AND_DISK, which means that the data is stored both in memory and on disk.</p><p>Usage</p><p>Here&#x27;s an example of how to use <code>persist(newLevel: StorageLevel)</code> in Scala:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.storage.StorageLevel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset = Seq((1, &quot;John&quot;), (2, &quot;Sarah&quot;), (3, &quot;Alex&quot;)).toDF(&quot;id&quot;, &quot;name&quot;).as[(Int, String)]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Persisting the dataset with MEMORY_ONLY storage level</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dataset.persist(StorageLevel.MEMORY_ONLY)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Doing some transformations on the persisted dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val filteredDataset = dataset.filter(_._2.startsWith(&quot;A&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Unpersisting the dataset from memory</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dataset.unpersist()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In the above example, we created a simple dataset of (id, name) pairs, and persisted it with the MEMORY_ONLY storage level. We then performed some transformations on the persisted dataset, and finally unpersisted it from memory.</p><p>Output</p><p>There is no output generated by <code>persist(newLevel: StorageLevel)</code> function. However, it can improve the performance of subsequent operations on the dataset by avoiding the need to recompute the dataset from scratch each time it is accessed.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="printschemalevel-int-unit">printSchema(level: Int): Unit<a class="hash-link" href="#printschemalevel-int-unit" title="Direct link to heading">​</a></h4><p>The <code>printSchema</code> method is used to print the schema of a Spark <code>Dataset</code>. The schema is a representation of the data structure of the <code>Dataset</code>, which includes the names and data types of its columns.</p><p>The method takes an optional <code>level</code> parameter that specifies the indentation level for nested fields. By default, the level is set to 0, which means that all fields are printed at the same level.</p><p>Usage Example:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Row}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;printSchemaExample&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// define a schema for a DataFrame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val schema = StructType(Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  StructField(&quot;name&quot;, StringType, nullable = false),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  StructField(&quot;age&quot;, IntegerType, nullable = true)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// create a sample DataFrame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Row(&quot;Alice&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Row(&quot;Bob&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Row(&quot;Charlie&quot;, null)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// print the schema</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.printSchema()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">root</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> |-- name: string (nullable = false)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> |-- age: integer (nullable = true)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_DXrL" id="schema-structtype">schema: StructType<a class="hash-link" href="#schema-structtype" title="Direct link to heading">​</a></h4><p><code>schema</code> is a method available in Spark 3&#x27;s Dataset API. It returns the schema of the Dataset as a StructType object. The schema describes the structure of the data in the Dataset, including column names, data types, and nullable constraints.</p><p>Usage Example:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.functions.col</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq((&quot;John&quot;, 25), (&quot;Bob&quot;, 30), (&quot;Jane&quot;, 35))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = spark.createDataFrame(data).toDF(&quot;name&quot;, &quot;age&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val mySchema: StructType = df.schema</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(&quot;Schema of my DataFrame:&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mySchema.printTreeString()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Schema of my DataFrame:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">root</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> |-- name: string (nullable = true)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> |-- age: integer (nullable = false)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we create a DataFrame called <code>df</code> from a sequence of tuples. We then use the <code>schema</code> method to obtain the schema of the DataFrame as a <code>StructType</code> object called <code>mySchema</code>. Finally, we use the <code>printTreeString</code> method to print the schema in a tree-like format. The output shows that the DataFrame has two columns, &quot;name&quot; and &quot;age&quot;, and the data types of these columns are String and Integer, respectively.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="storagelevel-storagelevel">storageLevel: StorageLevel<a class="hash-link" href="#storagelevel-storagelevel" title="Direct link to heading">​</a></h4><p><code>storageLevel</code> is a method in Spark 3 Dataset API that returns the storage level of the Dataset.</p><p>The storage level is the level of caching used to persist the Dataset in memory or disk, and it is specified using the <code>persist()</code> method. It determines the trade-off between memory usage and performance, as a higher storage level will use more memory but result in faster execution times.</p><p>The <code>storageLevel</code> method returns an object of the <code>StorageLevel</code> class, which represents the storage level of the Dataset. The <code>StorageLevel</code> class has several static methods that can be used to create a storage level object with different caching options, such as <code>MEMORY_ONLY</code>, <code>MEMORY_AND_DISK</code>, <code>DISK_ONLY</code>, etc.</p><p>Example usage:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.storage.StorageLevel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// create a DataFrame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = Seq((1, &quot;John&quot;), (2, &quot;Mary&quot;), (3, &quot;Bob&quot;)).toDF(&quot;id&quot;, &quot;name&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// cache the DataFrame with MEMORY_ONLY storage level</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.persist(StorageLevel.MEMORY_ONLY)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// get the storage level of the DataFrame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val level = df.storageLevel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// print the storage level</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(level)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Memory Deserialized 1x Replicated</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we create a DataFrame <code>df</code> and cache it in memory with <code>MEMORY_ONLY</code> storage level using the <code>persist()</code> method. We then call the <code>storageLevel</code> method on the DataFrame to get its storage level, and print it to the console. The output shows that the DataFrame is stored in memory, deserialized, and replicated once.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="todfcolnames-string-dataframe">toDF(colNames: String*): DataFrame<a class="hash-link" href="#todfcolnames-string-dataframe" title="Direct link to heading">​</a></h4><p>The <code>toDF</code> function is used to convert a <code>Dataset</code> into a <code>DataFrame</code>, where the column names are specified by the input argument <code>colNames</code>. This function returns a <code>DataFrame</code>.</p><p>Usage Example:</p><p>Suppose we have a case class <code>Person</code> and a <code>Dataset</code> of <code>Person</code> objects. We can convert this <code>Dataset</code> into a <code>DataFrame</code> as follows:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val people = Seq(Person(&quot;Alice&quot;, 25), Person(&quot;Bob&quot;, 30)).toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = people.toDF(&quot;person_name&quot;, &quot;person_age&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+------------+----------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|person_name |person_age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+------------+----------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice       |        25|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Bob         |        30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+------------+----------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p> toDF(): DataFrame</p><p>The <code>toDF</code> function is used to convert a <code>Dataset</code> into a <code>DataFrame</code>, where the column names are automatically generated based on the field names of the case class used to create the <code>Dataset</code>. This function returns a <code>DataFrame</code>.</p><p>Usage Example:</p><p>Suppose we have a case class <code>Person</code> and a <code>Dataset</code> of <code>Person</code> objects. We can convert this <code>Dataset</code> into a <code>DataFrame</code> as follows:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val people = Seq(Person(&quot;Alice&quot;, 25), Person(&quot;Bob&quot;, 30)).toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = people.toDF()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice| 25|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Bob| 30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Note: The <code>toDF</code> function generates column names based on the field names of the case class used to create the <code>Dataset</code>. It removes the prefix <code>_$outer</code> from the column names, which is added by Scala&#x27;s inner classes. If there are no field names in the case class, then the generated column names will be &quot;col0&quot;, &quot;col1&quot;, etc.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="tojavardd-javarddt">toJavaRDD: JavaRDD<!-- -->[T]<a class="hash-link" href="#tojavardd-javarddt" title="Direct link to heading">​</a></h4><p><code>toJavaRDD</code> is a function in Apache Spark&#x27;s Dataset API that converts a <code>Dataset[T]</code> object into a JavaRDD<!-- -->[T]<!-- --> object. The JavaRDD<!-- -->[T]<!-- --> object can then be used in Java code to perform operations on the data.</p><p>Here&#x27;s an example usage of <code>toJavaRDD</code>:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.api.java.JavaRDD</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{Dataset, SparkSession}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;toJavaRDDExample&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// create a Dataset of Person objects</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val peopleData: Seq[Person] = Seq(Person(&quot;Alice&quot;, 25), Person(&quot;Bob&quot;, 30), Person(&quot;Charlie&quot;, 35))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val peopleDS: Dataset[Person] = spark.createDataset(peopleData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// convert the Dataset to a JavaRDD</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val peopleRDD: JavaRDD[Person] = peopleDS.toJavaRDD()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// print out the elements in the JavaRDD</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">peopleRDD.foreach(println)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we create a <code>Dataset</code> of <code>Person</code> objects using the <code>createDataset</code> function of a <code>SparkSession</code>. We then call <code>toJavaRDD</code> on this <code>Dataset</code> to convert it into a <code>JavaRDD</code>. Finally, we print out the elements in the <code>JavaRDD</code> using the <code>foreach</code> function.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="function-name-unpersist">Function name: unpersist()<a class="hash-link" href="#function-name-unpersist" title="Direct link to heading">​</a></h4><p>Description: The <code>unpersist()</code> function in Spark 3 Dataset API is used to remove the persisted memory of the RDD or Dataset from the Spark&#x27;s cache. This method is useful when the data in the cache is no longer needed and we want to release the memory occupied by the cache for other use.</p><p>Usage example:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;unpersistExample&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[2]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = spark.read.csv(&quot;example.csv&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Cache the DataFrame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.cache()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Unpersist the DataFrame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.unpersist()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Check if the DataFrame is still in the cache</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(df.is_cached)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">false</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first read a CSV file and then cache the resulting DataFrame. We then unpersist the DataFrame and check whether it is still in the cache by using the <code>is_cached</code> method. The output shows that the DataFrame is no longer cached. </p><hr><p>Function name: unpersist(blocking: Boolean)</p><p>Description: The <code>unpersist(blocking: Boolean)</code> function in Spark 3 Dataset API is used to remove the persisted memory of the RDD or Dataset from the Spark&#x27;s cache. The blocking parameter indicates whether the method should block until the cache is removed.</p><p>Usage example:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;unpersistExample&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[2]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = spark.read.csv(&quot;example.csv&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Cache the DataFrame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.cache()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Unpersist the DataFrame blocking until it is removed</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.unpersist(blocking = true)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Check if the DataFrame is still in the cache</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(df.is_cached)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">false</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first read a CSV file and then cache the resulting DataFrame. We then unpersist the DataFrame with blocking set to true and check whether it is still in the cache by using the <code>is_cached</code> method. The output shows that the DataFrame is no longer cached.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="write-dataframewritert">write: DataFrameWriter<!-- -->[T]<a class="hash-link" href="#write-dataframewritert" title="Direct link to heading">​</a></h4><p>The <code>write</code> method is used to write the contents of a Spark <code>Dataset</code> to an external storage system in various formats such as CSV, JSON, Parquet, ORC, and more. It returns a <code>DataFrameWriter</code> object, which provides methods for configuring the write operation, such as setting the output path, format, partitioning, compression, and more.</p><p>Here&#x27;s an example of using the <code>write</code> method to write a <code>Dataset</code> of <code>Person</code> objects to a Parquet file:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{Dataset, SparkSession}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Write Dataset to Parquet&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val people = Seq(Person(&quot;Alice&quot;, 25), Person(&quot;Bob&quot;, 30), Person(&quot;Charlie&quot;, 35))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val peopleDS: Dataset[Person] = spark.createDataset(people)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">peopleDS.write</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .format(&quot;parquet&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .mode(&quot;overwrite&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .save(&quot;/path/to/output/parquet&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, we first define a case class <code>Person</code> and create a <code>Dataset</code> of <code>Person</code> objects. Then we call the <code>write</code> method on the <code>peopleDS</code> <code>Dataset</code>, which returns a <code>DataFrameWriter</code>. We configure the writer to use the Parquet format, overwrite any existing output, and save the output to the specified path. Finally, we call the <code>save</code> method to execute the write operation.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="writestream-datastreamwritert">writeStream: DataStreamWriter<!-- -->[T]<a class="hash-link" href="#writestream-datastreamwritert" title="Direct link to heading">​</a></h4><p>The <code>writeStream</code> function is used in Spark to write the contents of a streaming <code>Dataset</code> to an output sink. It returns a <code>DataStreamWriter</code> object, which can be used to configure the output sink and write the data.</p><p>Here&#x27;s a brief description of the function parameters and their usage:</p><ul><li><code>format(source: String)</code>: Specifies the format of the output sink. The <code>source</code> parameter should be a valid identifier for the sink format, such as <code>&quot;parquet&quot;</code>, <code>&quot;csv&quot;</code>, <code>&quot;kafka&quot;</code>, or <code>&quot;memory&quot;</code>.</li><li><code>outputMode(outputMode: String)</code>: Specifies the output mode of the sink. The <code>outputMode</code> parameter should be a valid identifier for the output mode, such as <code>&quot;append&quot;</code>, <code>&quot;complete&quot;</code>, or <code>&quot;update&quot;</code>.</li><li><code>option(key: String, value: String)</code>: Allows additional configuration options to be passed to the sink.</li><li><code>partitioning(columns: Column*)</code>: Specifies the partitioning scheme for the output data.</li><li><code>queryName(queryName: String)</code>: Specifies a name for the streaming query, which can be useful for monitoring and debugging.</li><li><code>trigger(trigger: Trigger)</code>: Specifies the trigger for the streaming query, which determines how often the sink is updated.</li></ul><p>Usage example:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.streaming.Trigger</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// create a streaming DataFrame from a Kafka topic</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = spark</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .readStream</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .format(&quot;kafka&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;subscribe&quot;, &quot;my_topic&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .load()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// write the streaming DataFrame to a console sink</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val query = df</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .writeStream</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .format(&quot;console&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .outputMode(&quot;append&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;truncate&quot;, false)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .start()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// trigger the sink to update every 10 seconds</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">query.trigger(Trigger.ProcessingTime(&quot;10 seconds&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// wait for the query to finish</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">query.awaitTermination()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>This code reads a streaming DataFrame from a Kafka topic and writes it to the console sink in append mode. The <code>trigger</code> function is used to update the sink every 10 seconds. Finally, the <code>awaitTermination</code> function is called to start the query and wait for it to finish.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="writetotable-string-dataframewriterv2t">writeTo(table: String): DataFrameWriterV2<!-- -->[T]<a class="hash-link" href="#writetotable-string-dataframewriterv2t" title="Direct link to heading">​</a></h4><p>The <code>writeTo</code> function in Spark 3 Dataset API allows you to write the content of a Dataset to a specific table in a catalog. </p><p>Here is a brief description of the function:</p><ul><li><code>writeTo(table: String): DataFrameWriterV2[T]</code>: Writes the content of the dataset to a specific table in a catalog.</li></ul><p>The <code>writeTo</code> function returns a <code>DataFrameWriterV2[T]</code> object that can be used to configure various output options for the data being written.</p><p>Usage example:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.SaveMode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val df = Seq((&quot;Alice&quot;, 25), (&quot;Bob&quot;, 30), (&quot;Charlie&quot;, 35)).toDF(&quot;name&quot;, &quot;age&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Write the content of the dataframe to a specific table in a catalog</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df.writeTo(&quot;my_database.my_table&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .mode(SaveMode.Append)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .save()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>In this example, the content of the DataFrame <code>df</code> is written to the table <code>my_database.my_table</code> using the <code>writeTo</code> function. The <code>mode</code> function is used to specify the save mode as <code>Append</code>. Finally, the <code>save</code> function is called to write the data to the specified table.</p><h2 class="anchor anchorWithStickyNavbar_DXrL" id="streaming">Streaming<a class="hash-link" href="#streaming" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_DXrL" id="def-isstreaming-boolean">def isStreaming: Boolean<a class="hash-link" href="#def-isstreaming-boolean" title="Direct link to heading">​</a></h4><p>Description:
The <code>def isStreaming</code> function is used to determine whether a Dataset contains one or more sources that continuously return data as it arrives, indicating that the Dataset is associated with a streaming source. When a Dataset reads data from a streaming source, it needs to be executed as a StreamingQuery using the <code>start()</code> method in DataStreamWriter. This function is particularly useful when working with streaming data in Apache Spark, as it allows you to distinguish between streaming and non-streaming Datasets.</p><p>Usage Example:
Suppose we have a Dataset <code>streamingData</code> that is reading data from a Kafka streaming source. We can use the <code>def isStreaming</code> function to check if the Dataset is associated with a streaming source.</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Streaming Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import spark.implicits._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Reading data from a Kafka streaming source</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val streamingData = spark.readStream</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .format(&quot;kafka&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;subscribe&quot;, &quot;topic1&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .load()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Checking if the Dataset is associated with a streaming source</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val isStreaming = streamingData.isStreaming</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(s&quot;Is the Dataset streaming? $isStreaming&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Is the Dataset streaming? true</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Explanation:
In the example above, we first create a SparkSession and import its implicits to enable Spark SQL functionalities. We then read data from a Kafka streaming source using <code>spark.readStream</code> and other options specific to Kafka. After reading the data, we call the <code>isStreaming</code> function on the <code>streamingData</code> Dataset to check if it is associated with a streaming source. Since we are reading data from a streaming source (Kafka), the <code>isStreaming</code> function returns <code>true</code>, indicating that the Dataset contains a streaming source. If the Dataset were reading data from a batch source, <code>isStreaming</code> would return <code>false</code>.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="def-withwatermarkeventtime-string-delaythreshold-string-datasett">def withWatermark(eventTime: String, delayThreshold: String): Dataset<!-- -->[T]<a class="hash-link" href="#def-withwatermarkeventtime-string-delaythreshold-string-datasett" title="Direct link to heading">​</a></h4><p>Description:
The <code>withWatermark</code> function is used to define an event time watermark for a Dataset that contains event time data. A watermark is a point in time before which it is assumed that no more late data will arrive. Spark uses watermarks for various purposes, including determining when a time window aggregation can be finalized and to optimize state management for ongoing aggregations, mapGroupsWithState, and dropDuplicates operators.</p><p>The current watermark is computed by finding the maximum event time (MAX(eventTime)) across all partitions in the query and then subtracting a user-specified <code>delayThreshold</code> from it. The <code>delayThreshold</code> is the minimum delay to wait for late data to arrive, relative to the latest record that has been processed. For example, if the <code>delayThreshold</code> is set to &quot;1 minute,&quot; it means that Spark will wait for data to arrive up to 1 minute after the latest processed record before finalizing time-based aggregations.</p><p>Please note that due to the cost of coordinating the watermark value across partitions, the actual watermark used is only guaranteed to be at least <code>delayThreshold</code> behind the actual event time. As a result, in some cases, Spark may still process records that arrive more than <code>delayThreshold</code> late.</p><p>Parameters:</p><ul><li>eventTime: The name of the column that contains the event time of each row in the Dataset.</li><li>delayThreshold: The minimum delay to wait for data to arrive late, specified as an interval (e.g., &quot;1 minute&quot; or &quot;5 hours&quot;). The <code>delayThreshold</code> should not be negative.</li></ul><p>Usage Example:
Suppose we have a streaming Dataset <code>streamingData</code> that contains event time data. We want to define an event time watermark for the event time column &quot;timestamp&quot; with a delay threshold of &quot;5 minutes.&quot;</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.functions._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Watermark Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import spark.implicits._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Reading streaming data with event time &quot;timestamp&quot; from a Kafka source</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val streamingData = spark.readStream</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .format(&quot;kafka&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .option(&quot;subscribe&quot;, &quot;topic1&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .load()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .selectExpr(&quot;CAST(value AS STRING)&quot;, &quot;CAST(timestamp AS TIMESTAMP)&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Defining an event time watermark with a delay threshold of &quot;5 minutes&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val watermarkedData = streamingData.withWatermark(&quot;timestamp&quot;, &quot;5 minutes&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Performing windowed aggregation based on event time with watermark applied</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val windowedAggregation = watermarkedData</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .groupBy(window($&quot;timestamp&quot;, &quot;10 minutes&quot;, &quot;5 minutes&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .count()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Starting the streaming query</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val query = windowedAggregation.writeStream</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .outputMode(&quot;update&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .format(&quot;console&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .start()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">query.awaitTermination()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Explanation:
In the example above, we create a SparkSession and import its implicits to enable Spark SQL functionalities. We then read streaming data from a Kafka source with an event time column &quot;timestamp.&quot; We apply a watermark with a delay threshold of &quot;5 minutes&quot; to the Dataset using the <code>withWatermark</code> function. After that, we perform windowed aggregation based on event time with a window size of &quot;10 minutes&quot; and a slide duration of &quot;5 minutes.&quot;</p><p>By applying the watermark, Spark will know when to finalize the windowed aggregations, and it will also minimize the state needed for ongoing aggregations. The streaming query is set to output the results in &quot;update&quot; mode and display them on the console.</p><p>Please note that in a real-world scenario, the output mode and the sink would depend on the use case and requirements.</p><h2 class="anchor anchorWithStickyNavbar_DXrL" id="typed-transformations">Typed transformations<a class="hash-link" href="#typed-transformations" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_DXrL" id="aliasalias-symbol-datasett"><code>alias(alias: Symbol): Dataset[T]</code><a class="hash-link" href="#aliasalias-symbol-datasett" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_DXrL" id="aliasalias-string-datasett"><code>alias(alias: String): Dataset[T]</code><a class="hash-link" href="#aliasalias-string-datasett" title="Direct link to heading">​</a></h4><p>Description:
The <code>alias</code> function in Apache Spark&#x27;s Dataset API is used to assign an alias (a new name) to a column in a Dataset. It creates a new Dataset with the specified column being renamed to the given alias. There are two overloaded versions of this function: one that takes a Scala Symbol as the alias parameter, and another that takes a String as the alias parameter. Both versions achieve the same result of renaming the column.</p><p>Parameters:</p><ul><li><code>alias</code> (Symbol or String): The alias is specified either as a Scala Symbol or a String. A Symbol is denoted by a prefix <code>:</code>, followed by the desired name. Symbols are used in Scala to represent simple names in a concise and efficient way. On the other hand, a String is a sequence of characters representing the desired name.</li></ul><p>Usage Example:
Suppose we have a Dataset <code>employees</code> that contains information about employees, and we want to rename the &quot;age&quot; column to &quot;employee_age&quot;.</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Assuming we have a case class to represent the Employee data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, department: String)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Alias Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import spark.implicits._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Alice&quot;, 30, &quot;HR&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Bob&quot;, 25, &quot;Engineering&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Eve&quot;, 28, &quot;Marketing&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating a Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employees: Dataset[Employee] = data.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Renaming the &quot;age&quot; column to &quot;employee_age&quot; using the alias function with a Symbol</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeesWithAliasSymbol = employees.alias(&#x27;employee_age)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Renaming the &quot;age&quot; column to &quot;employee_age&quot; using the alias function with a String</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeesWithAliasString = employees.alias(&quot;employee_age&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the result of both Datasets</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeesWithAliasSymbol.show()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeesWithAliasString.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+-----+------------+------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| name|employee_age|  department |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+------------+------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice|          30|          HR|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Bob|          25|Engineering|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Eve|          28|   Marketing|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+------------+------------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Explanation:
In the example above, we create a SparkSession and import its implicits to enable Spark SQL functionalities. We define a case class <code>Employee</code> to represent the data and create a sample Dataset <code>employees</code> with the sample data.</p><p>We use the <code>alias</code> function with both the Symbol <code>&#x27;employee_age</code> and the String <code>&quot;employee_age&quot;</code> to rename the &quot;age&quot; column to &quot;employee_age&quot; in two different Datasets: <code>employeesWithAliasSymbol</code> and <code>employeesWithAliasString</code>. The result is displayed, showing that both Datasets have successfully renamed the column &quot;age&quot; to &quot;employee_age&quot; in the output. The two overloaded versions of the <code>alias</code> function achieve the same result, providing flexibility for developers to choose the alias representation they prefer.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="asalias-symbol-datasett"><code>as(alias: Symbol): Dataset[T]</code><a class="hash-link" href="#asalias-symbol-datasett" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_DXrL" id="asalias-string-datasett"><code>as(alias: String): Dataset[T]</code><a class="hash-link" href="#asalias-string-datasett" title="Direct link to heading">​</a></h4><p>Description:
The <code>as</code> function in Apache Spark&#x27;s Dataset API is used to assign an alias (a new name) to the entire Dataset. It creates a new Dataset with the specified alias. There are two overloaded versions of this function: one that takes a Scala Symbol as the alias parameter, and another that takes a String as the alias parameter. Both versions achieve the same result of renaming the Dataset itself.</p><p>Parameters:</p><ul><li><code>alias</code> (Symbol or String): The alias is specified either as a Scala Symbol or a String. A Symbol is denoted by a prefix <code>:</code>, followed by the desired name. Symbols are used in Scala to represent simple names in a concise and efficient way. On the other hand, a String is a sequence of characters representing the desired name.</li></ul><p>Usage Example:
Suppose we have a Dataset <code>employees</code> that contains information about employees, and we want to assign an alias &quot;empData&quot; to the entire Dataset.</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Assuming we have a case class to represent the Employee data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, department: String)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Alias Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import spark.implicits._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Alice&quot;, 30, &quot;HR&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Bob&quot;, 25, &quot;Engineering&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Eve&quot;, 28, &quot;Marketing&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating a Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employees: Dataset[Employee] = data.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Assigning an alias &quot;empData&quot; to the entire Dataset using the as function with a Symbol</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeesWithAliasSymbol = employees.as(&#x27;empData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Assigning an alias &quot;empData&quot; to the entire Dataset using the as function with a String</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeesWithAliasString = employees.as(&quot;empData&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the result of both Datasets</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeesWithAliasSymbol.show()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">employeesWithAliasString.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| name|age|  department |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice| 30|          HR|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Bob| 25|Engineering|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Eve| 28|   Marketing|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+------------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Explanation:
In the example above, we create a SparkSession and import its implicits to enable Spark SQL functionalities. We define a case class <code>Employee</code> to represent the data and create a sample Dataset <code>employees</code> with the sample data.</p><p>We use the <code>as</code> function with both the Symbol <code>&#x27;empData</code> and the String <code>&quot;empData&quot;</code> to assign an alias &quot;empData&quot; to the entire Dataset in two different Datasets: <code>employeesWithAliasSymbol</code> and <code>employeesWithAliasString</code>. The result is displayed, showing that both Datasets have successfully been assigned the alias &quot;empData&quot; in the output. The two overloaded versions of the <code>as</code> function achieve the same result, providing flexibility for developers to choose the alias representation they prefer.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="coalescenumpartitions-int-datasett"><code>coalesce(numPartitions: Int): Dataset[T]</code><a class="hash-link" href="#coalescenumpartitions-int-datasett" title="Direct link to heading">​</a></h4><p>Description:
The <code>coalesce</code> function in Apache Spark&#x27;s Dataset API is used to reduce the number of partitions in a Dataset by combining them into a smaller number of partitions. It returns a new Dataset with the specified number of partitions. The data in the original partitions is redistributed into the new partitions in a balanced manner.</p><p>Parameters:</p><ul><li><code>numPartitions</code> (Int): The number of partitions that the new Dataset should have. It determines the level of parallelism during processing. The value of <code>numPartitions</code> should be greater than 0.</li></ul><p>Usage Example:
Suppose we have a Dataset <code>data</code> that contains some elements and is partitioned into four partitions, and we want to coalesce it into two partitions.</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Coalesce Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating a Dataset from the sample data and specifying the initial number of partitions (4)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset: Dataset[Int] = spark.createDataset(data)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Getting the number of partitions before coalescing</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val partitionsBeforeCoalesce = dataset.rdd.getNumPartitions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Coalescing the Dataset into two partitions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val coalescedDataset = dataset.coalesce(2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Getting the number of partitions after coalescing</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val partitionsAfterCoalesce = coalescedDataset.rdd.getNumPartitions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the result</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(s&quot;Number of partitions before coalesce: $partitionsBeforeCoalesce&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(s&quot;Number of partitions after coalesce: $partitionsAfterCoalesce&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Number of partitions before coalesce: 4</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Number of partitions after coalesce: 2</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Explanation:
In the example above, we create a SparkSession and import its implicits to enable Spark SQL functionalities. We define a sample Dataset <code>dataset</code> with ten elements and specify that it should be partitioned into four partitions by default.</p><p>We use the <code>coalesce</code> function with the parameter <code>2</code> to coalesce the Dataset into two partitions. The result is stored in the new Dataset <code>coalescedDataset</code>. The output shows that the number of partitions has been reduced from four to two after the coalesce operation. The <code>coalesce</code> function is useful for optimizing the parallelism in the processing of Datasets, especially when you want to reduce the number of partitions for better resource utilization or to avoid unnecessary data shuffling.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="distinct-datasett"><code>distinct(): Dataset[T]</code><a class="hash-link" href="#distinct-datasett" title="Direct link to heading">​</a></h4><p>Description:
The <code>distinct</code> function in Apache Spark&#x27;s Dataset API is used to remove duplicate rows from the Dataset. It returns a new Dataset with only the distinct rows. The deduplication process is performed based on the values of all columns in the Dataset.</p><p>Usage Example:
Suppose we have a Dataset <code>data</code> that contains some duplicate rows, and we want to remove these duplicates.</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Distinct Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset with duplicates</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (1, &quot;Alice&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (2, &quot;Bob&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (3, &quot;Alice&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (4, &quot;John&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (5, &quot;Bob&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (6, &quot;Alice&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating a Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset: Dataset[(Int, String)] = spark.createDataset(data)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Removing duplicates and creating a new Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val distinctDataset = dataset.distinct()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the result</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">distinctDataset.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| _1|  _2 |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  4| John|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  5|  Bob|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  1|Alice|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  2|  Bob|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  3|Alice|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Explanation:
In the example above, we create a SparkSession and import its implicits to enable Spark SQL functionalities. We define a sample Dataset <code>dataset</code> containing tuples with duplicate names. When we call the <code>distinct</code> function on the Dataset, it removes the duplicate rows and returns a new Dataset <code>distinctDataset</code> with only the distinct rows based on both columns.</p><p>The output displays the unique rows in the Dataset. In this case, the rows with names &quot;Alice&quot; and &quot;Bob&quot; appear only once in the resulting Dataset, and the duplicate rows have been removed.</p><p>The <code>distinct</code> function is useful when you need to eliminate duplicate records from a Dataset to ensure data consistency or to prepare the data for further analysis or processing.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="dropduplicatescol1-string-cols-string-datasett"><code>dropDuplicates(col1: String, cols: String*): Dataset[T]</code><a class="hash-link" href="#dropduplicatescol1-string-cols-string-datasett" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_DXrL" id="dropduplicatescolnames-arraystring-datasett"><code>dropDuplicates(colNames: Array[String]): Dataset[T]</code><a class="hash-link" href="#dropduplicatescolnames-arraystring-datasett" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_DXrL" id="dropduplicatescolnames-seqstring-datasett"><code>dropDuplicates(colNames: Seq[String]): Dataset[T]</code><a class="hash-link" href="#dropduplicatescolnames-seqstring-datasett" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_DXrL" id="dropduplicates-datasett"><code>dropDuplicates(): Dataset[T]</code><a class="hash-link" href="#dropduplicates-datasett" title="Direct link to heading">​</a></h4><p>Description:
The <code>dropDuplicates</code> function in Apache Spark&#x27;s Dataset API is used to remove duplicate rows from the Dataset based on specific columns. It returns a new Dataset with only the distinct rows after considering the specified columns. There are multiple variants of this function that allow you to provide the column names either as separate arguments, an Array, or a Seq.</p><p>Usage Example:
Suppose we have a Dataset <code>data</code> that contains some duplicate rows, and we want to remove these duplicates based on specific columns.</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;DropDuplicates Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset with duplicates</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (1, &quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (2, &quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (3, &quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (4, &quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (5, &quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (6, &quot;Alice&quot;, 28)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating a Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset: Dataset[(Int, String, Int)] = spark.createDataset(data)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Removing duplicates based on specific columns</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val distinctByCols = dataset.dropDuplicates(&quot;col2&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Removing duplicates based on an Array of column names</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val distinctByArray = dataset.dropDuplicates(Array(&quot;col1&quot;, &quot;col3&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Removing duplicates based on a Seq of column names</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val distinctBySeq = dataset.dropDuplicates(Seq(&quot;col1&quot;, &quot;col2&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Removing duplicates without specifying any columns (drops all duplicates)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val distinctAll = dataset.dropDuplicates()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the results</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">distinctByCols.show()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">distinctByArray.show()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">distinctBySeq.show()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">distinctAll.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Output:</p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|_1 |_2   |_3 |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|2  |Bob  |25 |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|1  |Alice|30 |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|4  |John |40 |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p>Explanation:
In the example above, we create a SparkSession and import its implicits to enable Spark SQL functionalities. We define a sample Dataset <code>dataset</code> containing tuples with duplicate values in some columns. We then demonstrate different ways to use the <code>dropDuplicates</code> function to remove duplicates based on specific columns or without specifying any columns.</p><ol><li><code>distinctByCols</code>: The duplicates are removed based on the column &quot;col2&quot;, resulting in unique rows with distinct &quot;col2&quot; values.</li><li><code>distinctByArray</code>: The duplicates are removed based on both &quot;col1&quot; and &quot;col3&quot; columns, resulting in unique rows with distinct combinations of &quot;col1&quot; and &quot;col3&quot; values.</li><li><code>distinctBySeq</code>: The duplicates are removed based on both &quot;col1&quot; and &quot;col2&quot; columns, resulting in unique rows with distinct combinations of &quot;col1&quot; and &quot;col2&quot; values.</li><li><code>distinctAll</code>: No specific columns are specified, so all duplicates across all columns are removed, resulting in unique rows with distinct combinations of values in all columns.</li></ol><p>The <code>dropDuplicates</code> function is useful when you need to eliminate duplicate records based on specific columns or across all columns to ensure data consistency and remove redundancy in the data for further analysis or processing.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="exceptother-datasett-datasett">except(other: Dataset<!-- -->[T]<!-- -->): Dataset<!-- -->[T]<a class="hash-link" href="#exceptother-datasett-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>except</code> function in Apache Spark&#x27;s Dataset API is used to return a new Dataset containing the rows from the current Dataset that are not present in the <code>other</code> Dataset. It computes the set difference between the two Datasets based on their rows, keeping only the unique rows from the current Dataset.</p><p><strong>Usage Example:</strong>
Suppose we have two Datasets, <code>datasetA</code> and <code>datasetB</code>, and we want to find the rows that are present in <code>datasetA</code> but not in <code>datasetB</code>.</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Except Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Datasets</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataA = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(1, &quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(2, &quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(3, &quot;John&quot;, 40)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataB = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(2, &quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(4, &quot;Kate&quot;, 35)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating Datasets from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val datasetA: Dataset[Employee] = spark.createDataset(dataA)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val datasetB: Dataset[Employee] = spark.createDataset(dataB)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Finding the rows present in datasetA but not in datasetB</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val result: Dataset[Employee] = datasetA.except(datasetB)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the result</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">result.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| id| name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  1|Alice| 30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  3| John| 40|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we create a SparkSession and import its implicits to enable Spark SQL functionalities. We define two sample Datasets, <code>datasetA</code> and <code>datasetB</code>, containing <code>Employee</code> case class objects. We use the <code>except</code> function to find the rows that are present in <code>datasetA</code> but not in <code>datasetB</code>. The result Dataset contains the rows <code>(1, &quot;Alice&quot;, 30)</code> and <code>(3, &quot;John&quot;, 40)</code> because they are unique to <code>datasetA</code> and not present in <code>datasetB</code>.</p><p>The <code>except</code> function is useful for finding the differences between two Datasets based on their rows. It can help identify unique records, identify missing data, and perform various types of set-based operations on Datasets.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="exceptallother-datasett-datasett">exceptAll(other: Dataset<!-- -->[T]<!-- -->): Dataset<!-- -->[T]<a class="hash-link" href="#exceptallother-datasett-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>exceptAll</code> function in Apache Spark&#x27;s Dataset API is used to return a new Dataset containing all the rows from the current Dataset that are not present in the <code>other</code> Dataset, including duplicates. It computes the set difference between the two Datasets based on their rows, keeping all occurrences of rows that are unique to the current Dataset.</p><p><strong>Usage Example:</strong>
Suppose we have two Datasets, <code>datasetA</code> and <code>datasetB</code>, and we want to find all the rows that are present in <code>datasetA</code> but not in <code>datasetB</code>, including duplicates.</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;ExceptAll Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Datasets</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataA = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(1, &quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(2, &quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(2, &quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(3, &quot;John&quot;, 40)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataB = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(2, &quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(4, &quot;Kate&quot;, 35)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating Datasets from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val datasetA: Dataset[Employee] = spark.createDataset(dataA)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val datasetB: Dataset[Employee] = spark.createDataset(dataB)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Finding all rows present in datasetA but not in datasetB, including duplicates</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val result: Dataset[Employee] = datasetA.exceptAll(datasetB)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the result</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">result.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| id| name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  1|Alice| 30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  2|  Bob| 25|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  3| John| 40|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  2|  Bob| 25|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we create a SparkSession and import its implicits to enable Spark SQL functionalities. We define two sample Datasets, <code>datasetA</code> and <code>datasetB</code>, containing <code>Employee</code> case class objects. We use the <code>exceptAll</code> function to find all the rows that are present in <code>datasetA</code> but not in <code>datasetB</code>, including duplicates. The result Dataset contains all occurrences of rows <code>(1, &quot;Alice&quot;, 30)</code>, <code>(2, &quot;Bob&quot;, 25)</code>, and <code>(3, &quot;John&quot;, 40)</code> from <code>datasetA</code> because they are unique to <code>datasetA</code> and not present in <code>datasetB</code>.</p><p>The <code>exceptAll</code> function is useful for finding all occurrences of rows that are unique to the current Dataset. It includes duplicate rows in the result if they exist in the original Dataset. This function is especially handy when you need to identify and retain all occurrences of distinct rows that are missing from another Dataset.</p><h4 class="anchor anchorWithStickyNavbar_DXrL" id="filterfunc-filterfunctiont-datasett">filter(func: FilterFunction<!-- -->[T]<!-- -->): Dataset<!-- -->[T]<a class="hash-link" href="#filterfunc-filterfunctiont-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>filter</code> function in Apache Spark&#x27;s Dataset API is used to create a new Dataset by selecting only the rows that satisfy a given condition specified by the <code>FilterFunction[T]</code>. The <code>FilterFunction[T]</code> is a user-defined function that takes a single argument of type <code>T</code> (the type of the Dataset&#x27;s elements) and returns a Boolean value, indicating whether the row should be included in the new Dataset or not.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Filter Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(1, &quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(2, &quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(3, &quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(4, &quot;Kate&quot;, 35)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset: Dataset[Employee] = spark.createDataset(data)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Defining the filter condition function</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val filterCondition: FilterFunction[Employee] = new FilterFunction[Employee] {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  override def call(employee: Employee): Boolean = employee.age &gt;= 30</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Applying the filter to the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val filteredDataset: Dataset[Employee] = dataset.filter(filterCondition)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the filtered Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">filteredDataset.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| id| name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  1|Alice| 30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  3| John| 40|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  4| Kate| 35|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_DXrL" id="filterfunc-t--boolean-datasett">filter(func: (T) ⇒ Boolean): Dataset<!-- -->[T]<a class="hash-link" href="#filterfunc-t--boolean-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>filter</code> function in Apache Spark&#x27;s Dataset API is used to create a new Dataset by selecting only the rows that satisfy a given condition specified by the user-defined function <code>func</code>. The function <code>func</code> takes a single argument of type <code>T</code> (the type of the Dataset&#x27;s elements) and returns a Boolean value, indicating whether the row should be included in the new Dataset or not.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Filter Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(1, &quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(2, &quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(3, &quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(4, &quot;Kate&quot;, 35)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset: Dataset[Employee] = spark.createDataset(data)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Defining the filter condition function</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def filterCondition(employee: Employee): Boolean = employee.age &gt;= 30</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Applying the filter to the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val filteredDataset: Dataset[Employee] = dataset.filter(filterCondition)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the filtered Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">filteredDataset.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| id| name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  1|Alice| 30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  3| John| 40|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  4| Kate| 35|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_DXrL" id="filterconditionexpr-string-datasett">filter(conditionExpr: String): Dataset<!-- -->[T]<a class="hash-link" href="#filterconditionexpr-string-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>filter</code> function in Apache Spark&#x27;s Dataset API is used to create a new Dataset by applying the specified SQL-like condition expression <code>conditionExpr</code>. The condition expression should be a valid SQL WHERE clause, and it is used to filter the rows in the Dataset.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Filter Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(1, &quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(2, &quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(3, &quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(4, &quot;Kate&quot;, 35)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset: Dataset[Employee] = spark.createDataset(data)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Applying the filter to the Dataset using condition expression</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val filteredDataset: Dataset[Employee] = dataset.filter(&quot;age &gt;= 30&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the filtered Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">filteredDataset.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| id| name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  1|Alice| 30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  3| John| 40|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  4| Kate| 35|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><h4 class="anchor anchorWithStickyNavbar_DXrL" id="filtercondition-column-datasett">filter(condition: Column): Dataset<!-- -->[T]<a class="hash-link" href="#filtercondition-column-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>filter</code> function in Apache Spark&#x27;s</p><p> Dataset API is used to create a new Dataset by applying the specified Column-based condition <code>condition</code>. The condition is created using DataFrame&#x27;s <code>Column</code> API, which allows for complex filtering expressions involving multiple columns.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.functions._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Filter Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(1, &quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(2, &quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(3, &quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(4, &quot;Kate&quot;, 35)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset: Dataset[Employee] = spark.createDataset(data)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Applying the filter to the Dataset using Column-based condition</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val filteredDataset: Dataset[Employee] = dataset.filter(col(&quot;age&quot;) &gt;= 30)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the filtered Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">filteredDataset.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| id| name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  1|Alice| 30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  3| John| 40|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  4| Kate| 35|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+-----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In all the above examples, we have a sample Dataset of employees (<code>dataset</code>) with columns <code>id</code>, <code>name</code>, and <code>age</code>. We use the <code>filter</code> function to create a new Dataset (<code>filteredDataset</code>) by selecting only the rows that satisfy the specified conditions.</p><ol><li>In the first example, we use a <code>FilterFunction[Employee]</code> to filter the rows where the employee&#x27;s age is greater than or equal to 30.</li><li>In the second example, we define a custom function <code>filterCondition</code> and use it with the <code>filter</code> function.</li><li>In the third example, we use a SQL-like condition expression <code>&quot;age &gt;= 30&quot;</code> to filter the rows.</li><li>In the fourth example, we use the DataFrame <code>Column</code> API to create the condition <code>col(&quot;age&quot;) &gt;= 30</code> and apply it to the Dataset.</li></ol><h4 class="anchor anchorWithStickyNavbar_DXrL" id="flatmapufunc-t--traversableonceuimplicit-arg0-encoderu-datasetu"><code>flatMap[U](func: (T) ⇒ TraversableOnce[U])(implicit arg0: Encoder[U]): Dataset[U]</code><a class="hash-link" href="#flatmapufunc-t--traversableonceuimplicit-arg0-encoderu-datasetu" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>flatMap</code> function in Apache Spark&#x27;s Dataset API is used to create a new Dataset by applying a transformation function <code>func</code> to each element of the original Dataset <code>T</code>, which returns a collection of elements of type <code>U</code>. The <code>flatMap</code> function then flattens these collections into a single Dataset of type <code>U</code>. The function <code>func</code> takes a single argument of type <code>T</code> (the type of the original Dataset&#x27;s elements) and returns a <code>TraversableOnce[U]</code>, which represents a collection of elements of type <code>U</code>.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;FlatMap Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(1, &quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(2, &quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(3, &quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(4, &quot;Kate&quot;, 35)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset: Dataset[Employee] = spark.createDataset(data)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Defining the flatMap transformation function</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def splitName(employee: Employee): TraversableOnce[String] = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  employee.name.split(&quot; &quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Applying the flatMap transformation to the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val nameDataset: Dataset[String] = dataset.flatMap(splitName)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the new Dataset with names</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">nameDataset.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+-----+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|value|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Bob|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| John|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Kate|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have a sample Dataset of employees (<code>dataset</code>) with columns <code>id</code>, <code>name</code>, and <code>age</code>. We define a custom function <code>splitName</code> that takes an <code>Employee</code> and splits the <code>name</code> into individual words using the space character. The <code>flatMap</code> function is then used to apply this transformation to each employee&#x27;s name, resulting in a new Dataset (<code>nameDataset</code>) containing the split names.</p><p><strong>Note:</strong></p><ul><li>The <code>flatMap</code> function is similar to the <code>map</code> function, but it allows returning a collection of elements (<code>TraversableOnce[U]</code>) instead of a single element. The collections are then flattened into a single Dataset.</li><li>The <code>implicit arg0: Encoder[U]</code> parameter allows Spark to automatically infer the schema of the new Dataset <code>U</code>. It is provided implicitly by Spark, and you don&#x27;t need to explicitly pass it when calling the <code>flatMap</code> function.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="groupbykeykfunc-t--kimplicit-arg0-encoderk-keyvaluegroupeddatasetk-t"><code>groupByKey[K](func: (T) ⇒ K)(implicit arg0: Encoder[K]): KeyValueGroupedDataset[K, T]</code><a class="hash-link" href="#groupbykeykfunc-t--kimplicit-arg0-encoderk-keyvaluegroupeddatasetk-t" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>groupByKey</code> function in Apache Spark&#x27;s Dataset API is used to group the elements in the Dataset based on the result of a user-defined function <code>func</code>. It returns a <code>KeyValueGroupedDataset</code> where the keys are the unique results of applying the function to each element, and the values are the elements themselves.</p><p><strong>Parameters:</strong></p><ul><li><code>func</code>: A user-defined function that takes an element of type <code>T</code> as input and returns a key of type <code>K</code>. This function is used to determine the grouping key for each element.</li><li><code>arg0</code>: An implicit parameter of type <code>Encoder[K]</code>, which provides the necessary serialization and deserialization methods for the key type <code>K</code>.</li></ul><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset elements</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Group By Key Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 35),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 28)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset: Dataset[Person] = spark.createDataset(data)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Grouping the elements by name using groupByKey</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val groupedDataset: KeyValueGroupedDataset[String, Person] = dataset.groupByKey(person =&gt; person.name)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the groups and their elements</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">groupedDataset.foreach {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  case (name, group) =&gt; println(s&quot;Group: $name&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                       group.foreach(println)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Group: Bob</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Bob,25</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Bob,28</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Group: Alice</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Alice,30</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Alice,35</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Group: John</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">John,40</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have a Dataset of <code>Person</code> objects with names and ages. We use the <code>groupByKey</code> function to group the elements based on their names. The result is a <code>KeyValueGroupedDataset</code> where the keys are the unique names (&quot;Bob&quot;, &quot;Alice&quot;, and &quot;John&quot;), and the values are the corresponding <code>Person</code> objects. The grouped Dataset is then iterated, and each group is displayed along with its elements.</p><p><strong>Note:</strong></p><ul><li>The <code>groupByKey</code> function is typically used in scenarios where you want to perform operations on groups of elements based on a common key, such as aggregations, filtering, or transformations.</li><li>After grouping, you can apply aggregation functions like <code>agg</code>, <code>count</code>, <code>sum</code>, etc. on the grouped Dataset to perform computations on each group.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="intersectother-datasett-datasett"><code>intersect(other: Dataset[T]): Dataset[T]</code><a class="hash-link" href="#intersectother-datasett-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>intersect</code> function in Apache Spark&#x27;s Dataset API is used to find the common elements between two Datasets. It returns a new Dataset that contains only the elements that are present in both the current Dataset and the <code>other</code> Dataset.</p><p><strong>Parameters:</strong></p><ul><li><code>other</code>: The other Dataset with which the intersection is to be computed. Both Datasets must have the same schema (column names and data types).</li></ul><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset elements</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Intersect Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the first Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data1 = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 35),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 28)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the second Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data2 = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Mary&quot;, 22),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 35),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Tom&quot;, 29)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Datasets from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset1: Dataset[Person] = spark.createDataset(data1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset2: Dataset[Person] = spark.createDataset(data2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Finding the common elements using intersect</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val commonElements: Dataset[Person] = dataset1.intersect(dataset2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the common elements</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">commonElements.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice| 35|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| John| 40|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Bob| 25|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have two Datasets (<code>dataset1</code> and <code>dataset2</code>) containing <code>Person</code> objects with names and ages. We use the <code>intersect</code> function to find the common elements between the two Datasets based on their contents. The result is a new Dataset <code>commonElements</code>, which contains only the <code>Person</code> objects that are present in both Datasets. The common elements are displayed using the <code>show()</code> method.</p><p><strong>Note:</strong></p><ul><li>The <code>intersect</code> function assumes that both Datasets have the same schema. If the Datasets have different schemas, you may need to perform data type conversions or transformations before using the <code>intersect</code> function.</li><li>The <code>intersect</code> function is a transformation operation, and it does not trigger any computation until an action is called on the resulting Dataset.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="intersectallother-datasett-datasett"><code>intersectAll(other: Dataset[T]): Dataset[T]</code><a class="hash-link" href="#intersectallother-datasett-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>intersectAll</code> function in Apache Spark&#x27;s Dataset API is used to find the common elements between two Datasets, including duplicate occurrences. It returns a new Dataset that contains all the elements that are present in both the current Dataset and the <code>other</code> Dataset, considering duplicates.</p><p><strong>Parameters:</strong></p><ul><li><code>other</code>: The other Dataset with which the intersection is to be computed. Both Datasets must have the same schema (column names and data types).</li></ul><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset elements</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;IntersectAll Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the first Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data1 = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 35),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 28)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the second Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data2 = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Mary&quot;, 22),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 35),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Tom&quot;, 29)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Datasets from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset1: Dataset[Person] = spark.createDataset(data1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset2: Dataset[Person] = spark.createDataset(data2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Finding the common elements (including duplicates) using intersectAll</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val commonElements: Dataset[Person] = dataset1.intersectAll(dataset2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the common elements (including duplicates)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">commonElements.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice| 35|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice| 35|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| John| 40|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Bob| 25|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have two Datasets (<code>dataset1</code> and <code>dataset2</code>) containing <code>Person</code> objects with names and ages. We use the <code>intersectAll</code> function to find the common elements between the two Datasets, including duplicates based on their contents. The result is a new Dataset <code>commonElements</code>, which contains all the <code>Person</code> objects that are present in both Datasets, considering duplicates. The common elements, along with their duplicates, are displayed using the <code>show()</code> method.</p><p><strong>Note:</strong></p><ul><li>The <code>intersectAll</code> function assumes that both Datasets have the same schema. If the Datasets have different schemas, you may need to perform data type conversions or transformations before using the <code>intersectAll</code> function.</li><li>The <code>intersectAll</code> function is a transformation operation, and it does not trigger any computation until an action is called on the resulting Dataset.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="joinwithuother-datasetu-condition-column-datasett-u"><code>joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)]</code><a class="hash-link" href="#joinwithuother-datasetu-condition-column-datasett-u" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>joinWith</code> function in Apache Spark&#x27;s Dataset API is used to perform an inner join between two Datasets based on a given condition (Column). It returns a new Dataset containing tuples of elements from the current Dataset (<code>this</code>) and the <code>other</code> Dataset that satisfy the specified join condition.</p><p><strong>Parameters:</strong></p><ul><li><code>other</code>: The other Dataset (<code>Dataset[U]</code>) with which the inner join is performed.</li><li><code>condition</code>: The join condition expressed as a Column. It specifies the criteria for joining the two Datasets. The join condition must evaluate to a Boolean expression, and the resulting Dataset will contain only the elements that satisfy this condition.</li></ul><p><strong>Return Type:</strong>
The function returns a new Dataset of tuples <code>(T, U)</code>, where <code>T</code> is the element type of the current Dataset (<code>this</code>), and <code>U</code> is the element type of the <code>other</code> Dataset.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.functions._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Datasets</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Address(name: String, city: String)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;joinWith Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the first Dataset (Person)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the second Dataset (Address)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val addressData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Address(&quot;Alice&quot;, &quot;New York&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Address(&quot;Bob&quot;, &quot;San Francisco&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Address(&quot;Mary&quot;, &quot;Los Angeles&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Datasets from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personDataset: Dataset[Person] = spark.createDataset(personData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val addressDataset: Dataset[Address] = spark.createDataset(addressData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Joining the Datasets using joinWith and a condition based on &#x27;name&#x27; column</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val joinedDataset: Dataset[(Person, Address)] = personDataset.joinWith(addressDataset,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  personDataset(&quot;name&quot;) === addressDataset(&quot;name&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the joined results</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">joinedDataset.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+--------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|        _1|              _2|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+--------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|{Alice, 30}| {Alice, New York}|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  {Bob, 25}|{Bob, San Francisco}|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+--------------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have two Datasets (<code>personDataset</code> and <code>addressDataset</code>) containing <code>Person</code> and <code>Address</code> objects, respectively. We use the <code>joinWith</code> function to perform an inner join between the Datasets based on the condition that the &#x27;name&#x27; column in <code>personDataset</code> should be equal to the &#x27;name&#x27; column in <code>addressDataset</code>. The resulting Dataset <code>joinedDataset</code> contains tuples of <code>(Person, Address)</code> representing the matched elements from both Datasets, where the &#x27;name&#x27; columns match.</p><p><strong>Note:</strong></p><ul><li>The <code>joinWith</code> function is useful when you want to perform an inner join and obtain a Dataset of tuples containing matched elements from both Datasets.</li><li>The condition specified in the <code>joinWith</code> function can involve multiple columns or more complex expressions using Spark&#x27;s built-in functions from <code>org.apache.spark.sql.functions</code>.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="joinwithuother-datasetu-condition-column-jointype-string-datasett-u"><code>joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)]</code><a class="hash-link" href="#joinwithuother-datasetu-condition-column-jointype-string-datasett-u" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>joinWith</code> function in Apache Spark&#x27;s Dataset API is used to perform a join between two Datasets based on a given condition (Column) and a specified join type. It returns a new Dataset containing tuples of elements from the current Dataset (<code>this</code>) and the <code>other</code> Dataset that satisfy the specified join condition.</p><p><strong>Parameters:</strong></p><ul><li><code>other</code>: The other Dataset (<code>Dataset[U]</code>) with which the join is performed.</li><li><code>condition</code>: The join condition expressed as a Column. It specifies the criteria for joining the two Datasets. The join condition must evaluate to a Boolean expression, and the resulting Dataset will contain only the elements that satisfy this condition.</li><li><code>joinType</code>: A string representing the type of join to be performed. The possible values for <code>joinType</code> are:<ul><li><code>&quot;inner&quot;</code>: Performs an inner join between the two Datasets. Only the matching elements are included in the resulting Dataset.</li><li><code>&quot;outer&quot;</code>: Performs a full outer join between the two Datasets. All elements from both Datasets are included in the resulting Dataset. If there is no match, null values will be present for the non-matching elements.</li><li><code>&quot;left_outer&quot;</code>: Performs a left outer join. All elements from the left Dataset are included in the resulting Dataset. If there is no match in the right Dataset, null values will be present for the non-matching elements.</li><li><code>&quot;right_outer&quot;</code>: Performs a right outer join. All elements from the right Dataset are included in the resulting Dataset. If there is no match in the left Dataset, null values will be present for the non-matching elements.</li></ul></li></ul><p><strong>Return Type:</strong>
The function returns a new Dataset of tuples <code>(T, U)</code>, where <code>T</code> is the element type of the current Dataset (<code>this</code>), and <code>U</code> is the element type of the <code>other</code> Dataset.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.functions._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Datasets</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Address(name: String, city: String)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;joinWith Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the first Dataset (Person)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the second Dataset (Address)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val addressData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Address(&quot;Alice&quot;, &quot;New York&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Address(&quot;Bob&quot;, &quot;San Francisco&quot;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Address(&quot;Mary&quot;, &quot;Los Angeles&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Datasets from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personDataset: Dataset[Person] = spark.createDataset(personData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val addressDataset: Dataset[Address] = spark.createDataset(addressData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Joining the Datasets using joinWith with a left outer join type and a condition based on &#x27;name&#x27; column</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val joinedDataset: Dataset[(Person, Address)] = personDataset.joinWith(addressDataset,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  personDataset(&quot;name&quot;) === addressDataset(&quot;name&quot;), &quot;left_outer&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the joined results</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">joinedDataset.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+--------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|        _1|              _2|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+--------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|{Alice, 30}| {Alice, New York}|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  {Bob, 25}|{Bob, San Francisco}|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| {John, 40}|            null|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+--------------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have two Datasets (<code>personDataset</code> and <code>addressDataset</code>) containing <code>Person</code> and <code>Address</code> objects, respectively. We use the <code>joinWith</code> function to perform a left outer join between the Datasets based on the condition that the &#x27;name&#x27; column in <code>personDataset</code> should be equal to the &#x27;name&#x27; column in <code>addressDataset</code>. The resulting Dataset <code>joinedDataset</code> contains tuples of <code>(Person, Address)</code> representing the matched elements from both Datasets for the specified join condition. Since there is no matching entry for &#x27;John&#x27; in the <code>addressDataset</code>, the &#x27;city&#x27; attribute for &#x27;John&#x27; is represented as null in the output.</p><p><strong>Note:</strong></p><ul><li>The <code>joinWith</code> function is useful when you want to perform a join and obtain a Dataset of tuples containing matched elements from both Datasets based on a specific join condition and type.</li><li>The condition specified in the <code>joinWith</code> function can involve multiple columns or more complex expressions using Spark&#x27;s built-in functions from <code>org.apache.spark.sql.functions</code>.</li><li>The <code>joinType</code> parameter determines the type of join to be performed (inner, outer, left_outer, or right_outer).</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="limitn-int-datasett"><code>limit(n: Int): Dataset[T]</code><a class="hash-link" href="#limitn-int-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>limit</code> function in Apache Spark&#x27;s Dataset API is used to select a fixed number of elements from the beginning of the Dataset. It returns a new Dataset containing the first <code>n</code> elements of the current Dataset (<code>this</code>).</p><p><strong>Parameters:</strong></p><ul><li><code>n</code>: The number of elements to select from the beginning of the Dataset. It should be a positive integer.</li></ul><p><strong>Return Type:</strong>
The function returns a new Dataset containing the first <code>n</code> elements of type <code>T</code>, where <code>T</code> is the element type of the current Dataset (<code>this</code>).</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;limit Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Mary&quot;, 35)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personDataset: Dataset[Person] = spark.createDataset(personData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Applying the limit function to select the first two elements</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val limitedDataset: Dataset[Person] = personDataset.limit(2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the limited results</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">limitedDataset.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+------+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+------+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Alice| 30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|   Bob| 25|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+------+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have a Dataset <code>personDataset</code> containing <code>Person</code> objects. We use the <code>limit</code> function to select the first two elements from the Dataset. The resulting Dataset <code>limitedDataset</code> contains the first two elements of <code>personDataset</code>.</p><p><strong>Note:</strong></p><ul><li>The <code>limit</code> function is commonly used to restrict the number of records to be processed, especially when dealing with large Datasets.</li><li>The number <code>n</code> specified in the <code>limit</code> function should be positive and should not exceed the total number of elements in the Dataset. If <code>n</code> is greater than the Dataset&#x27;s size, the function will return the entire Dataset without any change.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="limitn-int-datasett-1"><code>limit(n: Int): Dataset[T]</code><a class="hash-link" href="#limitn-int-datasett-1" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>limit</code> function in Apache Spark&#x27;s Dataset API is used to select a fixed number of elements from the beginning of the Dataset. It returns a new Dataset containing the first <code>n</code> elements of the current Dataset (<code>this</code>).</p><p><strong>Parameters:</strong></p><ul><li><code>n</code>: The number of elements to select from the beginning of the Dataset. It should be a positive integer.</li></ul><p><strong>Return Type:</strong>
The function returns a new Dataset containing the first <code>n</code> elements of type <code>T</code>, where <code>T</code> is the element type of the current Dataset (<code>this</code>).</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;limit Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Mary&quot;, 35)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personDataset: Dataset[Person] = spark.createDataset(personData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Applying the limit function to select the first two elements</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val limitedDataset: Dataset[Person] = personDataset.limit(2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the limited results</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">limitedDataset.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+------+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+------+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Alice| 30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|   Bob| 25|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+------+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have a Dataset <code>personDataset</code> containing <code>Person</code> objects. We use the <code>limit</code> function to select the first two elements from the Dataset. The resulting Dataset <code>limitedDataset</code> contains the first two elements of <code>personDataset</code>.</p><p><strong>Note:</strong></p><ul><li>The <code>limit</code> function is commonly used to restrict the number of records to be processed, especially when dealing with large Datasets.</li><li>The number <code>n</code> specified in the <code>limit</code> function should be positive and should not exceed the total number of elements in the Dataset. If <code>n</code> is greater than the Dataset&#x27;s size, the function will return the entire Dataset without any change.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="mappartitionsufunc-iteratort--iteratoruimplicit-arg0-encoderu-datasetu"><code>mapPartitions[U](func: (Iterator[T]) ⇒ Iterator[U])(implicit arg0: Encoder[U]): Dataset[U]</code><a class="hash-link" href="#mappartitionsufunc-iteratort--iteratoruimplicit-arg0-encoderu-datasetu" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>mapPartitions</code> function in Apache Spark&#x27;s Dataset API is used to apply a transformation to each partition of the Dataset <code>this</code>. It takes a mapping function that operates on an iterator of elements of type <code>T</code>, and the function returns an iterator of elements of type <code>U</code>. The transformation is performed on each partition, and the resulting elements from all partitions are combined to create a new Dataset of type <code>U</code>.</p><p><strong>Parameters:</strong></p><ul><li><code>func</code>: The mapping function that takes an iterator of elements of type <code>T</code> from a partition and returns an iterator of elements of type <code>U</code>. This function is applied to each partition of the Dataset.</li><li><code>arg0</code>: An implicit <code>Encoder</code> for type <code>U</code>, which is required to serialize the transformed elements of type <code>U</code> back to Spark&#x27;s internal binary format.</li></ul><p><strong>Return Type:</strong>
The function returns a new Dataset of type <code>U</code>, where <code>U</code> is the type of elements returned by the mapping function <code>func</code>.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;mapPartitions Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Mary&quot;, 35),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;David&quot;, 28),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Eva&quot;, 33)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personDataset: Dataset[Person] = spark.createDataset(personData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Applying the mapPartitions function to transform each age to its square</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val ageSquaredDataset: Dataset[Int] = personDataset.mapPartitions { partitionIterator =&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  partitionIterator.map(person =&gt; person.age * person.age)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Displaying the transformed Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ageSquaredDataset.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+-----+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|value|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  900|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  625|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| 1600|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| 1225|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  784|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| 1089|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have a Dataset <code>personDataset</code> containing <code>Person</code> objects. We use the <code>mapPartitions</code> function to transform each person&#x27;s age (of type <code>Int</code>) into its square (of type <code>Int</code>). The transformation is performed on each partition, and the resulting Dataset <code>ageSquaredDataset</code> contains the squares of ages.</p><p><strong>Note:</strong></p><ul><li>The <code>mapPartitions</code> function is useful when you need to apply a transformation that requires processing a whole partition at once. It can be more efficient than <code>map</code> when the transformation is costly and has a significant setup or teardown overhead.</li><li>The transformation function <code>func</code> takes an iterator for efficiency and should be designed to work on partitions of data, not on single elements.</li><li>The implicit <code>Encoder[U]</code> is required to serialize the transformed elements of type <code>U</code> back to Spark&#x27;s internal binary format. The SparkSession automatically provides suitable encoders for standard Scala types, so explicit encoders are often not necessary for basic transformations. However, in more complex scenarios or when working with custom types, explicit encoders may be required.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="observeobservation-observation-expr-column-exprs-column-datasett"><code>observe(observation: Observation, expr: Column, exprs: Column*): Dataset[T]</code><a class="hash-link" href="#observeobservation-observation-expr-column-exprs-column-datasett" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_DXrL" id="observename-string-expr-column-exprs-column-datasett"><code>observe(name: String, expr: Column, exprs: Column*): Dataset[T]</code><a class="hash-link" href="#observename-string-expr-column-exprs-column-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>observe</code> function in Apache Spark&#x27;s Dataset API is used to observe the internal state of a streaming query while it is running. It allows users to register observations on the Dataset during the query execution. Observations are used for debugging, monitoring, or any other custom operations that need to be performed during the execution of the streaming query.</p><p><strong>Parameters:</strong></p><ul><li><code>observation</code>: An instance of the <code>Observation</code> class, which represents the specific observation to be made during the query execution. The <code>Observation</code> class provides methods to register various types of observations, such as logging messages, writing data to external storage, or triggering user-defined actions.</li><li><code>name</code>: A string representing a name for the observation. This is used to identify the observation in the query output or logs.</li><li><code>expr</code>: A <code>Column</code> object representing the expression to be evaluated during the observation. This can be any valid Spark SQL expression that operates on the columns of the Dataset.</li><li><code>exprs</code>: Additional <code>Column</code> objects (varargs) representing more expressions to be evaluated during the observation.</li></ul><p><strong>Return Type:</strong>
The function returns a new Dataset of type <code>T</code>, which is the same as the original Dataset the observation is registered on. The observation does not affect the original Dataset but allows additional actions to be taken during its execution.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.streaming.{OutputMode, Observation}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;observe Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Mary&quot;, 35),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;David&quot;, 28),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Eva&quot;, 33)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personDataset: Dataset[Person] = spark.createDataset(personData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Registering an observation to log the age of each person</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val ageObservation = Observation.log(&quot;Age Observation:&quot;).withColumn(&quot;name&quot;, $&quot;name&quot;).withColumn(&quot;age&quot;, $&quot;age&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Applying the observe function to log the age of each person during the execution of the query</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val observedDataset: Dataset[Person] = personDataset.observe(ageObservation, $&quot;name&quot;, $&quot;age&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Defining a streaming query to write the observed data to the console</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val query = observedDataset.writeStream</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .outputMode(OutputMode.Append())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .format(&quot;console&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .start()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Waiting for the query to terminate</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">query.awaitTermination()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">-------------------------------------------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Batch: 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">-------------------------------------------</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice| 30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Bob| 25|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| John| 40|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Mary| 35|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|David| 28|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Eva| 33|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have a Dataset <code>personDataset</code> containing <code>Person</code> objects. We register an observation using <code>Observation.log</code> to log the age of each person during the execution of the query. The <code>observe</code> function is then used to apply this observation to the Dataset. The <code>observe</code> function does not modify the original Dataset but instead returns a new Dataset that includes the observation. Finally, we define a streaming query to write the observed data to the console in append mode. The streaming query is started, and the observed data is printed to the console.</p><p><strong>Note:</strong></p><ul><li>The <code>observe</code> function is used with streaming queries to observe the data as it flows through the query execution. It is particularly useful for debugging and monitoring streaming applications.</li><li>The <code>Observation</code> class provides various methods to register different types of observations, such as logging data, writing to external storage, or triggering user-defined actions.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="offsetn-int-datasett"><code>offset(n: Int): Dataset[T]</code><a class="hash-link" href="#offsetn-int-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>offset</code> function in Apache Spark&#x27;s Dataset API is used to skip a specified number of rows from the beginning of the Dataset. It returns a new Dataset that excludes the first <code>n</code> rows.</p><p><strong>Parameters:</strong></p><ul><li><code>n</code>: An integer representing the number of rows to skip from the beginning of the Dataset.</li></ul><p><strong>Return Type:</strong>
The function returns a new Dataset of type <code>T</code> that contains the remaining rows after skipping the first <code>n</code> rows.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;offset Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Mary&quot;, 35),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;David&quot;, 28),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Eva&quot;, 33)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personDataset: Dataset[Person] = spark.createDataset(personData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Applying the offset function to skip the first 2 rows</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val offsetDataset: Dataset[Person] = personDataset.offset(2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Showing the contents of the offsetDataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">offsetDataset.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|John| 40|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Mary| 35|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|David| 28|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Eva| 33|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have a Dataset <code>personDataset</code> containing <code>Person</code> objects. We apply the <code>offset</code> function with <code>n = 2</code> to skip the first two rows of the Dataset. The <code>offset</code> function creates a new Dataset <code>offsetDataset</code> that contains the remaining rows after skipping the first two rows. The <code>show</code> method is then used to display the contents of the <code>offsetDataset</code>.</p><p><strong>Note:</strong></p><ul><li>The <code>offset</code> function is particularly useful for scenarios where you want to skip some initial rows in the Dataset before performing further operations on the data. It is often used for pagination or to remove header rows from the data.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="orderbysortexprs-column-datasett"><code>orderBy(sortExprs: Column*): Dataset[T]</code><a class="hash-link" href="#orderbysortexprs-column-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>orderBy</code> function in Apache Spark&#x27;s Dataset API is used to sort the rows of the Dataset based on one or more columns in ascending or descending order. It returns a new Dataset with the rows sorted according to the specified sorting expressions.</p><p><strong>Parameters:</strong></p><ul><li><code>sortExprs</code>: One or more <code>Column</code> objects representing the sorting expressions. Each <code>Column</code> specifies a column by which the Dataset should be sorted. By default, the sorting is done in ascending order. To sort in descending order, use the <code>desc</code> function on the <code>Column</code>.</li></ul><p><strong>Return Type:</strong>
The function returns a new Dataset of type <code>T</code> with the rows sorted based on the specified sorting expressions.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.functions._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;orderBy Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Mary&quot;, 35),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;David&quot;, 28),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Eva&quot;, 33)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personDataset: Dataset[Person] = spark.createDataset(personData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sorting the Dataset by age in ascending order</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val sortedByAge: Dataset[Person] = personDataset.orderBy(col(&quot;age&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sorting the Dataset by name in descending order</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val sortedByNameDesc: Dataset[Person] = personDataset.orderBy(col(&quot;name&quot;).desc)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Showing the contents of the sortedByAge Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sortedByAge.show()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Showing the contents of the sortedByNameDesc Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sortedByNameDesc.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Bob| 25|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|David| 28|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice| 30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Eva| 33|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Mary| 35|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|John| 40|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Eva| 33|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|David| 28|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Mary | 35|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|John | 40|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice| 30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Bob | 25|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have a Dataset <code>personDataset</code> containing <code>Person</code> objects. We use the <code>orderBy</code> function to create two new Datasets <code>sortedByAge</code> and <code>sortedByNameDesc</code>. <code>sortedByAge</code> is sorted based on the <code>age</code> column in ascending order, while <code>sortedByNameDesc</code> is sorted based on the <code>name</code> column in descending order. The <code>show</code> method is then used to display the contents of both sorted Datasets.</p><p><strong>Note:</strong></p><ul><li>The <code>orderBy</code> function is commonly used to sort the data in a Dataset based on one or more columns, which is often a required operation in data processing and analysis.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="orderbysortcol-string-sortcols-string-datasett"><code>orderBy(sortCol: String, sortCols: String*): Dataset[T]</code><a class="hash-link" href="#orderbysortcol-string-sortcols-string-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>orderBy</code> function in Apache Spark&#x27;s Dataset API is used to sort the rows of the Dataset based on one or more columns in ascending or descending order specified by their column names. It returns a new Dataset with the rows sorted according to the specified sorting columns.</p><p><strong>Parameters:</strong></p><ul><li><code>sortCol</code>: The name of the column by which the Dataset should be sorted in ascending order.</li><li><code>sortCols</code>: Additional column names to specify the order of sorting. These columns are used to break ties when the primary column (<code>sortCol</code>) has equal values. By default, all columns are sorted in ascending order. To sort in descending order, use the <code>desc</code> function on the <code>Column</code> corresponding to the column name.</li></ul><p><strong>Return Type:</strong>
The function returns a new Dataset of type <code>T</code> with the rows sorted based on the specified sorting columns.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;orderBy Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30, 50000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25, 45000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40, 55000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Mary&quot;, 35, 52000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;David&quot;, 28, 48000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Eva&quot;, 33, 53000.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personDataset: Dataset[Person] = spark.createDataset(personData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sorting the Dataset by age in ascending order</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val sortedByAge: Dataset[Person] = personDataset.orderBy(&quot;age&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sorting the Dataset by age in ascending order and salary in descending order</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val sortedByAgeAndSalary: Dataset[Person] = personDataset.orderBy(&quot;age&quot;, &quot;salary&quot;.desc)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Showing the contents of the sortedByAge Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sortedByAge.show()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Showing the contents of the sortedByAgeAndSalary Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sortedByAgeAndSalary.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|name|age|salary|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Bob| 25|45000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|David| 28|48000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice| 30|50000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Eva| 33|53000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Mary| 35|52000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|John| 40|55000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|name|age|salary|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Bob| 25|45000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|David| 28|48000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice| 30|50000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Eva| 33|53000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Mary| 35|52000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|John| 40|55000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have a Dataset <code>personDataset</code> containing <code>Person</code> objects. We use the <code>orderBy</code> function to create two new Datasets <code>sortedByAge</code> and <code>sortedByAgeAndSalary</code>. <code>sortedByAge</code> is sorted based on the <code>age</code> column in ascending order. <code>sortedByAgeAndSalary</code> is sorted first based on the <code>age</code> column in ascending order and then based on the <code>salary</code> column in descending order. The <code>show</code> method is then used to display the contents of both sorted Datasets.</p><p><strong>Note:</strong></p><ul><li>The <code>orderBy</code> function is commonly used to sort the data in a Dataset based on one or more columns, which is often a required operation in data processing and analysis. The function provides flexibility to specify multiple sorting columns, which is useful when you need to establish a specific order for your data.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="randomsplitweights-arraydouble-arraydatasett"><code>randomSplit(weights: Array[Double]): Array[Dataset[T]]</code><a class="hash-link" href="#randomsplitweights-arraydouble-arraydatasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>randomSplit</code> function in Apache Spark&#x27;s Dataset API is used to split a Dataset randomly into multiple datasets based on the provided weights. It takes an array of weights as input, and the length of the array determines the number of splits. The weights represent the probability of each split in the output array. The sum of the weights should be 1.0.</p><p><strong>Parameters:</strong></p><ul><li><code>weights</code>: An array of doubles representing the probabilities of each split in the output array. The length of the array determines the number of splits.</li></ul><p><strong>Return Type:</strong>
The function returns an array of Datasets of type <code>T</code>, which are the randomly split subsets of the original Dataset.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;randomSplit Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Mary&quot;, 35),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;David&quot;, 28),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Eva&quot;, 33)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personDataset: Dataset[Person] = spark.createDataset(personData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Splitting the Dataset into two parts: 70% and 30%</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val weights = Array(0.7, 0.3)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val randomSplits: Array[Dataset[Person]] = personDataset.randomSplit(weights)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Showing the contents of the two splits</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">randomSplits(0).show() // 70% split</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">randomSplits(1).show() // 30% split</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice| 30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Bob| 25|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| John| 40|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Mary| 35|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| David| 28|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Eva| 33|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have a Dataset <code>personDataset</code> containing <code>Person</code> objects. We use the <code>randomSplit</code> function to split the Dataset into two parts with weights 0.7 and 0.3, respectively. This means that approximately 70% of the data will be in the first split and 30% in the second split. We then show the contents of both splits using the <code>show</code> method.</p><p><strong>Note:</strong></p><ul><li>The <code>randomSplit</code> function is useful when you want to divide your data randomly into subsets for tasks such as train-test splitting in machine learning or data sampling for exploratory analysis. The function allows you to specify the proportions of data in each subset using the <code>weights</code> parameter, providing flexibility in creating random splits based on your requirements.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="randomsplitweights-arraydouble-seed-long-arraydatasett"><code>randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]]</code><a class="hash-link" href="#randomsplitweights-arraydouble-seed-long-arraydatasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>randomSplit</code> function in Apache Spark&#x27;s Dataset API is used to split a Dataset randomly into multiple datasets based on the provided weights and a seed value. It takes an array of weights and a seed value as input. The length of the weights array determines the number of splits, and the weights represent the probability of each split in the output array. The sum of the weights should be 1.0.</p><p>The <code>seed</code> parameter is used to specify a seed for the random number generator. When a seed is provided, the random splits will be reproducible, i.e., if you run the same <code>randomSplit</code> operation with the same seed multiple times, you will get the same random splits.</p><p><strong>Parameters:</strong></p><ul><li><code>weights</code>: An array of doubles representing the probabilities of each split in the output array. The length of the array determines the number of splits.</li><li><code>seed</code>: A long integer value that sets the seed for the random number generator. Optional parameter; if not provided, the default random seed is used.</li></ul><p><strong>Return Type:</strong>
The function returns an array of Datasets of type <code>T</code>, which are the randomly split subsets of the original Dataset.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;randomSplit Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Mary&quot;, 35),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;David&quot;, 28),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Eva&quot;, 33)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personDataset: Dataset[Person] = spark.createDataset(personData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Splitting the Dataset into two parts: 70% and 30% with a seed value of 123</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val weights = Array(0.7, 0.3)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val seed = 123L</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val randomSplits: Array[Dataset[Person]] = personDataset.randomSplit(weights, seed)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Showing the contents of the two splits</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">randomSplits(0).show() // 70% split</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">randomSplits(1).show() // 30% split</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice| 30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Bob| 25|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| John| 40|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Mary| 35|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|David| 28|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Eva| 33|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have a Dataset <code>personDataset</code> containing <code>Person</code> objects. We use the <code>randomSplit</code> function to split the Dataset into two parts with weights 0.7 and 0.3, respectively, and a seed value of 123. This means that approximately 70% of the data will be in the first split and 30% in the second split. The seed value ensures that the random splits will be reproducible. We then show the contents of both splits using the <code>show</code> method.</p><p><strong>Note:</strong></p><ul><li>The <code>randomSplit</code> function is useful when you want to divide your data randomly into subsets for tasks such as train-test splitting in machine learning or data sampling for exploratory analysis. The function allows you to specify the proportions of data in each subset using the <code>weights</code> parameter, providing flexibility in creating random splits based on your requirements. The optional <code>seed</code> parameter ensures reproducibility of the random splits.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="randomsplitaslistweights-arraydouble-seed-long-listdatasett"><code>randomSplitAsList(weights: Array[Double], seed: Long): List[Dataset[T]]</code><a class="hash-link" href="#randomsplitaslistweights-arraydouble-seed-long-listdatasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>randomSplitAsList</code> function in Apache Spark&#x27;s Dataset API is used to split a Dataset randomly into multiple datasets based on the provided weights and a seed value. It is similar to the <code>randomSplit</code> function, but instead of returning an array of Datasets, it returns a List of Datasets. The length of the weights array determines the number of splits, and the weights represent the probability of each split in the output List. The sum of the weights should be 1.0.</p><p>The <code>seed</code> parameter is used to specify a seed for the random number generator. When a seed is provided, the random splits will be reproducible, i.e., if you run the same <code>randomSplitAsList</code> operation with the same seed multiple times, you will get the same random splits.</p><p><strong>Parameters:</strong></p><ul><li><code>weights</code>: An array of doubles representing the probabilities of each split in the output List. The length of the array determines the number of splits.</li><li><code>seed</code>: A long integer value that sets the seed for the random number generator. Optional parameter; if not provided, the default random seed is used.</li></ul><p><strong>Return Type:</strong>
The function returns a List of Datasets of type <code>T</code>, which are the randomly split subsets of the original Dataset.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;randomSplitAsList Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Mary&quot;, 35),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;David&quot;, 28),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Eva&quot;, 33)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personDataset: Dataset[Person] = spark.createDataset(personData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Splitting the Dataset into two parts: 70% and 30% with a seed value of 123</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val weights = Array(0.7, 0.3)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val seed = 123L</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val randomSplits: List[Dataset[Person]] = personDataset.randomSplitAsList(weights, seed)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Showing the contents of the two splits</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">randomSplits(0).show() // 70% split</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">randomSplits(1).show() // 30% split</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Alice| 30|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Bob| 25|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| John| 40|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Mary| 35|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| name|age|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|David| 28|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Eva| 33|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-----+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have a Dataset <code>personDataset</code> containing <code>Person</code> objects. We use the <code>randomSplitAsList</code> function to split the Dataset into two parts with weights 0.7 and 0.3, respectively, and a seed value of 123. This means that approximately 70% of the data will be in the first split and 30% in the second split. The seed value ensures that the random splits will be reproducible. We then show the contents of both splits using the <code>show</code> method.</p><p><strong>Note:</strong></p><ul><li>The <code>randomSplitAsList</code> function is similar to <code>randomSplit</code> and serves the same purpose of randomly dividing the data into subsets. The choice between the two functions depends on whether you prefer the output as an array or a list of Datasets. Both functions offer the same flexibility in creating random splits based on the provided weights and seed value.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="repartitionpartitionexprs-column-datasett"><code>repartition(partitionExprs: Column*): Dataset[T]</code><a class="hash-link" href="#repartitionpartitionexprs-column-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>repartition</code> function in Apache Spark&#x27;s Dataset API is used to redistribute the data across a specified number of partitions based on the provided partition expressions. It allows you to control the physical layout of the data, and it is particularly useful when you want to optimize data distribution for more efficient processing, such as reducing data skew and improving parallelism during transformations.</p><p><strong>Parameters:</strong></p><ul><li><code>partitionExprs</code>: One or more Column expressions representing the partitioning criteria. Each expression specifies the column or columns based on which the data should be partitioned. The data will be distributed across partitions according to the distinct values of the specified columns.</li></ul><p><strong>Return Type:</strong>
The function returns a new Dataset of type <code>T</code> that has been repartitioned according to the provided partition expressions.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.functions._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;repartition Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Mary&quot;, 35),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;David&quot;, 28),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Eva&quot;, 33)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personDataset: Dataset[Person] = spark.createDataset(personData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Repartition the Dataset based on the &quot;age&quot; column</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val repartitionedDataset: Dataset[Person] = personDataset.repartition(col(&quot;age&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Get the number of partitions in the new Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val numPartitions: Int = repartitionedDataset.rdd.getNumPartitions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(s&quot;Number of partitions after repartitioning: $numPartitions&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Number of partitions after repartitioning: 200</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have a Dataset <code>personDataset</code> containing <code>Person</code> objects. We use the <code>repartition</code> function to redistribute the data based on the &quot;age&quot; column. The Dataset is now repartitioned into 200 partitions, and the data is distributed across these partitions based on the distinct ages of the people. The number of partitions in the repartitioned Dataset is determined by Spark&#x27;s default number of partitions, which is 200 in this case.</p><p><strong>Note:</strong></p><ul><li>The <code>repartition</code> function allows you to control the data distribution across partitions, which can significantly impact the performance of Spark transformations and actions. It is essential to choose appropriate partition expressions based on the characteristics of your data and the operations you plan to perform on the Dataset.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="repartitionnumpartitions-int-partitionexprs-column-datasett"><code>repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T]</code><a class="hash-link" href="#repartitionnumpartitions-int-partitionexprs-column-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>repartition</code> function in Apache Spark&#x27;s Dataset API is used to redistribute the data across a specified number of partitions based on the provided partition expressions. It allows you to control both the physical layout of the data and the number of partitions in the resulting Dataset. This function is particularly useful when you want to optimize data distribution for more efficient processing, such as reducing data skew and improving parallelism during transformations.</p><p><strong>Parameters:</strong></p><ul><li><code>numPartitions</code>: The desired number of partitions that the Dataset should have after repartitioning. The data will be evenly distributed across the specified number of partitions.</li><li><code>partitionExprs</code>: One or more Column expressions representing the partitioning criteria. Each expression specifies the column or columns based on which the data should be partitioned. The data will be distributed across partitions according to the distinct values of the specified columns.</li></ul><p><strong>Return Type:</strong>
The function returns a new Dataset of type <code>T</code> that has been repartitioned into the specified number of partitions according to the provided partition expressions.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.functions._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;repartition Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 30),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 25),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;John&quot;, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Mary&quot;, 35),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;David&quot;, 28),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Eva&quot;, 33)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val personDataset: Dataset[Person] = spark.createDataset(personData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Repartition the Dataset into 2 partitions based on the &quot;age&quot; column</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val repartitionedDataset: Dataset[Person] = personDataset.repartition(2, col(&quot;age&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Get the number of partitions in the new Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val numPartitions: Int = repartitionedDataset.rdd.getNumPartitions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(s&quot;Number of partitions after repartitioning: $numPartitions&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Number of partitions after repartitioning: 2</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have a Dataset <code>personDataset</code> containing <code>Person</code> objects. We use the <code>repartition</code> function to redistribute the data into 2 partitions based on the &quot;age&quot; column. The data is evenly distributed across the 2 partitions based on the distinct ages of the people. The resulting <code>repartitionedDataset</code> has only 2 partitions, which we specified as the <code>numPartitions</code> parameter.</p><p><strong>Note:</strong></p><ul><li>The <code>repartition</code> function allows you to control both the number of partitions and the data distribution across those partitions, which can significantly impact the performance of Spark transformations and actions. It is essential to choose appropriate partition expressions and an optimal number of partitions based on the characteristics of your data and the operations you plan to perform on the Dataset.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="repartitionnumpartitions-int-datasett"><code>repartition(numPartitions: Int): Dataset[T]</code><a class="hash-link" href="#repartitionnumpartitions-int-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>repartition</code> function in Apache Spark&#x27;s Dataset API is used to increase or decrease the number of partitions of a Dataset to the specified value <code>numPartitions</code>. Repartitioning is the process of redistributing the data across the specified number of partitions, which can be useful for optimizing data distribution and parallelism during transformations and actions.</p><p><strong>Parameters:</strong></p><ul><li><code>numPartitions</code>: The desired number of partitions that the Dataset should have after repartitioning. The data will be evenly distributed across the specified number of partitions.</li></ul><p><strong>Return Type:</strong>
The function returns a new Dataset of type <code>T</code> that has been repartitioned into the specified number of partitions.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, department: String, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;repartition Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Alice&quot;, &quot;HR&quot;, 50000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Bob&quot;, &quot;Engineering&quot;, 60000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, &quot;HR&quot;, 55000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, &quot;Sales&quot;, 52000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;David&quot;, &quot;Engineering&quot;, 62000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Eva&quot;, &quot;Sales&quot;, 53000.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset: Dataset[Employee] = spark.createDataset(employeeData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Repartition the Dataset into 3 partitions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val repartitionedDataset: Dataset[Employee] = employeeDataset.repartition(3)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Get the number of partitions in the new Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val numPartitions: Int = repartitionedDataset.rdd.getNumPartitions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(s&quot;Number of partitions after repartitioning: $numPartitions&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Number of partitions after repartitioning: 3</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have a Dataset <code>employeeDataset</code> containing <code>Employee</code> objects. We use the <code>repartition</code> function to redistribute the data into 3 partitions. The data is evenly distributed across the 3 partitions. The resulting <code>repartitionedDataset</code> has 3 partitions as specified by the <code>numPartitions</code> parameter.</p><p><strong>Note:</strong></p><ul><li>The <code>repartition</code> function allows you to control the number of partitions for a Dataset. It can be used to increase the level of parallelism during processing or to reduce the number of partitions to avoid excessive overhead. Choosing an appropriate number of partitions can impact the performance of Spark operations, so it is essential to consider the characteristics of your data and the operations you plan to perform when deciding on the value for <code>numPartitions</code>.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="repartitionbyrangepartitionexprs-column-datasett"><code>repartitionByRange(partitionExprs: Column*): Dataset[T]</code><a class="hash-link" href="#repartitionbyrangepartitionexprs-column-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>repartitionByRange</code> function in Apache Spark&#x27;s Dataset API is used to perform range-based repartitioning of the Dataset based on the specified <code>partitionExprs</code>. Range-based repartitioning is useful when you want to distribute the data across partitions based on the range of values in the specified columns. This is often used for range-based partitioning in structured streaming.</p><p><strong>Parameters:</strong></p><ul><li><code>partitionExprs</code>: The columns based on which the range-based repartitioning should be performed. The data will be partitioned into ranges based on the values in these columns.</li></ul><p><strong>Return Type:</strong>
The function returns a new Dataset of type <code>T</code> that has been repartitioned by range based on the specified partition expressions.</p><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.functions._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Employee(name: String, age: Int, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;repartitionByRange Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Alice&quot;, 25, 50000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Bob&quot;, 30, 60000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;John&quot;, 22, 55000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Mary&quot;, 28, 52000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;David&quot;, 35, 62000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Employee(&quot;Eva&quot;, 27, 53000.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val employeeDataset: Dataset[Employee] = spark.createDataset(employeeData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Repartition the Dataset by range based on the &#x27;age&#x27; column</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val repartitionedByRangeDataset: Dataset[Employee] = employeeDataset.repartitionByRange(col(&quot;age&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Get the number of partitions in the new Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val numPartitions: Int = repartitionedByRangeDataset.rdd.getNumPartitions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(s&quot;Number of partitions after range-based repartitioning: $numPartitions&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Number of partitions after range-based repartitioning: 200</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In the example above, we have a Dataset <code>employeeDataset</code> containing <code>Employee</code> objects. We use the <code>repartitionByRange</code> function to redistribute the data across partitions based on the <code>age</code> column. The data is partitioned into ranges based on the values in the <code>age</code> column, and the number of partitions created is determined by the Spark configuration property <code>spark.sql.shuffle.partitions</code> (default is 200). The resulting <code>repartitionedByRangeDataset</code> is range-partitioned based on the values in the <code>age</code> column.</p><p><strong>Note:</strong></p><ul><li>The <code>repartitionByRange</code> function is typically used in structured streaming scenarios where you want to distribute data across partitions based on specific column ranges. The number of partitions created by this method is determined by the Spark configuration property <code>spark.sql.shuffle.partitions</code>, which controls the default number of partitions for shuffling operations. You can adjust this property to control the level of parallelism for the range-based repartitioning.</li></ul><p>Suppose we have a dataset containing information about various products in an online store. Each product has attributes such as <code>product_id</code>, <code>category</code>, <code>price</code>, and <code>stock_quantity</code>. We want to repartition the dataset by range based on both the <code>category</code> and <code>price</code> columns. This will help us efficiently distribute the data across partitions based on both the category and price ranges, which can be beneficial for certain analytical operations.</p><p>Let&#x27;s create the example and observe the output:</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.functions._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Product(product_id: Int, category: String, price: Double, stock_quantity: Int)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Advanced Range-Based Repartitioning Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val productData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Product(101, &quot;Electronics&quot;, 500.0, 100),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Product(102, &quot;Clothing&quot;, 35.5, 200),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Product(103, &quot;Electronics&quot;, 650.0, 50),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Product(104, &quot;Home &amp; Kitchen&quot;, 120.0, 80),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Product(105, &quot;Clothing&quot;, 45.0, 300),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Product(106, &quot;Electronics&quot;, 780.0, 70),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Product(107, &quot;Home &amp; Kitchen&quot;, 98.5, 120),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Product(108, &quot;Electronics&quot;, 890.0, 40),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Product(109, &quot;Clothing&quot;, 29.9, 250),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Product(110, &quot;Home &amp; Kitchen&quot;, 80.0, 150),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Product(111, &quot;Electronics&quot;, 550.0, 60)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val productDataset: Dataset[Product] = spark.createDataset(productData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Repartition the Dataset by range based on the &#x27;category&#x27; and &#x27;price&#x27; columns</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val repartitionedByRangeDataset: Dataset[Product] = productDataset.repartitionByRange(col(&quot;category&quot;), col(&quot;price&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Get the number of partitions in the new Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val numPartitions: Int = repartitionedByRangeDataset.rdd.getNumPartitions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(s&quot;Number of partitions after range-based repartitioning: $numPartitions&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Number of partitions after range-based repartitioning: 200</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In this example, we have a Dataset <code>productDataset</code> containing <code>Product</code> objects. We use the <code>repartitionByRange</code> function with the <code>col</code> method from <code>org.apache.spark.sql.functions</code> to repartition the data by range based on both the <code>category</code> and <code>price</code> columns. The resulting <code>repartitionedByRangeDataset</code> will have the data distributed across partitions based on both the category and price ranges.</p><p>The number of partitions created by <code>repartitionByRange</code> is determined by the Spark configuration property <code>spark.sql.shuffle.partitions</code>, which controls the default number of partitions for shuffling operations. In this example, the number of partitions is 200.</p><p><strong>Note:</strong></p><ul><li>Range-based repartitioning is useful when you have specific columns that can be used to partition the data into ranges for efficient processing of analytical queries.</li><li>In real-world scenarios, the number of partitions and the distribution of data across them can significantly impact the performance of Spark jobs. You can adjust the configuration property <code>spark.sql.shuffle.partitions</code> to optimize the number of partitions based on your specific use case and cluster resources.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="repartitionbyrangenumpartitions-int-partitionexprs-column-datasett"><code>repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T]</code><a class="hash-link" href="#repartitionbyrangenumpartitions-int-partitionexprs-column-datasett" title="Direct link to heading">​</a></h4><p>The <code>repartitionByRange</code> function is used to redistribute the data in a Spark 3 Dataset by range-based partitioning. It allows you to specify the number of partitions and one or more columns (partition expressions) to determine the range for the repartitioning. This function is useful when you want to control the number of partitions explicitly and distribute the data efficiently based on specific column ranges.</p><p><strong>Parameters</strong>:</p><ul><li><code>numPartitions</code> (Int): The number of partitions to create after repartitioning. This parameter determines how many partitions the dataset will be divided into.</li><li><code>partitionExprs</code> (Column*): One or more column expressions that will be used to determine the partition ranges. These columns should be numeric or date types to define meaningful ranges.</li></ul><p><strong>Return Type</strong>:</p><ul><li><code>Dataset[T]</code>: The resulting Dataset after repartitioning.</li></ul><p><strong>Usage Example</strong>:</p><p>Suppose we have a Dataset containing information about sales transactions in an online store. Each transaction has attributes like <code>transaction_id</code>, <code>customer_id</code>, <code>timestamp</code>, and <code>total_amount</code>. We want to repartition the dataset into 5 partitions based on the <code>customer_id</code> and <code>total_amount</code> columns.</p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.functions._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Case class representing the structure of the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Transaction(transaction_id: Int, customer_id: String, timestamp: Long, total_amount: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Range-Based Repartitioning Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val transactionData = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Transaction(1, &quot;Cust-101&quot;, 1622116800, 100.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Transaction(2, &quot;Cust-102&quot;, 1622203200, 250.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Transaction(3, &quot;Cust-101&quot;, 1622289600, 80.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Transaction(4, &quot;Cust-103&quot;, 1622376000, 150.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Transaction(5, &quot;Cust-102&quot;, 1622462400, 120.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Transaction(6, &quot;Cust-104&quot;, 1622548800, 50.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Transaction(7, &quot;Cust-101&quot;, 1622635200, 200.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Transaction(8, &quot;Cust-103&quot;, 1622721600, 300.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Transaction(9, &quot;Cust-102&quot;, 1622808000, 180.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Transaction(10, &quot;Cust-105&quot;, 1622894400, 70.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Creating the Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val transactionDataset: Dataset[Transaction] = spark.createDataset(transactionData)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Repartition the Dataset by range based on the &#x27;customer_id&#x27; and &#x27;total_amount&#x27; columns with 5 partitions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val repartitionedByRangeDataset: Dataset[Transaction] = transactionDataset.repartitionByRange(5, col(&quot;customer_id&quot;), col(&quot;total_amount&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Get the number of partitions in the new Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val numPartitions: Int = repartitionedByRangeDataset.rdd.getNumPartitions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">println(s&quot;Number of partitions after range-based repartitioning: $numPartitions&quot;)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">Number of partitions after range-based repartitioning: 5</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In this example, we have a Dataset <code>transactionDataset</code> containing <code>Transaction</code> objects. We use the <code>repartitionByRange</code> function to repartition the data into 5 partitions based on the <code>customer_id</code> and <code>total_amount</code> columns. The resulting <code>repartitionedByRangeDataset</code> will have the data distributed across 5 partitions according to the specified column ranges.</p><p>The <code>repartitionByRange</code> function allows us to explicitly set the number of partitions and control the data distribution for efficient processing of analytical queries.</p><p><strong>Note:</strong></p><ul><li>The number of partitions specified in the <code>repartitionByRange</code> function should be chosen carefully, considering the data size and the available resources in the cluster. In practice, you might need to tune this value based on your specific use case.</li><li>Range-based repartitioning is particularly useful when the dataset contains skewed data distributions, as it helps balance the data across partitions, enabling better parallelism during processing.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="samplewithreplacement-boolean-fraction-double-datasett"><code>sample(withReplacement: Boolean, fraction: Double): Dataset[T]</code><a class="hash-link" href="#samplewithreplacement-boolean-fraction-double-datasett" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_DXrL" id="samplewithreplacement-boolean-fraction-double-seed-long-datasett"><code>sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T]</code><a class="hash-link" href="#samplewithreplacement-boolean-fraction-double-seed-long-datasett" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_DXrL" id="samplefraction-double-datasett"><code>sample(fraction: Double): Dataset[T]</code><a class="hash-link" href="#samplefraction-double-datasett" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_DXrL" id="samplefraction-double-seed-long-datasett"><code>sample(fraction: Double, seed: Long): Dataset[T]</code><a class="hash-link" href="#samplefraction-double-seed-long-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>sample</code> function in Apache Spark is used to randomly sample elements from a Dataset. Sampling is a technique used to select a subset of data from a larger dataset for analysis or testing purposes. The function allows you to control the sampling behavior, such as with or without replacement and specifying the fraction of data to be sampled.</p><p><strong>Parameters:</strong></p><ul><li><code>withReplacement</code> (Boolean): A flag indicating whether the sampling should be done with replacement (true) or without replacement (false). When sampling with replacement, an element can be selected more than once in the sample, while in sampling without replacement, each element is selected only once in the sample.</li><li><code>fraction</code> (Double): The fraction of data to be sampled. It should be a value between 0 and 1, where 0 represents no data to be sampled, and 1 represents the entire dataset to be sampled.</li><li><code>seed</code> (Long): (Optional) A seed value for the random number generator. Providing a seed ensures that the sampling is deterministic and repeatable. If not provided, the sampling will be random and non-repeatable.</li></ul><p><strong>Return Type:</strong></p><ul><li><code>Dataset[T]</code>: The resulting Dataset containing the sampled elements.</li></ul><p><strong>Usage Examples:</strong></p><p><strong>Example 1: Sampling Without Replacement</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = 1 to 100</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Sampling Without Replacement Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset: Dataset[Int] = spark.createDataset(data)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample 20% of the data without replacement</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val sampledDataWithoutReplacement: Dataset[Int] = dataset.sample(withReplacement = false, fraction = 0.2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Show the sampled data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sampledDataWithoutReplacement.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|value|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  16|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  18|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  22|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  23|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  24|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  34|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  40|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  47|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  51|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  53|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Example 2: Sampling With Replacement</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = 1 to 100</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Sampling With Replacement Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset: Dataset[Int] = spark.createDataset(data)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample 30% of the data with replacement and a specific seed value</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val sampledDataWithReplacement: Dataset[Int] = dataset.sample(withReplacement = true, fraction = 0.3, seed = 123)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Show the sampled data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sampledDataWithReplacement.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|value|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  10|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  20|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  22|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  23|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  26|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  31|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  43|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  45|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  47|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  47|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  50|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  53|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  59|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  60|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  63|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  66|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  67|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  76|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  78|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  81|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+---+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In these examples, we have a Dataset <code>dataset</code> containing integer values from 1 to 100. We use the <code>sample</code> function to randomly sample elements from the dataset. The first example demonstrates sampling without replacement, where 20% of the data is sampled randomly. The second example demonstrates sampling with replacement, where 30% of the data is sampled with a specific seed value (123) for repeatable results.</p><p><strong>Note:</strong></p><ul><li>Sampling is an essential technique in data analysis and machine learning for validating models, creating training datasets, or understanding the characteristics of data. The <code>sample</code> function in Spark provides an easy way to perform random sampling on large datasets.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="selectu1-u2-u3-u4-u5c1-typedcolumnt-u1-c2-typedcolumnt-u2-c3-typedcolumnt-u3-c4-typedcolumnt-u4-c5-typedcolumnt-u5-datasetu1-u2-u3-u4-u5"><code>select[U1, U2, U3, U4, U5](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4], c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)]</code><a class="hash-link" href="#selectu1-u2-u3-u4-u5c1-typedcolumnt-u1-c2-typedcolumnt-u2-c3-typedcolumnt-u3-c4-typedcolumnt-u4-c5-typedcolumnt-u5-datasetu1-u2-u3-u4-u5" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>select</code> function in Apache Spark is used to project specific columns from a Dataset and create a new Dataset with the selected columns. The function allows you to specify up to five columns to be selected and returns a Dataset of tuples containing the selected columns&#x27; types.</p><p><strong>Parameters:</strong></p><ul><li><code>c1</code>, <code>c2</code>, <code>c3</code>, <code>c4</code>, <code>c5</code> (TypedColumn<!-- -->[T, U1]<!-- -->, TypedColumn<!-- -->[T, U2]<!-- -->, TypedColumn<!-- -->[T, U3]<!-- -->, TypedColumn<!-- -->[T, U4]<!-- -->, TypedColumn<!-- -->[T, U5]<!-- -->): The columns to be selected from the Dataset. Each column should be of a specific type <code>U1</code>, <code>U2</code>, <code>U3</code>, <code>U4</code>, <code>U5</code>, respectively.</li></ul><p><strong>Return Type:</strong></p><ul><li><code>Dataset[(U1, U2, U3, U4, U5)]</code>: The resulting Dataset containing tuples of the selected columns&#x27; types.</li></ul><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.functions._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data case class</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int, city: String, occupation: String, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 28, &quot;New York&quot;, &quot;Engineer&quot;, 75000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 22, &quot;San Francisco&quot;, &quot;Developer&quot;, 60000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Charlie&quot;, 32, &quot;Los Angeles&quot;, &quot;Data Scientist&quot;, 90000.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Select Function Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Import implicits for DataFrame operations</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import spark.implicits._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset: Dataset[Person] = data.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Select specific columns from the Dataset: name, age, and salary</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val selectedData: Dataset[(String, Int, Double)] = dataset.select($&quot;name&quot;, $&quot;age&quot;, $&quot;salary&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Show the selected data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">selectedData.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+-------+---+-------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|   name|age| salary|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------+---+-------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Alice| 28|75000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    Bob| 22|60000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Charlie| 32|90000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------+---+-------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In this example, we have a case class <code>Person</code> representing individual records with name, age, city, occupation, and salary fields. We create a Dataset <code>dataset</code> containing sample data of three people. Using the <code>select</code> function, we project only the <code>name</code>, <code>age</code>, and <code>salary</code> columns from the original Dataset. The selected columns are then stored in a new Dataset <code>selectedData</code>, which contains tuples of the types <code>(String, Int, Double)</code>. Finally, we show the resulting data, displaying only the selected columns.</p><p><strong>Note:</strong></p><ul><li>The <code>select</code> function is useful for focusing on specific columns of interest when working with large datasets and creating new datasets with the desired column combinations. It allows for better data manipulation and analysis by reducing the amount of data that needs to be processed.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="sortsortexprs-column-datasett"><code>sort(sortExprs: Column*): Dataset[T]</code><a class="hash-link" href="#sortsortexprs-column-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>sort</code> function in Apache Spark is used to sort the elements in a Dataset based on one or more columns. It returns a new Dataset with the elements sorted in ascending order by default.</p><p><strong>Parameters:</strong></p><ul><li><code>sortExprs</code> (Column*): One or more columns used for sorting the elements in the Dataset. You can pass multiple columns to sort the data in a specific order. Each column is represented as a <code>Column</code> object.</li></ul><p><strong>Return Type:</strong></p><ul><li><code>Dataset[T]</code>: The resulting Dataset with elements sorted based on the specified columns.</li></ul><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.functions._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data case class</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int, city: String, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 28, &quot;New York&quot;, 75000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 22, &quot;San Francisco&quot;, 60000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Charlie&quot;, 32, &quot;Los Angeles&quot;, 90000.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Sort Function Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Import implicits for DataFrame operations</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import spark.implicits._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset: Dataset[Person] = data.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sort the Dataset by the &quot;age&quot; column in ascending order</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val sortedData: Dataset[Person] = dataset.sort($&quot;age&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Show the sorted data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sortedData.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+-------+---+-------------+-------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|   name|age|         city| salary|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------+---+-------------+-------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    Bob| 22|San Francisco|60000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Alice| 28|     New York|75000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Charlie| 32|  Los Angeles|90000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------+---+-------------+-------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In this example, we have a case class <code>Person</code> representing individual records with name, age, city, and salary fields. We create a Dataset <code>dataset</code> containing sample data of three people. Using the <code>sort</code> function, we sort the elements in the Dataset based on the &quot;age&quot; column in ascending order (from the youngest to the oldest). The resulting sorted data is stored in a new Dataset <code>sortedData</code>, and we display it using the <code>show()</code> function.</p><p><strong>Note:</strong></p><ul><li>By default, the <code>sort</code> function sorts the elements in ascending order. If you need to sort in descending order, you can use the <code>desc()</code> function from <code>org.apache.spark.sql.functions</code> on the <code>Column</code> object representing the column to sort in descending order.</li><li>The <code>sort</code> function can take multiple columns as arguments to sort the data based on multiple criteria. The sorting will be done in the order of the provided columns.</li></ul><h4 class="anchor anchorWithStickyNavbar_DXrL" id="sortsortcol-string-sortcols-string-datasett"><code>sort(sortCol: String, sortCols: String*): Dataset[T]</code><a class="hash-link" href="#sortsortcol-string-sortcols-string-datasett" title="Direct link to heading">​</a></h4><p><strong>Description:</strong>
The <code>sort</code> function in Apache Spark is used to sort the elements in a Dataset based on one or more columns specified by their names. It returns a new Dataset with the elements sorted in ascending order by default.</p><p><strong>Parameters:</strong></p><ul><li><code>sortCol</code> (String): The name of the first column used for sorting the elements in the Dataset.</li><li><code>sortCols</code> (String*): Zero or more additional column names used for sorting the elements. You can pass multiple column names to sort the data in a specific order.</li></ul><p><strong>Return Type:</strong></p><ul><li><code>Dataset[T]</code>: The resulting Dataset with elements sorted based on the specified columns.</li></ul><p><strong>Usage Example:</strong></p><div class="codeBlockContainer_aLwb language-scala theme-code-block"><div class="codeBlockContent_INfL scala"><pre tabindex="0" class="prism-code language-scala codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.sql.{SparkSession, Dataset}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data case class</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">case class Person(name: String, age: Int, city: String, salary: Double)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sample data for the Dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val data = Seq(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Alice&quot;, 28, &quot;New York&quot;, 75000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Bob&quot;, 22, &quot;San Francisco&quot;, 60000.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Person(&quot;Charlie&quot;, 32, &quot;Los Angeles&quot;, 90000.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a SparkSession</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val spark = SparkSession.builder()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .appName(&quot;Sort Function Example&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .master(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  .getOrCreate()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Import implicits for DataFrame operations</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import spark.implicits._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Create a Dataset from the sample data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val dataset: Dataset[Person] = data.toDS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Sort the Dataset by the &quot;age&quot; column in ascending order</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val sortedData: Dataset[Person] = dataset.sort(&quot;age&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// Show the sorted data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sortedData.show()</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Output:</strong></p><div class="codeBlockContainer_aLwb theme-code-block"><div class="codeBlockContent_INfL"><pre tabindex="0" class="prism-code language-text codeBlock_HyjA thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_v42P"><span class="token-line" style="color:#393A34"><span class="token plain">+-------+---+-------------+-------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|   name|age|         city| salary|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------+---+-------------+-------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|    Bob| 22|San Francisco|60000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  Alice| 28|     New York|75000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|Charlie| 32|  Los Angeles|90000.0|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------+---+-------------+-------+</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_n_1r clean-btn">Copy</button></div></div><p><strong>Explanation:</strong>
In this example, we have a case class <code>Person</code> representing individual records with name, age, city, and salary fields. We create a Dataset <code>dataset</code> containing sample data of three people. Using the <code>sort</code> function, we sort the elements in the Dataset based on the &quot;age&quot; column in ascending order (from the youngest to the oldest). The resulting sorted data is stored in a new Dataset <code>sortedData</code>, and we display it using the <code>show()</code> function.</p><p><strong>Note:</strong></p><ul><li>By default, the <code>sort</code> function sorts the elements in ascending order. If you need to sort in descending order, you can append <code>.desc</code> to the column name, e.g., <code>sort(&quot;age&quot;.desc)</code>.</li><li>The <code>sort</code> function can take multiple column names as arguments to sort the data based on multiple criteria. The sorting will be done in the order of the provided columns.</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/dzharii/dzharii.github.io/docs/gpt4-generated-content/2023-05-03-apache-spark-ref.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_sc9w" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_Tebc"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/fun/2023-02-16-Seattle-Link-Pictograms"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">2023-02-16-Seattle-Link-Pictograms</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/psy-saved-articles/a-programmers-guide-to-stress"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">A Programmers Guide to Stress</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_VRyP thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#inspired-by" class="table-of-contents__link toc-highlight">Inspired by:</a></li><li><a href="#prompt" class="table-of-contents__link toc-highlight">Prompt:</a></li><li><a href="#actions" class="table-of-contents__link toc-highlight">Actions</a></li><li><a href="#basic-dataset-functions" class="table-of-contents__link toc-highlight">Basic Dataset functions</a></li><li><a href="#streaming" class="table-of-contents__link toc-highlight">Streaming</a></li><li><a href="#typed-transformations" class="table-of-contents__link toc-highlight">Typed transformations</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Wiki</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Disclaimer: The content on this website is my personal work and reflects my own views and opinions. 
            It is not associated with or endorsed by any of my past, current, or future employers. This site contains links to external websites and compiled pages from various sources. 
            I do not claim any rights or responsibility for the content of these external materials. 
            All content is provided "AS IS" without any guarantees or warranties.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.e3d900ad.js"></script>
<script src="/assets/js/main.77d6e0e3.js"></script>
</body>
</html>